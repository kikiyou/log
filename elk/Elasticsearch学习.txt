安装Elasticsearch
理解Elasticsearch最好的方式是去运行它，让我们开始吧！

安装Elasticsearch唯一的要求是安装官方新版的Java，地址：www.java.com

你可以从 elasticsearch.org/download 下载最新版本的Elasticsearch。

curl -L -O http://download.elasticsearch.org/PATH/TO/VERSION.zip <1>
unzip elasticsearch-$VERSION.zip
cd  elasticsearch-$VERSION
从 elasticsearch.org/download 获得最新可用的版本号并填入URL中
提示：

在生产环境安装时，除了以上方法，你还可以使用Debian或者RPM安装包，地址在这里：downloads page，或者也可以使用官方提供的 Puppet module 或者 Chef cookbook。
安装Marvel
Marvel是Elasticsearch的管理和监控工具，在开发环境下免费使用。它包含了一个叫做Sense的交互式控制台，使用户方便的通过浏览器直接与Elasticsearch进行交互。

Elasticsearch线上文档中的很多示例代码都附带一个View in Sense的链接。点击进去，就会在Sense控制台打开相应的实例。安装Marvel不是必须的，但是它可以通过在你本地Elasticsearch集群中运行示例代码而增加与此书的互动性。

Marvel是一个插件，可在Elasticsearch目录中运行以下命令来下载和安装：

./bin/plugin -i elasticsearch/marvel/latest
你可能想要禁用监控，你可以通过以下命令关闭Marvel：

echo 'marvel.agent.enabled: false' >> ./config/elasticsearch.yml
运行Elasticsearch
Elasticsearch已经准备就绪，执行以下命令可在前台启动：

./bin/elasticsearch
如果想在后台以守护进程模式运行，添加-d参数。

打开另一个终端进行测试：

curl 'http://localhost:9200/?pretty'
你能看到以下返回信息：

{
   "status": 200,
   "name": "Shrunken Bones",
   "version": {
      "number": "1.4.0",
      "lucene_version": "4.10"
   },
   "tagline": "You Know, for Search"
}
这说明你的ELasticsearch集群已经启动并且正常运行，接下来我们可以开始各种实验了。

集群和节点
节点(node)是一个运行着的Elasticsearch实例。集群(cluster)是一组具有相同cluster.name的节点集合，他们协同工作，共享数据并提供故障转移和扩展功能，当然一个节点也可以组成一个集群。

你最好找一个合适的名字来替代cluster.name的默认值，比如你自己的名字，这样可以防止一个新启动的节点加入到相同网络中的另一个同名的集群中。

你可以通过修改config/目录下的elasticsearch.yml文件，然后重启ELasticsearch来做到这一点。当Elasticsearch在前台运行，可以使用Ctrl-C快捷键终止，或者你可以调用shutdown API来关闭：

curl -XPOST 'http://localhost:9200/_shutdown'
查看Marvel和Sense
如果你安装了Marvel（作为管理和监控的工具），就可以在浏览器里通过以下地址访问它：

http://localhost:9200/_plugin/marvel/

你可以在Marvel中通过点击dashboards，在下拉菜单中访问Sense开发者控制台，或者直接访问以下地址：

http://localhost:9200/_plugin/marvel/sense/



与Elasticsearch交互
如何与Elasticsearch交互取决于你是否使用Java。

Java API

Elasticsearch为Java用户提供了两种内置客户端：

节点客户端(node client)：

节点客户端以无数据节点(none data node)身份加入集群，换言之，它自己不存储任何数据，但是它知道数据在集群中的具体位置，并且能够直接转发请求到对应的节点上。

传输客户端(Transport client)：

这个更轻量的传输客户端能够发送请求到远程集群。它自己不加入集群，只是简单转发请求给集群中的节点。

两个Java客户端都通过9300端口与集群交互，使用Elasticsearch传输协议(Elasticsearch Transport Protocol)。集群中的节点之间也通过9300端口进行通信。如果此端口未开放，你的节点将不能组成集群。

TIP

Java客户端所在的Elasticsearch版本必须与集群中其他节点一致，否则，它们可能互相无法识别。
关于Java API的更多信息请查看相关章节：Java API

基于HTTP协议，以JSON为数据交互格式的RESTful API

其他所有程序语言都可以使用RESTful API，通过9200端口的与Elasticsearch进行通信，你可以使用你喜欢的WEB客户端，事实上，如你所见，你甚至可以通过curl命令与Elasticsearch通信。

NOTE

Elasticsearch官方提供了多种程序语言的客户端——Groovy，Javascript， .NET，PHP，Perl，Python，以及 Ruby——还有很多由社区提供的客户端和插件，所有这些可以在文档中找到。
向Elasticsearch发出的请求的组成部分与其它普通的HTTP请求是一样的：

curl -X<VERB> '<PROTOCOL>://<HOST>:<PORT>/<PATH>?<QUERY_STRING>' -d '<BODY>'
VERB HTTP方法：GET, POST, PUT, HEAD, DELETE
PROTOCOL http或者https协议（只有在Elasticsearch前面有https代理的时候可用）
HOST Elasticsearch集群中的任何一个节点的主机名，如果是在本地的节点，那么就叫localhost
PORT Elasticsearch HTTP服务所在的端口，默认为9200
PATH API路径（例如_count将返回集群中文档的数量），PATH可以包含多个组件，例如_cluster/stats或者_nodes/stats/jvm
QUERY_STRING 一些可选的查询请求参数，例如?pretty参数将使请求返回更加美观易读的JSON数据
BODY 一个JSON格式的请求主体（如果请求需要的话）
举例说明，为了计算集群中的文档数量，我们可以这样做：

curl -XGET 'http://localhost:9200/_count?pretty' -d '
{
    "query": {
        "match_all": {}
    }
}
'
Elasticsearch返回一个类似200 OK的HTTP状态码和JSON格式的响应主体（除了HEAD请求）。上面的请求会得到如下的JSON格式的响应主体：

{
    "count" : 0,
    "_shards" : {
        "total" : 5,
        "successful" : 5,
        "failed" : 0
    }
}
我们看不到HTTP头是因为我们没有让curl显示它们，如果要显示，使用curl命令后跟-i参数:

curl -i -XGET 'localhost:9200/'
对于本书的其余部分，我们将简写curl请求中重复的部分，例如主机名和端口，还有curl命令本身。

一个完整的请求形如：

curl -XGET 'localhost:9200/_count?pretty' -d '
{
    "query": {
        "match_all": {}
    }
}'
我们将简写成这样：

GET /_count
{
    "query": {
        "match_all": {}
    }
}
事实上，在Sense控制台中也使用了与上面相同的格式。




面向文档
应用中的对象很少只是简单的键值列表，更多时候它拥有复杂的数据结构，比如包含日期、地理位置、另一个对象或者数组。

总有一天你会想到把这些对象存储到数据库中。将这些数据保存到由行和列组成的关系数据库中，就好像是把一个丰富，信息表现力强的对象拆散了放入一个非常大的表格中：你不得不拆散对象以适应表模式（通常一列表示一个字段），然后又不得不在查询的时候重建它们。

Elasticsearch是面向文档(document oriented)的，这意味着它可以存储整个对象或文档(document)。然而它不仅仅是存储，还会索引(index)每个文档的内容使之可以被搜索。在Elasticsearch中，你可以对文档（而非成行成列的数据）进行索引、搜索、排序、过滤。这种理解数据的方式与以往完全不同，这也是Elasticsearch能够执行复杂的全文搜索的原因之一。

JSON
ELasticsearch使用Javascript对象符号(JavaScript Object Notation)，也就是JSON，作为文档序列化格式。JSON现在已经被大多语言所支持，而且已经成为NoSQL领域的标准格式。它简洁、简单且容易阅读。

以下使用JSON文档来表示一个用户对象：

{
    "email":      "john@smith.com",
    "first_name": "John",
    "last_name":  "Smith",
    "info": {
        "bio":         "Eco-warrior and defender of the weak",
        "age":         25,
        "interests": [ "dolphins", "whales" ]
    },
    "join_date": "2014/05/01"
}
尽管原始的user对象很复杂，但它的结构和对象的含义已经被完整的体现在JSON中了，在Elasticsearch中将对象转化为JSON并做索引要比在表结构中做相同的事情简单的多。

NOTE

尽管几乎所有的语言都有相应的模块用于将任意数据结构转换为JSON，但每种语言处理细节不同。具体请查看“serialization” or “marshalling”两个用于处理JSON的模块。Elasticsearch官方客户端会自动为你序列化和反序列化JSON。




索引开始第一步
我们现在开始进行一个简单教程，它涵盖了一些基本的概念介绍，比如索引(indexing)、搜索(search)以及聚合(aggregations)。通过这个教程，我们可以让你对Elasticsearch能做的事以及其易用程度有一个大致的感觉。

我们接下来将陆续介绍一些术语和基本的概念，但就算你没有马上完全理解也没有关系。我们将在本书的各个章节中更加深入的探讨这些内容。

所以，坐下来，开始以旋风般的速度来感受Elasticsearch的能力吧！

让我们建立一个员工目录
假设我们刚好在Megacorp工作，这时人力资源部门出于某种目的需要让我们创建一个员工目录，这个目录用于促进人文关怀和用于实时协同工作，所以它有以下不同的需求：

数据能够包含多个值的标签、数字和纯文本。
检索任何员工的所有信息。
支持结构化搜索，例如查找30岁以上的员工。
支持简单的全文搜索和更复杂的短语(phrase)搜索
高亮搜索结果中的关键字
能够利用图表管理分析这些数据
索引员工文档
我们首先要做的是存储员工数据，每个文档代表一个员工。在Elasticsearch中存储数据的行为就叫做索引(indexing)，不过在索引之前，我们需要明确数据应该存储在哪里。

在Elasticsearch中，文档归属于一种类型(type),而这些类型存在于索引(index)中，我们可以画一些简单的对比图来类比传统关系型数据库：

Relational DB -> Databases -> Tables -> Rows -> Columns
Elasticsearch -> Indices   -> Types  -> Documents -> Fields
Elasticsearch集群可以包含多个索引(indices)（数据库），每一个索引可以包含多个类型(types)（表），每一个类型包含多个文档(documents)（行），然后每个文档包含多个字段(Fields)（列）。

「索引」含义的区分

你可能已经注意到索引(index)这个词在Elasticsearch中有着不同的含义，所以有必要在此做一下区分:

索引（名词） 如上文所述，一个索引(index)就像是传统关系数据库中的数据库，它是相关文档存储的地方，index的复数是indices 或indexes。
索引（动词） 「索引一个文档」表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL中的INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。
倒排索引 传统数据库为特定列增加一个索引，例如B-Tree索引来加速检索。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的。
默认情况下，文档中的所有字段都会被索引（拥有一个倒排索引），只有这样他们才是可被搜索的。

我们将会在倒排索引章节中更详细的讨论。

所以为了创建员工目录，我们将进行如下操作：

为每个员工的文档(document)建立索引，每个文档包含了相应员工的所有信息。
每个文档的类型为employee。
employee类型归属于索引megacorp。
megacorp索引存储在Elasticsearch集群中。
实际上这些都是很容易的（尽管看起来有许多步骤）。我们能通过一个命令执行完成的操作：

PUT /megacorp/employee/1
{
    "first_name" : "John",
    "last_name" :  "Smith",
    "age" :        25,
    "about" :      "I love to go rock climbing",
    "interests": [ "sports", "music" ]
}
我们看到path:/megacorp/employee/1包含三部分信息：

名字	说明
megacorp	索引名
employee	类型名
1	这个员工的ID
请求实体（JSON文档），包含了这个员工的所有信息。他的名字叫“John Smith”，25岁，喜欢攀岩。

很简单吧！它不需要你做额外的管理操作，比如创建索引或者定义每个字段的数据类型。我们能够直接索引文档，Elasticsearch已经内置所有的缺省设置，所有管理操作都是透明的。

接下来，让我们在目录中加入更多员工信息：

PUT /megacorp/employee/2
{
    "first_name" :  "Jane",
    "last_name" :   "Smith",
    "age" :         32,
    "about" :       "I like to collect rock albums",
    "interests":  [ "music" ]
}

PUT /megacorp/employee/3
{
    "first_name" :  "Douglas",
    "last_name" :   "Fir",
    "age" :         35,
    "about":        "I like to build cabinets",
    "interests":  [ "forestry" ]
}




检索文档
现在Elasticsearch中已经存储了一些数据，我们可以根据业务需求开始工作了。第一个需求是能够检索单个员工的信息。

这对于Elasticsearch来说非常简单。我们只要执行HTTP GET请求并指出文档的“地址”——索引、类型和ID既可。根据这三部分信息，我们就可以返回原始JSON文档：

GET /megacorp/employee/1
响应的内容中包含一些文档的元信息，John Smith的原始JSON文档包含在_source字段中。

{
  "_index" :   "megacorp",
  "_type" :    "employee",
  "_id" :      "1",
  "_version" : 1,
  "found" :    true,
  "_source" :  {
      "first_name" :  "John",
      "last_name" :   "Smith",
      "age" :         25,
      "about" :       "I love to go rock climbing",
      "interests":  [ "sports", "music" ]
  }
}
我们通过HTTP方法GET来检索文档，同样的，我们可以使用DELETE方法删除文档，使用HEAD方法检查某文档是否存在。如果想更新已存在的文档，我们只需再PUT一次。
简单搜索
GET请求非常简单——你能轻松获取你想要的文档。让我们来进一步尝试一些东西，比如简单的搜索！

我们尝试一个最简单的搜索全部员工的请求：

GET /megacorp/employee/_search
你可以看到我们依然使用megacorp索引和employee类型，但是我们在结尾使用关键字_search来取代原来的文档ID。响应内容的hits数组中包含了我们所有的三个文档。默认情况下搜索会返回前10个结果。

{
   "took":      6,
   "timed_out": false,
   "_shards": { ... },
   "hits": {
      "total":      3,
      "max_score":  1,
      "hits": [
         {
            "_index":         "megacorp",
            "_type":          "employee",
            "_id":            "3",
            "_score":         1,
            "_source": {
               "first_name":  "Douglas",
               "last_name":   "Fir",
               "age":         35,
               "about":       "I like to build cabinets",
               "interests": [ "forestry" ]
            }
         },
         {
            "_index":         "megacorp",
            "_type":          "employee",
            "_id":            "1",
            "_score":         1,
            "_source": {
               "first_name":  "John",
               "last_name":   "Smith",
               "age":         25,
               "about":       "I love to go rock climbing",
               "interests": [ "sports", "music" ]
            }
         },
         {
            "_index":         "megacorp",
            "_type":          "employee",
            "_id":            "2",
            "_score":         1,
            "_source": {
               "first_name":  "Jane",
               "last_name":   "Smith",
               "age":         32,
               "about":       "I like to collect rock albums",
               "interests": [ "music" ]
            }
         }
      ]
   }
}
注意：

响应内容不仅会告诉我们哪些文档被匹配到，而且这些文档内容完整的被包含在其中—我们在给用户展示搜索结果时需要用到的所有信息都有了。
接下来，让我们搜索姓氏中包含“Smith”的员工。要做到这一点，我们将在命令行中使用轻量级的搜索方法。这种方法常被称作查询字符串(query string)搜索，因为我们像传递URL参数一样去传递查询语句：

GET /megacorp/employee/_search?q=last_name:Smith
我们在请求中依旧使用_search关键字，然后将查询语句传递给参数q=。这样就可以得到所有姓氏为Smith的结果：

{
   ...
   "hits": {
      "total":      2,
      "max_score":  0.30685282,
      "hits": [
         {
            ...
            "_source": {
               "first_name":  "John",
               "last_name":   "Smith",
               "age":         25,
               "about":       "I love to go rock climbing",
               "interests": [ "sports", "music" ]
            }
         },
         {
            ...
            "_source": {
               "first_name":  "Jane",
               "last_name":   "Smith",
               "age":         32,
               "about":       "I like to collect rock albums",
               "interests": [ "music" ]
            }
         }
      ]
   }
}
使用DSL语句查询
查询字符串搜索便于通过命令行完成特定(ad hoc)的搜索，但是它也有局限性（参阅简单搜索章节）。Elasticsearch提供丰富且灵活的查询语言叫做DSL查询(Query DSL),它允许你构建更加复杂、强大的查询。

DSL(Domain Specific Language特定领域语言)以JSON请求体的形式出现。我们可以这样表示之前关于“Smith”的查询:

GET /megacorp/employee/_search
{
    "query" : {
        "match" : {
            "last_name" : "Smith"
        }
    }
}
这会返回与之前查询相同的结果。你可以看到有些东西改变了，我们不再使用查询字符串(query string)做为参数，而是使用请求体代替。这个请求体使用JSON表示，其中使用了match语句（查询类型之一，具体我们以后会学到）。

更复杂的搜索
我们让搜索稍微再变的复杂一些。我们依旧想要找到姓氏为“Smith”的员工，但是我们只想得到年龄大于30岁的员工。我们的语句将添加过滤器(filter),它使得我们高效率的执行一个结构化搜索：

GET /megacorp/employee/_search
{
    "query" : {
        "filtered" : {
            "filter" : {
                "range" : {
                    "age" : { "gt" : 30 } <1>
                }
            },
            "query" : {
                "match" : {
                    "last_name" : "smith" <2>
                }
            }
        }
    }
}
这部分查询属于**区间过滤器(range filter)**,它用于查找所有年龄大于30岁的数据——`gt`为"greater than"的缩写。
这部分查询与之前的`match`**语句(query)**一致。
现在不要担心语法太多，我们将会在以后详细的讨论。你只要知道我们添加了一个过滤器(filter)用于执行区间搜索，然后重复利用了之前的match语句。现在我们的搜索结果只显示了一个32岁且名字是“Jane Smith”的员工：

{
   ...
   "hits": {
      "total":      1,
      "max_score":  0.30685282,
      "hits": [
         {
            ...
            "_source": {
               "first_name":  "Jane",
               "last_name":   "Smith",
               "age":         32,
               "about":       "I like to collect rock albums",
               "interests": [ "music" ]
            }
         }
      ]
   }
}
全文搜索
到目前为止搜索都很简单：搜索特定的名字，通过年龄筛选。让我们尝试一种更高级的搜索，全文搜索——一种传统数据库很难实现的功能。

我们将会搜索所有喜欢“rock climbing”的员工：

GET /megacorp/employee/_search
{
    "query" : {
        "match" : {
            "about" : "rock climbing"
        }
    }
}
你可以看到我们使用了之前的match查询，从about字段中搜索"rock climbing"，我们得到了两个匹配文档：

{
   ...
   "hits": {
      "total":      2,
      "max_score":  0.16273327,
      "hits": [
         {
            ...
            "_score":         0.16273327, <1>
            "_source": {
               "first_name":  "John",
               "last_name":   "Smith",
               "age":         25,
               "about":       "I love to go rock climbing",
               "interests": [ "sports", "music" ]
            }
         },
         {
            ...
            "_score":         0.016878016, <2>
            "_source": {
               "first_name":  "Jane",
               "last_name":   "Smith",
               "age":         32,
               "about":       "I like to collect rock albums",
               "interests": [ "music" ]
            }
         }
      ]
   }
}
结果相关性评分。
默认情况下，Elasticsearch根据结果相关性评分来对结果集进行排序，所谓的「结果相关性评分」就是文档与查询条件的匹配程度。很显然，排名第一的John Smith的about字段明确的写到“rock climbing”。

但是为什么Jane Smith也会出现在结果里呢？原因是“rock”在她的abuot字段中被提及了。因为只有“rock”被提及而“climbing”没有，所以她的_score要低于John。

这个例子很好的解释了Elasticsearch如何在各种文本字段中进行全文搜索，并且返回相关性最大的结果集。相关性(relevance)的概念在Elasticsearch中非常重要，而这个概念在传统关系型数据库中是不可想象的，因为传统数据库对记录的查询只有匹配或者不匹配。

短语搜索
目前我们可以在字段中搜索单独的一个词，这挺好的，但是有时候你想要确切的匹配若干个单词或者短语(phrases)。例如我们想要查询同时包含"rock"和"climbing"（并且是相邻的）的员工记录。

要做到这个，我们只要将match查询变更为match_phrase查询即可:

GET /megacorp/employee/_search
{
    "query" : {
        "match_phrase" : {
            "about" : "rock climbing"
        }
    }
}
毫无疑问，该查询返回John Smith的文档：

{
   ...
   "hits": {
      "total":      1,
      "max_score":  0.23013961,
      "hits": [
         {
            ...
            "_score":         0.23013961,
            "_source": {
               "first_name":  "John",
               "last_name":   "Smith",
               "age":         25,
               "about":       "I love to go rock climbing",
               "interests": [ "sports", "music" ]
            }
         }
      ]
   }
}
高亮我们的搜索
很多应用喜欢从每个搜索结果中高亮(highlight)匹配到的关键字，这样用户可以知道为什么这些文档和查询相匹配。在Elasticsearch中高亮片段是非常容易的。

让我们在之前的语句上增加highlight参数：

GET /megacorp/employee/_search
{
    "query" : {
        "match_phrase" : {
            "about" : "rock climbing"
        }
    },
    "highlight": {
        "fields" : {
            "about" : {}
        }
    }
}
当我们运行这个语句时，会命中与之前相同的结果，但是在返回结果中会有一个新的部分叫做highlight，这里包含了来自about字段中的文本，并且用<em></em>来标识匹配到的单词。

{
   ...
   "hits": {
      "total":      1,
      "max_score":  0.23013961,
      "hits": [
         {
            ...
            "_score":         0.23013961,
            "_source": {
               "first_name":  "John",
               "last_name":   "Smith",
               "age":         25,
               "about":       "I love to go rock climbing",
               "interests": [ "sports", "music" ]
            },
            "highlight": {
               "about": [
                  "I love to go <em>rock</em> <em>climbing</em>" <1>
               ]
            }
         }
      ]
   }
}
原有文本中高亮的片段
你可以在高亮章节阅读更多关于搜索高亮的部分。




分析
最后，我们还有一个需求需要完成：允许管理者在职员目录中进行一些分析。 Elasticsearch有一个功能叫做聚合(aggregations)，它允许你在数据上生成复杂的分析统计。它很像SQL中的GROUP BY但是功能更强大。

举个例子，让我们找到所有职员中最大的共同点（兴趣爱好）是什么：

GET /megacorp/employee/_search
{
  "aggs": {
    "all_interests": {
      "terms": { "field": "interests" }
    }
  }
}
暂时先忽略语法只看查询结果：

{
   ...
   "hits": { ... },
   "aggregations": {
      "all_interests": {
         "buckets": [
            {
               "key":       "music",
               "doc_count": 2
            },
            {
               "key":       "forestry",
               "doc_count": 1
            },
            {
               "key":       "sports",
               "doc_count": 1
            }
         ]
      }
   }
}
我们可以看到两个职员对音乐有兴趣，一个喜欢林学，一个喜欢运动。这些数据并没有被预先计算好，它们是实时的从匹配查询语句的文档中动态计算生成的。如果我们想知道所有姓"Smith"的人最大的共同点（兴趣爱好），我们只需要增加合适的语句既可：

GET /megacorp/employee/_search
{
  "query": {
    "match": {
      "last_name": "smith"
    }
  },
  "aggs": {
    "all_interests": {
      "terms": {
        "field": "interests"
      }
    }
  }
}
all_interests聚合已经变成只包含和查询语句相匹配的文档了：

  ...
  "all_interests": {
     "buckets": [
        {
           "key": "music",
           "doc_count": 2
        },
        {
           "key": "sports",
           "doc_count": 1
        }
     ]
  }
聚合也允许分级汇总。例如，让我们统计每种兴趣下职员的平均年龄：

GET /megacorp/employee/_search
{
    "aggs" : {
        "all_interests" : {
            "terms" : { "field" : "interests" },
            "aggs" : {
                "avg_age" : {
                    "avg" : { "field" : "age" }
                }
            }
        }
    }
}
虽然这次返回的聚合结果有些复杂，但任然很容易理解：

  ...
  "all_interests": {
     "buckets": [
        {
           "key": "music",
           "doc_count": 2,
           "avg_age": {
              "value": 28.5
           }
        },
        {
           "key": "forestry",
           "doc_count": 1,
           "avg_age": {
              "value": 35
           }
        },
        {
           "key": "sports",
           "doc_count": 1,
           "avg_age": {
              "value": 25
           }
        }
     ]
  }
该聚合结果比之前的聚合结果要更加丰富。我们依然得到了兴趣以及数量（指具有该兴趣的员工人数）的列表，但是现在每个兴趣额外拥有avg_age字段来显示具有该兴趣员工的平均年龄。

即使你还不理解语法，但你也可以大概感觉到通过这个特性可以完成相当复杂的聚合工作，你可以处理任何类型的数据。




分布式的特性
在章节的开始我们提到Elasticsearch可以扩展到上百（甚至上千）的服务器来处理PB级的数据。然而我们的教程只是给出了一些使用Elasticsearch的例子，并未涉及相关机制。Elasticsearch为分布式而生，而且它的设计隐藏了分布式本身的复杂性。

Elasticsearch在分布式概念上做了很大程度上的透明化，在教程中你不需要知道任何关于分布式系统、分片、集群发现或者其他大量的分布式概念。所有的教程你即可以运行在你的笔记本上，也可以运行在拥有100个节点的集群上，其工作方式是一样的。

Elasticsearch致力于隐藏分布式系统的复杂性。以下这些操作都是在底层自动完成的：

将你的文档分区到不同的容器或者分片(shards)中，它们可以存在于一个或多个节点中。
将分片均匀的分配到各个节点，对索引和搜索做负载均衡。
冗余每一个分片，防止硬件故障造成的数据丢失。
将集群中任意一个节点上的请求路由到相应数据所在的节点。
无论是增加节点，还是移除节点，分片都可以做到无缝的扩展和迁移。
当你阅读本书时，你可以遇到关于Elasticsearch分布式特性的补充章节。这些章节将教给你如何扩展集群和故障转移，如何处理文档存储，如何执行分布式搜索，分片是什么以及如何工作。

这些章节不是必读的——不懂这些内部机制也可以使用Elasticsearch的。但是这些能够帮助你更深入和完整的了解Elasticsearch。你可以略读它们，然后在你需要更深入的理解时再回头翻阅。



集群内部工作方式
补充章节

正如之前提及的，这是关于Elasticsearch在分布式环境下工作机制的一些补充章节的第一部分。这个章节我们解释一些通用的术语，例如集群(cluster)、节点(node)和分片(shard)，Elasticsearch的扩展机制，以及它如何处理硬件故障。

尽管这章不是必读的——你在使用Elasticsearch的时候可以长时间甚至永远都不必担心分片、复制和故障转移——但是它会帮助你理解Elasticsearch内部的工作流程，你可以先跳过这章，以后再来查阅。
Elasticsearch用于构建高可用和可扩展的系统。扩展的方式可以是购买更好的服务器(纵向扩展(vertical scale or scaling up))或者购买更多的服务器（横向扩展(horizontal scale or scaling out)）。

Elasticsearch虽然能从更强大的硬件中获得更好的性能，但是纵向扩展有它的局限性。真正的扩展应该是横向的，它通过增加节点来均摊负载和增加可靠性。

对于大多数数据库而言，横向扩展意味着你的程序将做非常大的改动才能利用这些新添加的设备。对比来说，Elasticsearch天生就是分布式的：它知道如何管理节点来提供高扩展和高可用。这意味着你的程序不需要关心这些。

在这章我们将探索如何创建你的集群(cluster)、节点(node)和分片(shards)，使其按照你的需求进行扩展，并保证在硬件故障时数据依旧安全。


空集群
如果我们启动一个单独的节点，它还没有数据和索引，这个集群看起来就像图1。

A cluster with one empty node 图1：只有一个空节点的集群

一个节点(node)就是一个Elasticsearch实例，而一个集群(cluster)由一个或多个节点组成，它们具有相同的cluster.name，它们协同工作，分享数据和负载。当加入新的节点或者删除一个节点时，集群就会感知到并平衡数据。

集群中一个节点会被选举为主节点(master),它将临时管理集群级别的一些变更，例如新建或删除索引、增加或移除节点等。主节点不参与文档级别的变更或搜索，这意味着在流量增长的时候，该主节点不会成为集群的瓶颈。任何节点都可以成为主节点。我们例子中的集群只有一个节点，所以它会充当主节点的角色。

做为用户，我们能够与集群中的任何节点通信，包括主节点。每一个节点都知道文档存在于哪个节点上，它们可以转发请求到相应的节点上。我们访问的节点负责收集各节点返回的数据，最后一起返回给客户端。这一切都由Elasticsearch处理。



集群健康
在Elasticsearch集群中可以监控统计很多信息，但是只有一个是最重要的：集群健康(cluster health)。集群健康有三种状态：green、yellow或red。

GET /_cluster/health
在一个没有索引的空集群中运行如上查询，将返回这些信息：

{
   "cluster_name":          "elasticsearch",
   "status":                "green", <1>
   "timed_out":             false,
   "number_of_nodes":       1,
   "number_of_data_nodes":  1,
   "active_primary_shards": 0,
   "active_shards":         0,
   "relocating_shards":     0,
   "initializing_shards":   0,
   "unassigned_shards":     0
}
`status` 是我们最感兴趣的字段
status字段提供一个综合的指标来表示集群的的服务状况。三种颜色各自的含义：

颜色	意义
green	所有主要分片和复制分片都可用
yellow	所有主要分片可用，但不是所有复制分片都可用
red	不是所有的主要分片都可用
在接下来的章节，我们将说明什么是主要分片(primary shard)和复制分片(replica shard)，并说明这些颜色（状态）在实际环境中的意义。




添加索引
为了将数据添加到Elasticsearch，我们需要索引(index)——一个存储关联数据的地方。实际上，索引只是一个用来指向一个或多个分片(shards)的“逻辑命名空间(logical namespace)”.

一个分片(shard)是一个最小级别“工作单元(worker unit)”,它只是保存了索引中所有数据的一部分。在接下来的《深入分片》一章，我们将详细说明分片的工作原理，但是现在我们只要知道分片就是一个Lucene实例，并且它本身就是一个完整的搜索引擎。我们的文档存储在分片中，并且在分片中被索引，但是我们的应用程序不会直接与它们通信，取而代之的是，直接与索引通信。

分片是Elasticsearch在集群中分发数据的关键。把分片想象成数据的容器。文档存储在分片中，然后分片分配到你集群中的节点上。当你的集群扩容或缩小，Elasticsearch将会自动在你的节点间迁移分片，以使集群保持平衡。

分片可以是主分片(primary shard)或者是复制分片(replica shard)。你索引中的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。

理论上主分片能存储的数据大小是没有限制的，限制取决于你实际的使用情况。分片的最大容量完全取决于你的使用状况：硬件存储的大小、文档的大小和复杂度、如何索引和查询你的文档，以及你期望的响应时间。
复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的shard取回文档。

当索引创建完成的时候，主分片的数量就固定了，但是复制分片的数量可以随时调整。

让我们在集群中唯一一个空节点上创建一个叫做blogs的索引。默认情况下，一个索引被分配5个主分片，但是为了演示的目的，我们只分配3个主分片和一个复制分片（每个主分片都有一个复制分片）：

PUT /blogs
{
   "settings" : {
      "number_of_shards" : 3,
      "number_of_replicas" : 1
   }
}
附带索引的单一节点集群： 有一个索引的单一节点集群

我们的集群现在看起来就像上图——三个主分片都被分配到Node 1。如果我们现在检查集群健康(cluster-health)，我们将见到以下信息：

{
   "cluster_name":          "elasticsearch",
   "status":                "yellow", <1>
   "timed_out":             false,
   "number_of_nodes":       1,
   "number_of_data_nodes":  1,
   "active_primary_shards": 3,
   "active_shards":         3,
   "relocating_shards":     0,
   "initializing_shards":   0,
   "unassigned_shards":     3 <2>
}
集群的状态现在是 `yellow`
我们的三个复制分片还没有被分配到节点上
集群的健康状态yellow表示所有的主分片(primary shards)启动并且正常运行了——集群已经可以正常处理任何请求——但是复制分片(replica shards)还没有全部可用。事实上所有的三个复制分片现在都是unassigned状态——它们还未被分配给节点。在同一个节点上保存相同的数据副本是没有必要的，如果这个节点故障了，那所有的数据副本也会丢失。

现在我们的集群已经功能完备，但是依旧存在因硬件故障而导致数据丢失的风险。




增加故障转移
在单一节点上运行意味着有单点故障的风险——没有数据备份。幸运的是，要防止单点故障，我们唯一需要做的就是启动另一个节点。

启动第二个节点

为了测试在增加第二个节点后发生了什么，你可以使用与第一个节点相同的方式启动第二个节点（《运行Elasticsearch》一章），而且命令行在同一个目录——一个节点可以启动多个Elasticsearch实例。

只要第二个节点与第一个节点有相同的cluster.name（请看./config/elasticsearch.yml文件），它就能自动发现并加入第一个节点所在的集群。如果没有，检查日志找出哪里出了问题。这可能是网络广播被禁用，或者防火墙阻止了节点通信。
如果我们启动了第二个节点，这个集群看起来就像下图。

双节点集群——所有的主分片和复制分片都已分配: 双节点集群

第二个节点已经加入集群，三个复制分片(replica shards)也已经被分配了——分别对应三个主分片，这意味着在丢失任意一个节点的情况下依旧可以保证数据的完整性。

文档的索引将首先被存储在主分片中，然后并发复制到对应的复制节点上。这可以确保我们的数据在主节点和复制节点上都可以被检索。

cluster-health现在的状态是green，这意味着所有的6个分片（三个主分片和三个复制分片）都已可用：

{
   "cluster_name":          "elasticsearch",
   "status":                "green", <1>
   "timed_out":             false,
   "number_of_nodes":       2,
   "number_of_data_nodes":  2,
   "active_primary_shards": 3,
   "active_shards":         6,
   "relocating_shards":     0,
   "initializing_shards":   0,
   "unassigned_shards":     0
}
集群的状态是`green`.
我们的集群不仅是功能完备的，而且是高可用的。


横向扩展
随着应用需求的增长，我们该如何扩展？如果我们启动第三个节点，我们的集群会重新组织自己，就像图4：

图4：包含3个节点的集群——分片已经被重新分配以平衡负载： 三节点集群

Node3包含了分别来自Node 1和Node 2的一个分片，这样每个节点就有两个分片，和之前相比少了一个，这意味着每个节点上的分片将获得更多的硬件资源（CPU、RAM、I/O）。

分片本身就是一个完整的搜索引擎，它可以使用单一节点的所有资源。我们拥有6个分片（3个主分片和三个复制分片），最多可以扩展到6个节点，每个节点上有一个分片，每个分片可以100%使用这个节点的资源。


继续扩展
如果我们要扩展到6个以上的节点，要怎么做？

主分片的数量在创建索引时已经确定。实际上，这个数量定义了能存储到索引里数据的最大数量（实际的数量取决于你的数据、硬件和应用场景）。然而，主分片或者复制分片都可以处理读请求——搜索或文档检索，所以数据的冗余越多，我们能处理的搜索吞吐量就越大。

复制分片的数量可以在运行中的集群中动态地变更，这允许我们可以根据需求扩大或者缩小规模。让我们把复制分片的数量从原来的1增加到2：

PUT /blogs/_settings
{
   "number_of_replicas" : 2
}
图5：增加number_of_replicas到2： 三节点两复制集群

从图中可以看出，blogs索引现在有9个分片：3个主分片和6个复制分片。这意味着我们能够扩展到9个节点，再次变成每个节点一个分片。这样使我们的搜索性能相比原始的三节点集群增加三倍。

当然，在同样数量的节点上增加更多的复制分片并不能提高性能，因为这样做的话平均每个分片的所占有的硬件资源就减少了（译者注：大部分请求都聚集到了分片少的节点，导致一个节点吞吐量太大，反而降低性能），你需要增加硬件来提高吞吐量。

不过这些额外的复制节点使我们有更多的冗余：通过以上对节点的设置，我们能够承受两个节点故障而不丢失数据。


应对故障
我们已经说过Elasticsearch可以应对节点失效，所以让我们继续尝试。如果我们杀掉第一个节点的进程（以下简称杀掉节点），我们的集群看起来就像这样：

图5：杀掉第一个节点后的集群

杀掉一个节点后的集群

我们杀掉的节点是一个主节点。一个集群必须要有一个主节点才能使其功能正常，所以集群做的第一件事就是各节点选举了一个新的主节点：Node 2。

主分片1和2在我们杀掉Node 1时已经丢失，我们的索引在丢失主分片时不能正常工作。如果此时我们检查集群健康，我们将看到状态red：不是所有主节点都可用！

幸运的是丢失的两个主分片的完整拷贝存在于其他节点上，所以新主节点做的第一件事是把这些在Node 2和Node 3上的复制分片升级为主分片，这时集群健康回到yellow状态。这个提升是瞬间完成的，就好像按了一下开关。

为什么集群健康状态是yellow而不是green？我们有三个主分片，但是我们指定了每个主分片对应两个复制分片，当前却只有一个复制分片被分配，这就是集群状态无法达到green的原因，不过不用太担心这个：当我们杀掉Node 2，我们的程序依然可以在没有丢失数据的情况下继续运行，因为Node 3还有每个分片的拷贝。

如果我们重启Node 1，集群将能够重新分配丢失的复制分片，集群状况与上一节的 图5：增加number_of_replicas到2 类似。如果Node 1依旧有旧分片的拷贝，它将会尝试再利用它们，它只会从主分片上复制在故障期间有数据变更的那一部分。

现在你应该对分片如何使Elasticsearch可以水平扩展并保证数据安全有了一个清晰的认识。接下来我们将会讨论分片生命周期的更多细节。







数据吞吐
无论程序怎么写，意图是一样的：组织数据为我们的目标所服务。但数据并不只是由随机比特和字节组成，我们在数据节点间建立关联来表示现实世界中的实体或者“某些东西”。属于同一个人的名字和Email地址会有更多的意义。

在现实世界中，并不是所有相同类型的实体看起来都是一样的。一个人可能有一个家庭电话号码，另一个人可能只有一个手机号码，有些人可能两者都有。一个人可能有三个Email地址，其他人可能没有。西班牙人可能有两个姓氏，但是英国人（英语系国家的人）可能只有一个。

面向对象编程语言流行的原因之一，是我们可以用对象来表示和处理现实生活中那些有着潜在关系和复杂结构的实体。到目前为止，这种方式还不错。

但当我们想存储这些实体时问题便来了。传统上，我们以行和列的形式把数据存储在关系型数据库中，相当于使用电子表格。这种固定的存储方式导致对象的灵活性不复存在了。

但是如何能以对象的形式存储对象呢？相对于围绕表格去为我们的程序去建模，我们可以专注于使用数据，把对象本来的灵活性找回来。

对象(object)是一种语言相关，记录在内存中的的数据结构。为了在网络间发送，或者存储它，我们需要一些标准的格式来表示它。JSON (JavaScript Object Notation)是一种可读的以文本来表示对象的方式。它已经成为NoSQL世界中数据交换的一种事实标准。当对象被序列化为JSON，它就成为JSON文档(JSON document)了。

Elasticsearch是一个分布式的文档(document)存储引擎。它可以实时存储并检索复杂数据结构——序列化的JSON文档。换言说，一旦文档被存储在Elasticsearch中，它就可以在集群的任一节点上被检索。

当然，我们不仅需要存储数据，还要快速的批量查询。虽然已经有很多NoSQL的解决方案允许我们以文档的形式存储对象，但它们依旧需要考虑如何查询这些数据，以及哪些字段需要被索引以便检索时更加快速。

在Elasticsearch中，每一个字段的数据都是默认被索引的。也就是说，每个字段专门有一个反向索引用于快速检索。而且，与其它数据库不同，它可以在同一个查询中利用所有的这些反向索引，以惊人的速度返回结果。

在这一章我们将探讨如何使用API来创建、检索、更新和删除文档。目前，我们并不关心数据如何在文档中以及如何查询他们。所有我们关心的是文档如何安全在Elasticsearch中存储，以及如何让它们返回。



什么是文档？
程序中大多的实体或对象能够被序列化为包含键值对的JSON对象，键(key)是字段(field)或属性(property)的名字，值(value)可以是字符串、数字、布尔类型、另一个对象、值数组或者其他特殊类型，比如表示日期的字符串或者表示地理位置的对象。

{
    "name":         "John Smith",
    "age":          42,
    "confirmed":    true,
    "join_date":    "2014-06-01",
    "home": {
        "lat":      51.5,
        "lon":      0.1
    },
    "accounts": [
        {
            "type": "facebook",
            "id":   "johnsmith"
        },
        {
            "type": "twitter",
            "id":   "johnsmith"
        }
    ]
}
通常，我们可以认为对象(object)和文档(document)是等价相通的。不过，他们还是有所差别：对象(Object)是一个JSON结构体——类似于哈希、hashmap、字典或者关联数组；对象(Object)中还可能包含其他对象(Object)。 在Elasticsearch中，文档(document)这个术语有着特殊含义。它特指最顶层结构或者根对象(root object)序列化成的JSON数据（以唯一ID标识并存储于Elasticsearch中）。

文档元数据
一个文档不只有数据。它还包含了元数据(metadata)——关于文档的信息。三个必须的元数据节点是：

节点	说明
_index	文档存储的地方
_type	文档代表的对象的类
_id	文档的唯一标识
_index

索引(index)类似于关系型数据库里的“数据库”——它是我们存储和索引关联数据的地方。

提示：

事实上，我们的数据被存储和索引在分片(shards)中，索引只是一个把一个或多个分片分组在一起的逻辑空间。然而，这只是一些内部细节——我们的程序完全不用关心分片。对于我们的程序而言，文档存储在索引(index)中。剩下的细节由Elasticsearch关心既可。
我们将会在《索引管理》章节中探讨如何创建并管理索引，但现在，我们将让Elasticsearch为我们创建索引。我们唯一需要做的仅仅是选择一个索引名。这个名字必须是全部小写，不能以下划线开头，不能包含逗号。让我们使用website做为索引名。

_type

在应用中，我们使用对象表示一些“事物”，例如一个用户、一篇博客、一个评论，或者一封邮件。每个对象都属于一个类(class)，这个类定义了属性或与对象关联的数据。user类的对象可能包含姓名、性别、年龄和Email地址。

在关系型数据库中，我们经常将相同类的对象存储在一个表里，因为它们有着相同的结构。同理，在Elasticsearch中，我们使用相同类型(type)的文档表示相同的“事物”，因为他们的数据结构也是相同的。

每个类型(type)都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。所有类型下的文档被存储在同一个索引下，但是类型的映射(mapping)会告诉Elasticsearch不同的文档如何被索引。 我们将会在《映射》章节探讨如何定义和管理映射，但是现在我们将依赖Elasticsearch去自动处理数据结构。

_type的名字可以是大写或小写，不能包含下划线或逗号。我们将使用blog做为类型名。

_id

id仅仅是一个字符串，它与_index和_type组合时，就可以在Elasticsearch中唯一标识一个文档。当创建一个文档，你可以自定义_id，也可以让Elasticsearch帮你自动生成。

其它元数据

还有一些其它的元数据，我们将在《映射》章节探讨。使用上面提到的元素，我们已经可以在Elasticsearch中存储文档并通过ID检索——换言说，把Elasticsearch做为文档存储器使用了。


索引一个文档
文档通过index API被索引——使数据可以被存储和搜索。但是首先我们需要决定文档所在。正如我们讨论的，文档通过其_index、_type、_id唯一确定。们可以自己提供一个_id，或者也使用index API 为我们生成一个。

使用自己的ID

如果你的文档有自然的标识符（例如user_account字段或者其他值表示文档），你就可以提供自己的_id，使用这种形式的index API：

PUT /{index}/{type}/{id}
{
  "field": "value",
  ...
}
例如我们的索引叫做“website”，类型叫做“blog”，我们选择的ID是“123”，那么这个索引请求就像这样：

PUT /website/blog/123
{
  "title": "My first blog entry",
  "text":  "Just trying this out...",
  "date":  "2014/01/01"
}
Elasticsearch的响应：

{
   "_index":    "website",
   "_type":     "blog",
   "_id":       "123",
   "_version":  1,
   "created":   true
}
响应指出请求的索引已经被成功创建，这个索引中包含_index、_type和_id元数据，以及一个新元素：_version。

Elasticsearch中每个文档都有版本号，每当文档变化（包括删除）都会使_version增加。在《版本控制》章节中我们将探讨如何使用_version号确保你程序的一部分不会覆盖掉另一部分所做的更改。

自增ID

如果我们的数据没有自然ID，我们可以让Elasticsearch自动为我们生成。请求结构发生了变化：PUT方法——“在这个URL中存储文档”变成了POST方法——"在这个文档下存储文档"。（译者注：原来是把文档存储到某个ID对应的空间，现在是把这个文档添加到某个_type下）。

URL现在只包含_index和_type两个字段：

POST /website/blog/
{
  "title": "My second blog entry",
  "text":  "Still trying this out...",
  "date":  "2014/01/01"
}
响应内容与刚才类似，只有_id字段变成了自动生成的值：

{
   "_index":    "website",
   "_type":     "blog",
   "_id":       "wM0OSFhDQXGZAWDf0-drSA",
   "_version":  1,
   "created":   true
}
自动生成的ID有22个字符长，URL-safe, Base64-encoded string universally unique identifiers, 或者叫 UUIDs。



检索文档
想要从Elasticsearch中获取文档，我们使用同样的_index、_type、_id，但是HTTP方法改为GET：

GET /website/blog/123?pretty
响应包含了现在熟悉的元数据节点，增加了_source字段，它包含了在创建索引时我们发送给Elasticsearch的原始文档。

{
  "_index" :   "website",
  "_type" :    "blog",
  "_id" :      "123",
  "_version" : 1,
  "found" :    true,
  "_source" :  {
      "title": "My first blog entry",
      "text":  "Just trying this out...",
      "date":  "2014/01/01"
  }
}
pretty

在任意的查询字符串中增加pretty参数，类似于上面的例子。会让Elasticsearch美化输出(pretty-print)JSON响应以便更加容易阅读。_source字段不会被美化，它的样子与我们输入的一致。
GET请求返回的响应内容包括{"found": true}。这意味着文档已经找到。如果我们请求一个不存在的文档，依旧会得到一个JSON，不过found值变成了false。

此外，HTTP响应状态码也会变成'404 Not Found'代替'200 OK'。我们可以在curl后加-i参数得到响应头：

curl -i -XGET http://localhost:9200/website/blog/124?pretty
现在响应类似于这样：

HTTP/1.1 404 Not Found
Content-Type: application/json; charset=UTF-8
Content-Length: 83

{
  "_index" : "website",
  "_type" :  "blog",
  "_id" :    "124",
  "found" :  false
}
检索文档的一部分

通常，GET请求将返回文档的全部，存储在_source参数中。但是可能你感兴趣的字段只是title。请求个别字段可以使用_source参数。多个字段可以使用逗号分隔：

GET /website/blog/123?_source=title,text
_source字段现在只包含我们请求的字段，而且过滤了date字段：

{
  "_index" :   "website",
  "_type" :    "blog",
  "_id" :      "123",
  "_version" : 1,
  "exists" :   true,
  "_source" : {
      "title": "My first blog entry" ,
      "text":  "Just trying this out..."
  }
}
或者你只想得到_source字段而不要其他的元数据，你可以这样请求：

GET /website/blog/123/_source
它仅仅返回:

{
   "title": "My first blog entry",
   "text":  "Just trying this out...",
   "date":  "2014/01/01"
}



检查文档是否存在
如果你想做的只是检查文档是否存在——你对内容完全不感兴趣——使用HEAD方法来代替GET。HEAD请求不会返回响应体，只有HTTP头：

curl -i -XHEAD http://localhost:9200/website/blog/123
Elasticsearch将会返回200 OK状态如果你的文档存在：

HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
如果不存在返回404 Not Found：

curl -i -XHEAD http://localhost:9200/website/blog/124
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
当然，这只表示你在查询的那一刻文档不存在，但并不表示几毫秒后依旧不存在。另一个进程在这期间可能创建新文档。


更新整个文档
文档在Elasticsearch中是不可变的——我们不能修改他们。如果需要更新已存在的文档，我们可以使用《索引文档》章节提到的index API 重建索引(reindex) 或者替换掉它。

PUT /website/blog/123
{
  "title": "My first blog entry",
  "text":  "I am starting to get the hang of this...",
  "date":  "2014/01/02"
}
在响应中，我们可以看到Elasticsearch把_version增加了。

{
  "_index" :   "website",
  "_type" :    "blog",
  "_id" :      "123",
  "_version" : 2,
  "created":   false <1>
}
`created`标识为`false`因为同索引、同类型下已经存在同ID的文档。
在内部，Elasticsearch已经标记旧文档为删除并添加了一个完整的新文档。旧版本文档不会立即消失，但你也不能去访问它。Elasticsearch会在你继续索引更多数据时清理被删除的文档。

在本章的后面，我们将会在《局部更新》中探讨update API。这个API 似乎 允许你修改文档的局部，但事实上Elasticsearch遵循与之前所说完全相同的过程，这个过程如下：

从旧文档中检索JSON
修改它
删除旧文档
索引新文档
唯一的不同是update API完成这一过程只需要一个客户端请求既可，不再需要get和index请求了。



创建一个新文档
当索引一个文档，我们如何确定是完全创建了一个新的还是覆盖了一个已经存在的呢？

请记住_index、_type、_id三者唯一确定一个文档。所以要想保证文档是新加入的，最简单的方式是使用POST方法让Elasticsearch自动生成唯一_id：

POST /website/blog/
{ ... }
然而，如果想使用自定义的_id，我们必须告诉Elasticsearch应该在_index、_type、_id三者都不同时才接受请求。为了做到这点有两种方法，它们其实做的是同一件事情。你可以选择适合自己的方式：

第一种方法使用op_type查询参数：

PUT /website/blog/123?op_type=create
{ ... }
或者第二种方法是在URL后加/_create做为端点：

PUT /website/blog/123/_create
{ ... }
如果请求成功的创建了一个新文档，Elasticsearch将返回正常的元数据且响应状态码是201 Created。

另一方面，如果包含相同的_index、_type和_id的文档已经存在，Elasticsearch将返回409 Conflict响应状态码，错误信息类似如下：

{
  "error" : "DocumentAlreadyExistsException[[website][4] [blog][123]:
             document already exists]",
  "status" : 409
}



删除文档
删除文档的语法模式与之前基本一致，只不过要使用DELETE方法：

DELETE /website/blog/123
如果文档被找到，Elasticsearch将返回200 OK状态码和以下响应体。注意_version数字已经增加了。

{
  "found" :    true,
  "_index" :   "website",
  "_type" :    "blog",
  "_id" :      "123",
  "_version" : 3
}
如果文档未找到，我们将得到一个404 Not Found状态码，响应体是这样的：

{
  "found" :    false,
  "_index" :   "website",
  "_type" :    "blog",
  "_id" :      "123",
  "_version" : 4
}
尽管文档不存在——"found"的值是false——_version依旧增加了。这是内部记录的一部分，它确保在多节点间不同操作可以有正确的顺序。

正如在《更新文档》一章中提到的，删除一个文档也不会立即从磁盘上移除，它只是被标记成已删除。Elasticsearch将会在你之后添加更多索引的时候才会在后台进行删除内容的清理。






处理冲突
当使用index API更新文档的时候，我们读取原始文档，做修改，然后将整个文档(whole document)一次性重新索引。最近的索引请求会生效——Elasticsearch中只存储最后被索引的任何文档。如果其他人同时也修改了这个文档，他们的修改将会丢失。

很多时候，这并不是一个问题。或许我们主要的数据存储在关系型数据库中，然后拷贝数据到Elasticsearch中只是为了可以用于搜索。或许两个人同时修改文档的机会很少。亦或者偶尔的修改丢失对于我们的工作来说并无大碍。

但有时丢失修改是一个很严重的问题。想象一下我们使用Elasticsearch存储大量在线商店的库存信息。每当销售一个商品，Elasticsearch中的库存就要减一。

一天，老板决定做一个促销。瞬间，我们每秒就销售了几个商品。想象两个同时运行的web进程，两者同时处理一件商品的订单：

img-data-lww

web_1让stock_count失效是因为web_2没有察觉到stock_count的拷贝已经过期（译者注：web_1取数据，减一后更新了stock_count。可惜在web_1更新stock_count前它就拿到了数据，这个数据已经是过期的了，当web_2再回来更新stock_count时这个数字就是错的。这样就会造成看似卖了一件东西，其实是卖了两件，这个应该属于幻读。）。结果是我们认为自己确实还有更多的商品，最终顾客会因为销售给他们没有的东西而失望。

变化越是频繁，或读取和更新间的时间越长，越容易丢失我们的更改。

在数据库中，有两种通用的方法确保在并发更新时修改不丢失：

悲观并发控制（Pessimistic concurrency control）

这在关系型数据库中被广泛的使用，假设冲突的更改经常发生，为了解决冲突我们把访问区块化。典型的例子是在读一行数据前锁定这行，然后确保只有加锁的那个线程可以修改这行数据。

乐观并发控制（Optimistic concurrency control）：

被Elasticsearch使用，假设冲突不经常发生，也不区块化访问，然而，如果在读写过程中数据发生了变化，更新操作将失败。这时候由程序决定在失败后如何解决冲突。实际情况中，可以重新尝试更新，刷新数据（重新读取）或者直接反馈给用户。

乐观并发控制
Elasticsearch是分布式的。当文档被创建、更新或删除，文档的新版本会被复制到集群的其它节点。Elasticsearch即是同步的又是异步的，意思是这些复制请求都是平行发送的，并无序(out of sequence)的到达目的地。这就需要一种方法确保老版本的文档永远不会覆盖新的版本。

上文我们提到index、get、delete请求时，我们指出每个文档都有一个_version号码，这个号码在文档被改变时加一。Elasticsearch使用这个_version保证所有修改都被正确排序。当一个旧版本出现在新版本之后，它会被简单的忽略。

我们利用_version的这一优点确保数据不会因为修改冲突而丢失。我们可以指定文档的version来做想要的更改。如果那个版本号不是现在的，我们的请求就失败了。

Let's create a new blog post: 让我们创建一个新的博文：

PUT /website/blog/1/_create
{
  "title": "My first blog entry",
  "text":  "Just trying this out..."
}
响应体告诉我们这是一个新建的文档，它的_version是1。现在假设我们要编辑这个文档：把数据加载到web表单中，修改，然后保存成新版本。

首先我们检索文档：

GET /website/blog/1
响应体包含相同的_version是1

{
  "_index" :   "website",
  "_type" :    "blog",
  "_id" :      "1",
  "_version" : 1,
  "found" :    true,
  "_source" :  {
      "title": "My first blog entry",
      "text":  "Just trying this out..."
  }
}
现在，当我们通过重新索引文档保存修改时，我们这样指定了version参数：

PUT /website/blog/1?version=1 <1>
{
  "title": "My first blog entry",
  "text":  "Starting to get the hang of this..."
}
我们只希望文档的`_version`是`1`时更新才生效。
This request succeeds, and the response body tells us that the _version has been incremented to 2:

请求成功，响应体告诉我们_version已经增加到2：

{
  "_index":   "website",
  "_type":    "blog",
  "_id":      "1",
  "_version": 2
  "created":  false
}
然而，如果我们重新运行相同的索引请求，依旧指定version=1，Elasticsearch将返回409 Conflict状态的HTTP响应。响应体类似这样：

{
  "error" : "VersionConflictEngineException[[website][2] [blog][1]:
             version conflict, current [2], provided [1]]",
  "status" : 409
}
这告诉我们当前_version是2，但是我们指定想要更新的版本是1。

我们需要做什么取决于程序的需求。我们可以告知用户其他人修改了文档，你应该在保存前再看一下。而对于上文提到的商品stock_count，我们需要重新检索最新文档然后申请新的更改操作。

所有更新和删除文档的请求都接受version参数，它可以允许在你的代码中增加乐观锁控制。

使用外部版本控制系统
一种常见的结构是使用一些其他的数据库做为主数据库，然后使用Elasticsearch搜索数据，这意味着所有主数据库发生变化，就要将其拷贝到Elasticsearch中。如果有多个进程负责这些数据的同步，就会遇到上面提到的并发问题。

如果主数据库有版本字段——或一些类似于timestamp等可以用于版本控制的字段——是你就可以在Elasticsearch的查询字符串后面添加version_type=external来使用这些版本号。版本号必须是整数，大于零小于9.2e+18——Java中的正的long。

外部版本号与之前说的内部版本号在处理的时候有些不同。它不再检查_version是否与请求中指定的一致，而是检查是否小于指定的版本。如果请求成功，外部版本号就会被存储到_version中。

外部版本号不仅在索引和删除请求中指定，也可以在创建(create)新文档中指定。

例如，创建一个包含外部版本号5的新博客，我们可以这样做：

PUT /website/blog/2?version=5&version_type=external
{
  "title": "My first external blog entry",
  "text":  "Starting to get the hang of this..."
}
在响应中，我们能看到当前的_version号码是5：

{
  "_index":   "website",
  "_type":    "blog",
  "_id":      "2",
  "_version": 5,
  "created":  true
}
现在我们更新这个文档，指定一个新version号码为10：

PUT /website/blog/2?version=10&version_type=external
{
  "title": "My first external blog entry",
  "text":  "This is a piece of cake..."
}
请求成功的设置了当前_version为10：

{
  "_index":   "website",
  "_type":    "blog",
  "_id":      "2",
  "_version": 10,
  "created":  false
}
如果你重新运行这个请求，就会返回一个像之前一样的冲突错误，因为指定的外部版本号不大于当前在Elasticsearch中的版本。



文档局部更新
在《更新文档》一章，我们说了一种通过检索，修改，然后重建整文档的索引方法来更新文档。这是对的。然而，使用update API，我们可以使用一个请求来实现局部更新，例如增加数量的操作。

我们也说过文档是不可变的——它们不能被更改，只能被替换。update API必须遵循相同的规则。表面看来，我们似乎是局部更新了文档的位置，内部却是像我们之前说的一样简单的使用update API处理相同的检索-修改-重建索引流程，我们也减少了其他进程可能导致冲突的修改。

最简单的update请求表单接受一个局部文档参数doc，它会合并到现有文档中——对象合并在一起，存在的标量字段被覆盖，新字段被添加。举个例子，我们可以使用以下请求为博客添加一个tags字段和一个views字段：

POST /website/blog/1/_update
{
   "doc" : {
      "tags" : [ "testing" ],
      "views": 0
   }
}
如果请求成功，我们将看到类似index请求的响应结果：

{
   "_index" :   "website",
   "_id" :      "1",
   "_type" :    "blog",
   "_version" : 3
}
检索文档文档显示被更新的_source字段：

{
   "_index":    "website",
   "_type":     "blog",
   "_id":       "1",
   "_version":  3,
   "found":     true,
   "_source": {
      "title":  "My first blog entry",
      "text":   "Starting to get the hang of this...",
      "tags": [ "testing" ], <1>
      "views":  0 <1>
   }
}
我们新添加的字段已经被添加到`_source`字段中。
使用脚本局部更新

使用Groovy脚本

这时候当API不能满足要求时，Elasticsearch允许你使用脚本实现自己的逻辑。脚本支持非常多的API，例如搜索、排序、聚合和文档更新。脚本可以通过请求的一部分、检索特殊的.scripts索引或者从磁盘加载方式执行。

默认的脚本语言是Groovy，一个快速且功能丰富的脚本语言，语法类似于Javascript。它在一个沙盒(sandbox)中运行，以防止恶意用户毁坏Elasticsearch或攻击服务器。

你可以在《脚本参考文档》中获得更多信息。
脚本能够使用update API改变_source字段的内容，它在脚本内部以ctx._source表示。例如，我们可以使用脚本增加博客的views数量：

POST /website/blog/1/_update
{
   "script" : "ctx._source.views+=1"
}
我们还可以使用脚本增加一个新标签到tags数组中。在这个例子中，我们定义了一个新标签做为参数而不是硬编码在脚本里。这允许Elasticsearch未来可以重复利用脚本，而不是在想要增加新标签时必须每次编译新脚本：

POST /website/blog/1/_update
{
   "script" : "ctx._source.tags+=new_tag",
   "params" : {
      "new_tag" : "search"
   }
}
获取最后两个有效请求的文档：

{
   "_index":    "website",
   "_type":     "blog",
   "_id":       "1",
   "_version":  5,
   "found":     true,
   "_source": {
      "title":  "My first blog entry",
      "text":   "Starting to get the hang of this...",
      "tags":  ["testing", "search"], <1>
      "views":  1 <2>
   }
}
`search`标签已经被添加到`tags`数组。
`views`字段已经被增加。
通过设置ctx.op为delete我们可以根据内容删除文档：

POST /website/blog/1/_update
{
   "script" : "ctx.op = ctx._source.views == count ? 'delete' : 'none'",
    "params" : {
        "count": 1
    }
}
更新可能不存在的文档

想象我们要在Elasticsearch中存储浏览量计数器。每当有用户访问页面，我们增加这个页面的浏览量。但如果这是个新页面，我们并不确定这个计数器存在与否。当我们试图更新一个不存在的文档，更新将失败。

在这种情况下，我们可以使用upsert参数定义文档来使其不存在时被创建。

POST /website/pageviews/1/_update
{
   "script" : "ctx._source.views+=1",
   "upsert": {
       "views": 1
   }
}
第一次执行这个请求，upsert值被索引为一个新文档，初始化views字段为1.接下来文档已经存在，所以script被更新代替，增加views数量。

更新和冲突

这这一节的介绍中，我们介绍了如何在检索(retrieve)和重建索引(reindex)中保持更小的窗口，如何减少冲突性变更发生的概率，不过这些无法被完全避免，像一个其他进程在update进行重建索引时修改了文档这种情况依旧可能发生。

为了避免丢失数据，update API在检索(retrieve)阶段检索文档的当前_version，然后在重建索引(reindex)阶段通过index请求提交。如果其他进程在检索(retrieve)和重加索引(reindex)阶段修改了文档，_version将不能被匹配，然后更新失败。

对于多用户的局部更新，文档被修改了并不要紧。例如，两个进程都要增加页面浏览量，增加的顺序我们并不关心——如果冲突发生，我们唯一要做的仅仅是重新尝试更新既可。

这些可以通过retry_on_conflict参数设置重试次数来自动完成，这样update操作将会在发生错误前重试——这个值默认为0。

POST /website/pageviews/1/_update?retry_on_conflict=5 <1>
{
   "script" : "ctx._source.views+=1",
   "upsert": {
       "views": 0
   }
}
在错误发生前重试更新5次
这适用于像增加计数这种顺序无关的操作，但是还有一种顺序非常重要的情况。例如index API，使用“保留最后更新(last-write-wins)”的update API，但它依旧接受一个version参数以允许你使用乐观并发控制(optimistic concurrency control)来指定你要更细文档的版本。




检索多个文档
像Elasticsearch一样，检索多个文档依旧非常快。合并多个请求可以避免每个请求单独的网络开销。如果你需要从Elasticsearch中检索多个文档，相对于一个一个的检索，更快的方式是在一个请求中使用multi-get或者mget API。

mget API参数是一个docs数组，数组的每个节点定义一个文档的_index、_type、_id元数据。如果你只想检索一个或几个确定的字段，也可以定义一个_source参数：

POST /_mget
{
   "docs" : [
      {
         "_index" : "website",
         "_type" :  "blog",
         "_id" :    2
      },
      {
         "_index" : "website",
         "_type" :  "pageviews",
         "_id" :    1,
         "_source": "views"
      }
   ]
}
响应体也包含一个docs数组，每个文档还包含一个响应，它们按照请求定义的顺序排列。每个这样的响应与单独使用get request响应体相同：

{
   "docs" : [
      {
         "_index" :   "website",
         "_id" :      "2",
         "_type" :    "blog",
         "found" :    true,
         "_source" : {
            "text" :  "This is a piece of cake...",
            "title" : "My first external blog entry"
         },
         "_version" : 10
      },
      {
         "_index" :   "website",
         "_id" :      "1",
         "_type" :    "pageviews",
         "found" :    true,
         "_version" : 2,
         "_source" : {
            "views" : 2
         }
      }
   ]
}
如果你想检索的文档在同一个_index中（甚至在同一个_type中），你就可以在URL中定义一个默认的/_index或者/_index/_type。

你依旧可以在单独的请求中使用这些值：

POST /website/blog/_mget
{
   "docs" : [
      { "_id" : 2 },
      { "_type" : "pageviews", "_id" :   1 }
   ]
}
事实上，如果所有文档具有相同_index和_type，你可以通过简单的ids数组来代替完整的docs数组：

POST /website/blog/_mget
{
   "ids" : [ "2", "1" ]
}
注意到我们请求的第二个文档并不存在。我们定义了类型为blog，但是ID为1的文档类型为pageviews。这个不存在的文档会在响应体中被告知。

{
  "docs" : [
    {
      "_index" :   "website",
      "_type" :    "blog",
      "_id" :      "2",
      "_version" : 10,
      "found" :    true,
      "_source" : {
        "title":   "My first external blog entry",
        "text":    "This is a piece of cake..."
      }
    },
    {
      "_index" :   "website",
      "_type" :    "blog",
      "_id" :      "1",
      "found" :    false  <1>
    }
  ]
}
这个文档不存在
事实上第二个文档不存在并不影响第一个文档的检索。每个文档的检索和报告都是独立的。

注意：

尽管前面提到有一个文档没有被找到，但HTTP请求状态码还是200。事实上，就算所有文档都找不到，请求也还是返回200，原因是mget请求本身成功了。如果想知道每个文档是否都成功了，你需要检查found标志。




更省时的批量操作
就像mget允许我们一次性检索多个文档一样，bulk API允许我们使用单一请求来实现多个文档的create、index、update或delete。这对索引类似于日志活动这样的数据流非常有用，它们可以以成百上千的数据为一个批次按序进行索引。

bulk请求体如下，它有一点不同寻常：

{ action: { metadata }}\n
{ request body        }\n
{ action: { metadata }}\n
{ request body        }\n
...
这种格式类似于用"\n"符号连接起来的一行一行的JSON文档流(stream)。两个重要的点需要注意：

每行必须以"\n"符号结尾，包括最后一行。这些都是作为每行有效的分离而做的标记。

每一行的数据不能包含未被转义的换行符，它们会干扰分析——这意味着JSON不能被美化打印。
提示:

在《批量格式》一章我们介绍了为什么bulk API使用这种格式。
action/metadata这一行定义了文档行为(what action)发生在哪个文档(which document)之上。

行为(action)必须是以下几种：

行为	解释
create	当文档不存在时创建之。详见《创建文档》
index	创建新文档或替换已有文档。见《索引文档》和《更新文档》
update	局部更新文档。见《局部更新》
delete	删除一个文档。见《删除文档》
在索引、创建、更新或删除时必须指定文档的_index、_type、_id这些元数据(metadata)。

例如删除请求看起来像这样：

{ "delete": { "_index": "website", "_type": "blog", "_id": "123" }}
请求体(request body)由文档的_source组成——文档所包含的一些字段以及其值。它被index和create操作所必须，这是有道理的：你必须提供文档用来索引。

这些还被update操作所必需，而且请求体的组成应该与update API（doc, upsert, script等等）一致。删除操作不需要请求体(request body)。

{ "create":  { "_index": "website", "_type": "blog", "_id": "123" }}
{ "title":    "My first blog post" }
如果定义_id，ID将会被自动创建：

{ "index": { "_index": "website", "_type": "blog" }}
{ "title":    "My second blog post" }
为了将这些放在一起，bulk请求表单是这样的：

POST /_bulk
{ "delete": { "_index": "website", "_type": "blog", "_id": "123" }} <1>
{ "create": { "_index": "website", "_type": "blog", "_id": "123" }}
{ "title":    "My first blog post" }
{ "index":  { "_index": "website", "_type": "blog" }}
{ "title":    "My second blog post" }
{ "update": { "_index": "website", "_type": "blog", "_id": "123", "_retry_on_conflict" : 3} }
{ "doc" : {"title" : "My updated blog post"} } <2>
注意`delete`**行为(action)**没有请求体，它紧接着另一个**行为(action)**
记得最后一个换行符
Elasticsearch响应包含一个items数组，它罗列了每一个请求的结果，结果的顺序与我们请求的顺序相同：

{
   "took": 4,
   "errors": false, <1>
   "items": [
      {  "delete": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "_version": 2,
            "status":   200,
            "found":    true
      }},
      {  "create": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "_version": 3,
            "status":   201
      }},
      {  "create": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "EiwfApScQiiy7TIKFxRCTw",
            "_version": 1,
            "status":   201
      }},
      {  "update": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "_version": 4,
            "status":   200
      }}
   ]
}}
所有子请求都成功完成。
每个子请求都被独立的执行，所以一个子请求的错误并不影响其它请求。如果任何一个请求失败，顶层的error标记将被设置为true，然后错误的细节将在相应的请求中被报告：

POST /_bulk
{ "create": { "_index": "website", "_type": "blog", "_id": "123" }}
{ "title":    "Cannot create - it already exists" }
{ "index":  { "_index": "website", "_type": "blog", "_id": "123" }}
{ "title":    "But we can update it" }
响应中我们将看到create文档123失败了，因为文档已经存在，但是后来的在123上执行的index请求成功了：

{
   "took": 3,
   "errors": true, <1>
   "items": [
      {  "create": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "status":   409, <2>
            "error":    "DocumentAlreadyExistsException <3>
                        [[website][4] [blog][123]:
                        document already exists]"
      }},
      {  "index": {
            "_index":   "website",
            "_type":    "blog",
            "_id":      "123",
            "_version": 5,
            "status":   200 <4>
      }}
   ]
}
一个或多个请求失败。
这个请求的HTTP状态码被报告为`409 CONFLICT`。
错误消息说明了什么请求错误。
第二个请求成功了，状态码是`200 OK`。
这些说明bulk请求不是原子操作——它们不能实现事务。每个请求操作时分开的，所以每个请求的成功与否不干扰其它操作。

不要重复
你可能在同一个index下的同一个type里批量索引日志数据。为每个文档指定相同的元数据是多余的。就像mget API，bulk请求也可以在URL中使用/_index或/_index/_type:

POST /website/_bulk
{ "index": { "_type": "log" }}
{ "event": "User logged in" }
你依旧可以覆盖元数据行的_index和_type，在没有覆盖时它会使用URL中的值作为默认值：

POST /website/log/_bulk
{ "index": {}}
{ "event": "User logged in" }
{ "index": { "_type": "blog" }}
{ "title": "Overriding the default type" }
多大才算太大？
整个批量请求需要被加载到接受我们请求节点的内存里，所以请求越大，给其它请求可用的内存就越小。有一个最佳的bulk请求大小。超过这个大小，性能不再提升而且可能降低。

最佳大小，当然并不是一个固定的数字。它完全取决于你的硬件、你文档的大小和复杂度以及索引和搜索的负载。幸运的是，这个最佳点(sweetspot)还是容易找到的：

试着批量索引标准的文档，随着大小的增长，当性能开始降低，说明你每个批次的大小太大了。开始的数量可以在1000~5000个文档之间，如果你的文档非常大，可以使用较小的批次。

通常着眼于你请求批次的物理大小是非常有用的。一千个1kB的文档和一千个1MB的文档大不相同。一个好的批次最好保持在5-15MB大小间。










分布式文档存储
在上一章，我们看到了将数据放入索引然后检索它们的所有方法。不过我们有意略过了许多关于数据是如何在集群中分布和获取的相关技术细节。这种使用和细节分离是刻意为之的——你不需要知道数据在Elasticsearch如何分布它就会很好的工作。

这一章我们深入这些内部细节来帮助你更好的理解数据是如何在分布式系统中存储的。

注意：

下面的信息只是出于兴趣阅读，你不必为了使用Elasticsearch而弄懂和记住所有的细节。讨论的这些选项只提供给高级用户。

阅读这一部分只是让你了解下系统如何工作，并让你知道这些信息以备以后参考，所以不要被细节吓到。


路由文档到分片
当你索引一个文档，它被存储在单独一个主分片上。Elasticsearch是如何知道文档属于哪个分片的呢？当你创建一个新文档，它是如何知道是应该存储在分片1还是分片2上的呢？

进程不能是随机的，因为我们将来要检索文档。事实上，它根据一个简单的算法决定：

shard = hash(routing) % number_of_primary_shards
routing值是一个任意字符串，它默认是_id但也可以自定义。这个routing字符串通过哈希函数生成一个数字，然后除以主切片的数量得到一个余数(remainder)，余数的范围永远是0到number_of_primary_shards - 1，这个数字就是特定文档所在的分片。

这也解释了为什么主分片的数量只能在创建索引时定义且不能修改：如果主分片的数量在未来改变了，所有先前的路由值就失效了，文档也就永远找不到了。

有时用户认为固定数量的主分片会让之后的扩展变得很困难。现实中，有些技术会在你需要的时候让扩展变得容易。我们将在《扩展》章节讨论。
所有的文档API（get、index、delete、bulk、update、mget）都接收一个routing参数，它用来自定义文档到分片的映射。自定义路由值可以确保所有相关文档——例如属于同一个人的文档——被保存在同一分片上。我们将在《扩展》章节说明你为什么需要这么做。



主分片和复制分片如何交互
为了阐述意图，我们假设有三个节点的集群。它包含一个叫做bblogs的索引并拥有两个主分片。每个主分片有两个复制分片。相同的分片不会放在同一个节点上，所以我们的集群是这样的：

有三个节点一个索引的集群

我们能够发送请求给集群中任意一个节点。每个节点都有能力处理任意请求。每个节点都知道任意文档所在的节点，所以也可以将请求转发到需要的节点。下面的例子中，我们将发送所有请求给Node 1，这个节点我们将会称之为请求节点(requesting node)

提示：

当我们发送请求，最好的做法是循环通过所有节点请求，这样可以平衡负载。



新建、索引和删除文档
新建、索引和删除请求都是写(write)操作，它们必须在主分片上成功完成才能复制到相关的复制分片上。

新建、索引或删除单一文档

下面我们罗列在主分片和复制分片上成功新建、索引或删除一个文档必要的顺序步骤：

客户端给Node 1发送新建、索引或删除请求。
节点使用文档的_id确定文档属于分片0。它转发请求到Node 3，分片0位于这个节点上。
Node 3在主分片上执行请求，如果成功，它转发请求到相应的位于Node 1和Node 2的复制节点上。当所有的复制节点报告成功，Node 3报告成功到请求的节点，请求的节点再报告给客户端。
客户端接收到成功响应的时候，文档的修改已经被应用于主分片和所有的复制分片。你的修改生效了。

有很多可选的请求参数允许你更改这一过程。你可能想牺牲一些安全来提高性能。这一选项很少使用因为Elasticsearch已经足够快，不过为了内容的完整我们将做一些阐述。

replication

复制默认的值是sync。这将导致主分片得到复制分片的成功响应后才返回。

如果你设置replication为async，请求在主分片上被执行后就会返回给客户端。它依旧会转发请求给复制节点，但你将不知道复制节点成功与否。

上面的这个选项不建议使用。默认的sync复制允许Elasticsearch强制反馈传输。async复制可能会因为在不等待其它分片就绪的情况下发送过多的请求而使Elasticsearch过载。

consistency

默认主分片在尝试写入时需要规定数量(quorum)或过半的分片（可以是主节点或复制节点）可用。这是防止数据被写入到错的网络分区。规定的数量计算公式如下：

int( (primary + number_of_replicas) / 2 ) + 1
consistency允许的值为one（只有一个主分片），all（所有主分片和复制分片）或者默认的quorum或过半分片。

注意number_of_replicas是在索引中的的设置，用来定义复制分片的数量，而不是现在活动的复制节点的数量。如果你定义了索引有3个复制节点，那规定数量是：

int( (primary + 3 replicas) / 2 ) + 1 = 3
但如果你只有2个节点，那你的活动分片不够规定数量，也就不能索引或删除任何文档。

timeout

当分片副本不足时会怎样？Elasticsearch会等待更多的分片出现。默认等待一分钟。如果需要，你可以设置timeout参数让它终止的更早：100表示100毫秒，30s表示30秒。

注意：

新索引默认有1个复制分片，这意味着为了满足quorum的要求需要两个活动的分片。当然，这个默认设置将阻止我们在单一节点集群中进行操作。为了避开这个问题，规定数量只有在number_of_replicas大于一时才生效。



检索文档
文档能够从主分片或任意一个复制分片被检索。

检索一个文档

下面我们罗列在主分片或复制分片上检索一个文档必要的顺序步骤：

客户端给Node 1发送get请求。
节点使用文档的_id确定文档属于分片0。分片0对应的复制分片在三个节点上都有。此时，它转发请求到Node 2。
Node 2返回endangered给Node 1然后返回给客户端。
对于读请求，为了平衡负载，请求节点会为每个请求选择不同的分片——它会循环所有分片副本。

可能的情况是，一个被索引的文档已经存在于主分片上却还没来得及同步到复制分片上。这时复制分片会报告文档未找到，主分片会成功返回文档。一旦索引请求成功返回给用户，文档则在主分片和复制分片都是可用的。



局部更新文档
update API 结合了之前提到的读和写的模式。

局部更新文档

下面我们罗列执行局部更新必要的顺序步骤：

客户端给Node 1发送更新请求。
它转发请求到主分片所在节点Node 3。
Node 3从主分片检索出文档，修改_source字段的JSON，然后在主分片上重建索引。如果有其他进程修改了文档，它以retry_on_conflict设置的次数重复步骤3，都未成功则放弃。
如果Node 3成功更新文档，它同时转发文档的新版本到Node 1和Node 2上的复制节点以重建索引。当所有复制节点报告成功，Node 3返回成功给请求节点，然后返回给客户端。
update API还接受《新建、索引和删除》章节提到的routing、replication、consistency和timout参数。

基于文档的复制

当主分片转发更改给复制分片时，并不是转发更新请求，而是转发整个文档的新版本。记住这些修改转发到复制节点是异步的，它们并不能保证到达的顺序与发送相同。如果Elasticsearch转发的仅仅是修改请求，修改的顺序可能是错误的，那得到的就是个损坏的文档。



多文档模式
mget和bulk API与单独的文档类似。差别是请求节点知道每个文档所在的分片。它把多文档请求拆成每个分片的对文档请求，然后转发每个参与的节点。

一旦接收到每个节点的应答，然后整理这些响应组合为一个单独的响应，最后返回给客户端。

通过mget检索多个文档

下面我们将罗列通过一个mget请求检索多个文档的顺序步骤：

客户端向Node 1发送mget请求。
Node 1为每个分片构建一个多条数据检索请求，然后转发到这些请求所需的主分片或复制分片上。当所有回复被接收，Node 1构建响应并返回给客户端。
routing 参数可以被docs中的每个文档设置。

通过打包批量修改文档

下面我们将罗列使用一个bulk执行多个create、index、delete和update请求的顺序步骤：

客户端向Node 1发送bulk请求。
Node 1为每个分片构建批量请求，然后转发到这些请求所需的主分片上。
主分片一个接一个的按序执行操作。当一个操作执行完，主分片转发新文档（或者删除部分）给对应的复制节点，然后执行下一个操作。复制节点为报告所有操作完成，节点报告给请求节点，请求节点整理响应并返回给客户端。
bulk API还可以在最上层使用replication和consistency参数，routing参数则在每个请求的元数据中使用。



为什么是奇怪的格式？
当我们在《批量》一章中学习了批量请求后，你可能会问：“为什么bulk API需要带换行符的奇怪格式，而不是像mget API一样使用JSON数组？”

为了回答这个问题，我们需要简单的介绍一下背景：

批量中每个引用的文档属于不同的主分片，每个分片可能被分布于集群中的某个节点上。这意味着批量中的每个操作(action)需要被转发到对应的分片和节点上。

如果每个单独的请求被包装到JSON数组中，那意味着我们需要：

解析JSON为数组（包括文档数据，可能非常大）
检查每个请求决定应该到哪个分片上
为每个分片创建一个请求的数组
序列化这些数组为内部传输格式
发送请求到每个分片
这可行，但需要大量的RAM来承载本质上相同的数据，还要创建更多的数据结构使得JVM花更多的时间执行垃圾回收。

取而代之的，Elasticsearch则是从网络缓冲区中一行一行的直接读取数据。它使用换行符识别和解析action/metadata行，以决定哪些分片来处理这个请求。

这些行请求直接转发到对应的分片上。这些没有冗余复制，没有多余的数据结构。整个请求过程使用最小的内存在进行。









搜索——基本的工具
到目前为止，我们已经学会了如何使用elasticsearch作为一个简单的NoSQL风格的分布式文件存储器——我们可以将一个JSON文档扔给Elasticsearch，也可以根据ID检索它们。但Elasticsearch真正强大之处在于可以从混乱的数据中找出有意义的信息——从大数据到全面的信息。

这也是为什么我们使用结构化的JSON文档，而不是无结构的二进制数据。Elasticsearch不只会存储(store)文档，也会索引(indexes)文档内容来使之可以被搜索。

每个文档里的字段都会被索引并被查询。而且不仅如此。在简单查询时，Elasticsearch可以使用所有的索引，以非常快的速度返回结果。这让你永远不必考虑传统数据库的一些东西。

A search can be: 搜索(search)可以：

在类似于gender或者age这样的字段上使用结构化查询，join_date这样的字段上使用排序，就像SQL的结构化查询一样。
全文检索，可以使用所有字段来匹配关键字，然后按照关联性(relevance)排序返回结果。
或者结合以上两条。
很多搜索都是开箱即用的，为了充分挖掘Elasticsearch的潜力，你需要理解以下三个概念：

概念	解释
映射(Mapping)	数据在每个字段中的解释说明
分析(Analysis)	全文是如何处理的可以被搜索的
领域特定语言查询(Query DSL)	Elasticsearch使用的灵活的、强大的查询语言
以上提到的每个点都是一个巨大的话题，我们将在《深入搜索》一章阐述它们。本章节我们将介绍这三点的一些基本概念——仅仅帮助你大致了解搜索是如何工作的。

我们将使用最简单的形式开始介绍search API.

测试数据

本章节测试用的数据可以在这里被找到https://gist.github.com/clintongormley/8579281

你可以把这些命令复制到终端中执行以便可以实践本章的例子。


空搜索

最基本的搜索API表单是空搜索(empty search)，它没有指定任何的查询条件，只返回集群索引中的所有文档：

GET /_search
响应内容（为了编辑简洁）类似于这样：

{
   "hits" : {
      "total" :       14,
      "hits" : [
        {
          "_index":   "us",
          "_type":    "tweet",
          "_id":      "7",
          "_score":   1,
          "_source": {
             "date":    "2014-09-17",
             "name":    "John Smith",
             "tweet":   "The Query DSL is really powerful and flexible",
             "user_id": 2
          }
       },
        ... 9 RESULTS REMOVED ...
      ],
      "max_score" :   1
   },
   "took" :           4,
   "_shards" : {
      "failed" :      0,
      "successful" :  10,
      "total" :       10
   },
   "timed_out" :      false
}
hits
响应中最重要的部分是hits，它包含了total字段来表示匹配到的文档总数，hits数组还包含了匹配到的前10条数据。

hits数组中的每个结果都包含_index、_type和文档的_id字段，被加入到_source字段中这意味着在搜索结果中我们将可以直接使用全部文档。这不像其他搜索引擎只返回文档ID，需要你单独去获取文档。

每个节点都有一个_score字段，这是相关性得分(relevance score)，它衡量了文档与查询的匹配程度。默认的，返回的结果中关联性最大的文档排在首位；这意味着，它是按照_score降序排列的。这种情况下，我们没有指定任何查询，所以所有文档的相关性是一样的，因此所有结果的_score都是取得一个中间值1

max_score指的是所有文档匹配查询中_score的最大值。

took
took告诉我们整个搜索请求花费的毫秒数。

shards
_shards节点告诉我们参与查询的分片数（total字段），有多少是成功的（successful字段），有多少的是失败的（failed字段）。通常我们不希望分片失败，不过这个有可能发生。如果我们遭受一些重大的故障导致主分片和复制分片都故障，那这个分片的数据将无法响应给搜索请求。这种情况下，Elasticsearch将报告分片failed，但仍将继续返回剩余分片上的结果。

timeout
time_out值告诉我们查询超时与否。一般的，搜索请求不会超时。如果响应速度比完整的结果更重要，你可以定义timeout参数为10或者10ms（10毫秒），或者1s（1秒）

GET /_search?timeout=10ms
Elasticsearch将返回在请求超时前收集到的结果。

超时不是一个断路器（circuit breaker）（译者注：关于断路器的理解请看警告）。

警告
需要注意的是timeout不会停止执行查询，它仅仅告诉你目前顺利返回结果的节点然后关闭连接。在后台，其他分片可能依旧执行查询，尽管结果已经被发送。

使用超时是因为对于你的业务需求（译者注：SLA，Service-Level Agreement服务等级协议，在此我翻译为业务需求）来说非常重要，而不是因为你想中断执行长时间运行的查询。



多索引和多类别
你注意到空搜索的结果中不同类型的文档——user和tweet——来自于不同的索引——us和gb。

通过限制搜索的不同索引或类型，我们可以在集群中跨所有文档搜索。Elasticsearch转发搜索请求到集群中平行的主分片或每个分片的复制分片上，收集结果后选择顶部十个返回给我们。

通常，当然，你可能想搜索一个或几个自定的索引或类型，我们能通过定义URL中的索引或类型达到这个目的，像这样：

/_search

在所有索引的所有类型中搜索

/gb/_search

在索引gb的所有类型中搜索

/gb,us/_search

在索引gb和us的所有类型中搜索

/g*,u*/_search

在以g或u开头的索引的所有类型中搜索

/gb/user/_search

在索引gb的类型user中搜索

/gb,us/user,tweet/_search

在索引gb和us的类型为user和tweet中搜索

/_all/user,tweet/_search

在所有索引的user和tweet中搜索 search types user and tweet in all indices

当你搜索包含单一索引时，Elasticsearch转发搜索请求到这个索引的主分片或每个分片的复制分片上，然后聚集每个分片的结果。搜索包含多个索引也是同样的方式——只不过或有更多的分片被关联。

重要
搜索一个索引有5个主分片和5个索引各有一个分片事实上是一样的。
接下来，你将看到这些简单的情况如何灵活的扩展以适应你需求的变更。



分页
《空搜索》一节告诉我们在集群中有14个文档匹配我们的（空）搜索语句。但是只有10个文档在hits数组中。我们如何看到其他文档？

和SQL使用LIMIT关键字返回只有一页的结果一样，Elasticsearch接受from和size参数：

size: 结果数，默认10

from: 跳过开始的结果数，默认0

如果你想每页显示5个结果，页码从1到3，那请求如下：

GET /_search?size=5
GET /_search?size=5&from=5
GET /_search?size=5&from=10
应该当心分页太深或者一次请求太多的结果。结果在返回前会被排序。但是记住一个搜索请求常常涉及多个分片。每个分片生成自己排好序的结果，它们接着需要集中起来排序以确保整体排序正确。

在集群系统中深度分页

为了理解为什么深度分页是有问题的，让我们假设在一个有5个主分片的索引中搜索。当我们请求结果的第一页（结果1到10）时，每个分片产生自己最顶端10个结果然后返回它们给请求节点(requesting node)，它再排序这所有的50个结果以选出顶端的10个结果。

现在假设我们请求第1000页——结果10001到10010。工作方式都相同，不同的是每个分片都必须产生顶端的10010个结果。然后请求节点排序这50050个结果并丢弃50040个！

你可以看到在分布式系统中，排序结果的花费随着分页的深入而成倍增长。这也是为什么网络搜索引擎中任何语句不能返回多于1000个结果的原因。

TIP

在《重建索引》章节我们将阐述如何能高效的检索大量文档



简易搜索
search API有两种表单：一种是“简易版”的查询字符串(query string)将所有参数通过查询字符串定义，另一种版本使用JSON完整的表示请求体(request body)，这种富搜索语言叫做结构化查询语句（DSL）

查询字符串搜索对于在命令行下运行点对点(ad hoc)查询特别有用。例如这个语句查询所有类型为tweet并在tweet字段中包含elasticsearch字符的文档：

GET /_all/tweet/_search?q=tweet:elasticsearch
下一个语句查找name字段中包含"john"和tweet字段包含"mary"的结果。实际的查询只需要：

+name:john +tweet:mary
但是百分比编码(percent encoding)（译者注：就是url编码）需要将查询字符串参数变得更加神秘：

GET /_search?q=%2Bname%3Ajohn+%2Btweet%3Amary
"+"前缀表示语句匹配条件必须被满足。类似的"-"前缀表示条件必须不被满足。所有条件如果没有+或-表示是可选的——匹配越多，相关的文档就越多。

_all字段

返回包含"mary"字符的所有文档的简单搜索：

GET /_search?q=mary
在前一个例子中，我们搜索tweet或name字段中包含某个字符的结果。然而，这个语句返回的结果在三个不同的字段中包含"mary"：

用户的名字是“Mary”
“Mary”发的六个推文
针对“@mary”的一个推文
Elasticsearch是如何设法找到三个不同字段的结果的？

当你索引一个文档，Elasticsearch把所有字符串字段值连接起来放在一个大字符串中，它被索引为一个特殊的字段_all。例如，当索引这个文档：

{
    "tweet":    "However did I manage before Elasticsearch?",
    "date":     "2014-09-14",
    "name":     "Mary Jones",
    "user_id":  1
}
这好比我们增加了一个叫做_all的额外字段值：

"However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1"
查询字符串在其他字段被定以前使用_all字段搜索。

TIP

_all字段对于开始一个新应用时是一个有用的特性。之后，如果你定义字段来代替_all字段，你的搜索结果将更加可控。当_all字段不再使用，你可以停用它，这个会在《全字段》章节阐述。
更复杂的语句

下一个搜索推特的语句：

_all field

name字段包含"mary"或"john"
date晚于2014-09-10
_all字段包含"aggregations"或"geo"
+name:(mary john) +date:>2014-09-10 +(aggregations geo)
编码后的查询字符串变得不太容易阅读：

?q=%2Bname%3A(mary+john)+%2Bdate%3A%3E2014-09-10+%2B(aggregations+geo)
就像你上面看到的例子，简单(lite)查询字符串搜索惊人的强大。它的查询语法，会在《查询字符串语法》章节阐述。参考文档允许我们简洁明快的表示复杂的查询。这对于命令行下一次性查询或者开发模式下非常有用。

然而，你可以看到简洁带来了隐晦和调试困难。而且它很脆弱——查询字符串中一个细小的语法错误，像-、:、/或"错位就会导致返回错误而不是结果。

最后，查询字符串搜索允许任意用户在索引中任何一个字段上运行潜在的慢查询语句，可能暴露私有信息甚至使你的集群瘫痪。

TIP

因为这些原因，我们不建议直接暴露查询字符串搜索给用户，除非这些用户对于你的数据和集群可信。
取而代之的，生产环境我们一般依赖全功能的请求体搜索API，它能完成前面所有的事情，甚至更多。在了解它们之前，我们首先需要看看数据是如何在Elasticsearch中被索引的。







映射(mapping)机制用于进行字段类型确认，将每个字段匹配为一种确定的数据类型(string, number, booleans, date等)。

分析(analysis)机制用于进行全文文本(Full Text)的分词，以建立供搜索用的反向索引。


映射及分析
当在索引中处理数据时，我们注意到一些奇怪的事。有些东西似乎被破坏了：

在索引中有12个tweets，只有一个包含日期2014-09-15，但是我们看看下面查询中的total hits。

GET /_search?q=2014              # 12 个结果
GET /_search?q=2014-09-15        # 还是 12 个结果 !
GET /_search?q=date:2014-09-15   # 1  一个结果
GET /_search?q=date:2014         # 0  个结果 !
为什么全日期的查询返回所有的tweets，而针对date字段进行年度查询却什么都不返回？ 为什么我们的结果因查询_all字段(译者注：默认所有字段中进行查询)或date字段而变得不同？

想必是因为我们的数据在_all字段的索引方式和在date字段的索引方式不同而导致。

让我们看看Elasticsearch在对gb索引中的tweet类型进行mapping(也称之为模式定义[注：此词有待重新定义(schema definition)])后是如何解读我们的文档结构：

GET /gb/_mapping/tweet
返回：

{
   "gb": {
      "mappings": {
         "tweet": {
            "properties": {
               "date": {
                  "type": "date",
                  "format": "dateOptionalTime"
               },
               "name": {
                  "type": "string"
               },
               "tweet": {
                  "type": "string"
               },
               "user_id": {
                  "type": "long"
               }
            }
         }
      }
   }
}
Elasticsearch为对字段类型进行猜测，动态生成了字段和类型的映射关系。返回的信息显示了date字段被识别为date类型。_all因为是默认字段所以没有在此显示，不过我们知道它是string类型。

date类型的字段和string类型的字段的索引方式是不同的，因此导致查询结果的不同，这并不会让我们觉得惊讶。

你会期望每一种核心数据类型(strings, numbers, booleans及dates)以不同的方式进行索引，而这点也是现实：在Elasticsearch中他们是被区别对待的。

但是更大的区别在于确切值(exact values)(比如string类型)及全文文本(full text)之间。

这两者的区别才真的很重要 - 这是区分搜索引擎和其他数据库的根本差异。




确切值(Exact values) vs. 全文文本(Full text)
Elasticsearch中的数据可以大致分为两种类型：

确切值 及 全文文本。

确切值是确定的，正如它的名字一样。比如一个date或用户ID，也可以包含更多的字符串比如username或email地址。

确切值"Foo"和"foo"就并不相同。确切值2014和2014-09-15也不相同。

全文文本，从另一个角度来说是文本化的数据(常常以人类的语言书写)，比如一片推文(Twitter的文章)或邮件正文。

全文文本常常被称为非结构化数据，其实是一种用词不当的称谓，实际上自然语言是高度结构化的。

问题是自然语言的语法规则是如此的复杂，计算机难以正确解析。例如这个句子：

May is fun but June bores me.
到底是说的月份还是人呢？

确切值是很容易查询的，因为结果是二进制的 -- 要么匹配，要么不匹配。下面的查询很容易以SQL表达：

WHERE name    = "John Smith"
  AND user_id = 2
  AND date    > "2014-09-15"
而对于全文数据的查询来说，却有些微妙。我们不会去询问这篇文档是否匹配查询要求？。 但是，我们会询问这篇文档和查询的匹配程度如何？。换句话说，对于查询条件，这篇文档的_相关性_有多高？

我们很少确切的匹配整个全文文本。我们想在全文中查询包含查询文本的部分。不仅如此，我们还期望搜索引擎能理解我们的意图：

一个针对"UK"的查询将返回涉及"United Kingdom"的文档

一个针对"jump"的查询同时能够匹配"jumped"， "jumps"， "jumping"甚至"leap"

"johnny walker"也能匹配"Johnnie Walker"， "johnnie depp"及"Johnny Depp"

"fox news hunting"能返回有关hunting on Fox News的故事，而"fox hunting news"也能返回关于fox hunting的新闻故事。
为了方便在全文文本字段中进行这些类型的查询，Elasticsearch首先对文本分析(analyzes)，然后使用结果建立一个倒排索引。我们将在以下两个章节讨论倒排索引及分析过程。



倒排索引
Elasticsearch使用一种叫做倒排索引(inverted index)的结构来做快速的全文搜索。倒排索引由在文档中出现的唯一的单词列表，以及对于每个单词在文档中的位置组成。

例如，我们有两个文档，每个文档content字段包含：

The quick brown fox jumped over the lazy dog
Quick brown foxes leap over lazy dogs in summer
为了创建倒排索引，我们首先切分每个文档的content字段为单独的单词（我们把它们叫做词(terms)或者表征(tokens)）（译者注：关于terms和tokens的翻译比较生硬，只需知道语句分词后的个体叫做这两个。），把所有的唯一词放入列表并排序，结果是这个样子的：

Term	Doc_1	Doc_2
Quick		X
The	X	
brown	X	X
dog	X	
dogs		X
fox	X	
foxes		X
in		X
jumped	X	
lazy	X	X
leap		X
over	X	X
quick	X	
summer		X
the	X	
现在，如果我们想搜索"quick brown"，我们只需要找到每个词在哪个文档中出现即可：

Term	Doc_1	Doc_2
brown	X	X
quick	X	
-----	-------	-----
Total	2	1
两个文档都匹配，但是第一个比第二个有更多的匹配项。 如果我们加入简单的相似度算法(similarity algorithm)，计算匹配单词的数目，这样我们就可以说第一个文档比第二个匹配度更高——对于我们的查询具有更多相关性。

但是在我们的倒排索引中还有些问题：

"Quick"和"quick"被认为是不同的单词，但是用户可能认为它们是相同的。
"fox"和"foxes"很相似，就像"dog"和"dogs"——它们都是同根词。
"jumped"和"leap"不是同根词，但意思相似——它们是同义词。
上面的索引中，搜索"+Quick +fox"不会匹配任何文档（记住，前缀+表示单词必须匹配到）。只有"Quick"和"fox"都在同一文档中才可以匹配查询，但是第一个文档包含"quick fox"且第二个文档包含"Quick foxes"。（译者注：这段真啰嗦，说白了就是单复数和同义词没法匹配）

用户可以合理地希望两个文档都能匹配查询，我们也可以做得更好。

如果我们将词为统一为标准格式，这样就可以找到不是确切匹配查询，但是足以相似从而可以关联的文档。例如：

"Quick"可以转为小写成为"quick"。
"foxes"可以被转为根形式"fox"。同理"dogs"可以被转为"dog"。
"jumped"和"leap"同义就可以只索引为单个词"jump"
现在的索引：

Term	Doc_1	Doc_2
brown	X	X
dog	X	X
fox	X	X
in		X
jump	X	X
lazy	X	X
over	X	X
quick	X	X
summer		X
the	X	X
但我们还未成功。我们的搜索"+Quick +fox"依旧失败，因为"Quick"的确切值已经不在索引里，不过，如果我们使用相同的标准化规则处理查询字符串的content字段，查询将变成"+quick +fox"，这样就可以匹配到两个文档。

IMPORTANT

这很重要。你只可以找到确实存在于索引中的词，所以索引文本和查询字符串都要标准化为相同的形式。
这个标记化和标准化的过程叫做分词(analysis)，这个在下节中我们讨论。


分析和分析器
分析(analysis)是这样一个过程：

首先，标记化一个文本块为适用于倒排索引单独的词(term)
然后标准化这些词为标准形式，提高它们的“可搜索性”或“查全率”
这个工作是分析器(analyzer)完成的。一个分析器(analyzer)只是一个包装用于将三个功能放到一个包里：

字符过滤器

首先字符串经过字符过滤器(character filter)，它们的工作是在标记化前处理字符串。字符过滤器能够去除HTML标记，或者转换"&"为"and"。

分词器

下一步，分词器(tokenizer)被标记化成独立的词。一个简单的分词器(tokenizer)可以根据空格或逗号将单词分开（译者注：这个在中文中不适用）。

标记过滤

最后，每个词都通过所有标记过滤(token filters)，它可以修改词（例如将"Quick"转为小写），去掉词（例如停用词像"a"、"and"、"the"等等），或者增加词（例如同义词像"jump"和"leap"）

Elasticsearch提供很多开箱即用的字符过滤器，分词器和标记过滤器。这些可以组合来创建自定义的分析器以应对不同的需求。我们将在《自定义分析器》章节详细讨论。

内建的分析器

不过，Elasticsearch还附带了一些预装的分析器，你可以直接使用它们。下面我们列出了最重要的几个分析器，来演示这个字符串分词后的表现差异：

"Set the shape to semi-transparent by calling set_trans(5)"
标准分析器

标准分析器是Elasticsearch默认使用的分析器。对于文本分析，它对于任何语言都是最佳选择（译者注：就是没啥特殊需求，对于任何一个国家的语言，这个分析器就够用了）。它根据Unicode Consortium的定义的单词边界(word boundaries)来切分文本，然后去掉大部分标点符号。最后，把所有词转为小写。产生的结果为：

set, the, shape, to, semi, transparent, by, calling, set_trans, 5
简单分析器

简单分析器将非单个字母的文本切分，然后把每个词转为小写。产生的结果为：

set, the, shape, to, semi, transparent, by, calling, set, trans
空格分析器

空格分析器依据空格切分文本。它不转换小写。产生结果为：

Set, the, shape, to, semi-transparent, by, calling, set_trans(5)
语言分析器

特定语言分析器适用于很多语言。它们能够考虑到特定语言的特性。例如，english分析器自带一套英语停用词库——像and或the这些与语义无关的通用词。这些词被移除后，因为语法规则的存在，英语单词的主体含义依旧能被理解（译者注：stem English words这句不知道该如何翻译，查了字典，我理解的大概意思应该是将英语语句比作一株植物，去掉无用的枝叶，主干依旧存在，停用词好比枝叶，存在与否并不影响对这句话的理解。）。

english分析器将会产生以下结果：

set, shape, semi, transpar, call, set_tran, 5
注意"transparent"、"calling"和"set_trans"是如何转为词干的。

当分析器被使用

当我们索引(index)一个文档，全文字段会被分析为单独的词来创建倒排索引。不过，当我们在全文字段搜索(search)时，我们要让查询字符串经过同样的分析流程处理，以确保这些词在索引中存在。

全文查询我们将在稍后讨论，理解每个字段是如何定义的，这样才可以让它们做正确的事：

当你查询全文(full text)字段，查询将使用相同的分析器来分析查询字符串，以产生正确的词列表。
当你查询一个确切值(exact value)字段，查询将不分析查询字符串，但是你可以自己指定。
现在你可以明白为什么《映射和分析》的开头会产生那种结果：

date字段包含一个确切值：单独的一个词"2014-09-15"。
_all字段是一个全文字段，所以分析过程将日期转为三个词："2014"、"09"和"15"。
当我们在_all字段查询2014，它一个匹配到12条推文，因为这些推文都包含词2014：

GET /_search?q=2014              # 12 results
当我们在_all字段中查询2014-09-15，首先分析查询字符串，产生匹配任一词2014、09或15的查询语句，它依旧匹配12个推文，因为它们都包含词2014。

GET /_search?q=2014-09-15        # 12 results !
当我们在date字段中查询2014-09-15，它查询一个确切的日期，然后只找到一条推文：

GET /_search?q=date:2014-09-15   # 1  result
当我们在date字段中查询2014，没有找到文档，因为没有文档包含那个确切的日期：

GET /_search?q=date:2014         # 0  results !
测试分析器

尤其当你是Elasticsearch新手时，对于如何分词以及存储到索引中理解起来比较困难。为了更好的理解如何进行，你可以使用analyze API来查看文本是如何被分析的。在查询字符串参数中指定要使用的分析器，被分析的文本做为请求体：

GET /_analyze?analyzer=standard&text=Text to analyze
结果中每个节点在代表一个词：

{
   "tokens": [
      {
         "token":        "text",
         "start_offset": 0,
         "end_offset":   4,
         "type":         "<ALPHANUM>",
         "position":     1
      },
      {
         "token":        "to",
         "start_offset": 5,
         "end_offset":   7,
         "type":         "<ALPHANUM>",
         "position":     2
      },
      {
         "token":        "analyze",
         "start_offset": 8,
         "end_offset":   15,
         "type":         "<ALPHANUM>",
         "position":     3
      }
   ]
}
token是一个实际被存储在索引中的词。position指明词在原文本中是第几个出现的。start_offset和end_offset表示词在原文本中占据的位置。

analyze API 对于理解Elasticsearch索引的内在细节是个非常有用的工具，随着内容的推进，我们将继续讨论它。

指定分析器

当Elasticsearch在你的文档中探测到一个新的字符串字段，它将自动设置它为全文string字段并用standard分析器分析。

你不可能总是想要这样做。也许你想使用一个更适合这个数据的语言分析器。或者，你只想把字符串字段当作一个普通的字段——不做任何分析，只存储确切值，就像字符串类型的用户ID或者内部状态字段或者标签。

为了达到这种效果，我们必须通过映射(mapping)人工设置这些字段。


映射
为了能够把日期字段处理成日期，把数字字段处理成数字，把字符串字段处理成全文本（Full-text）或精确的字符串值，Elasticsearch需要知道每个字段里面都包含了什么类型。这些类型和字段的信息存储（包含）在映射（mapping）中。

正如《数据吞吐》一节所说，索引中每个文档都有一个类型(type)。 每个类型拥有自己的映射(mapping)或者模式定义(schema definition)。一个映射定义了字段类型，每个字段的数据类型，以及字段被Elasticsearch处理的方式。映射还用于设置关联到类型上的元数据。

在《映射》章节我们将探讨映射的细节。这节我们只是带你入门。

核心简单字段类型

Elasticsearch支持以下简单字段类型：

类型	表示的数据类型
String	string
Whole number	byte, short, integer, long
Floating point	float, double
Boolean	boolean
Date	date
当你索引一个包含新字段的文档——一个之前没有的字段——Elasticsearch将使用动态映射猜测字段类型，这类型来自于JSON的基本数据类型，使用以下规则：

JSON type	Field type
Boolean: true or false	"boolean"
Whole number: 123	"long"
Floating point: 123.45	"double"
String, valid date: "2014-09-15"	"date"
String: "foo bar"	"string"
注意

这意味着，如果你索引一个带引号的数字——"123"，它将被映射为"string"类型，而不是"long"类型。然而，如果字段已经被映射为"long"类型，Elasticsearch将尝试转换字符串为long，并在转换失败时会抛出异常。
查看映射

我们可以使用_mapping后缀来查看Elasticsearch中的映射。在本章开始我们已经找到索引gb类型tweet中的映射：

GET /gb/_mapping/tweet
这展示给了我们字段的映射（叫做属性(properties)），这些映射是Elasticsearch在创建索引时动态生成的：

{
   "gb": {
      "mappings": {
         "tweet": {
            "properties": {
               "date": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "name": {
                  "type": "string"
               },
               "tweet": {
                  "type": "string"
               },
               "user_id": {
                  "type": "long"
               }
            }
         }
      }
   }
}
小提示

错误的映射，例如把age字段映射为string类型而不是integer类型，会造成查询结果混乱。

要检查映射类型，而不是假设它是正确的！
自定义字段映射

虽然大多数情况下基本数据类型已经能够满足，但你也会经常需要自定义一些特殊类型（fields），特别是字符串字段类型。 自定义类型可以使你完成一下几点：

区分全文（full text）字符串字段和准确字符串字段（译者注：就是分词与不分词，全文的一般要分词，准确的就不需要分词，比如『中国』这个词。全文会分成『中』和『国』，但作为一个国家标识的时候我们是不需要分词的，所以它就应该是一个准确的字符串字段）。
使用特定语言的分析器（译者注：例如中文、英文、阿拉伯语，不同文字的断字、断词方式的差异）
优化部分匹配字段
指定自定义日期格式（译者注：这个比较好理解,例如英文的 Feb,12,2016 和 中文的 2016年2月12日）
以及更多
映射中最重要的字段参数是type。除了string类型的字段，你可能很少需要映射其他的type：

{
    "number_of_clicks": {
        "type": "integer"
    }
}
string类型的字段，默认的，考虑到包含全文本，它们的值在索引前要经过分析器分析，并且在全文搜索此字段前要把查询语句做分析处理。

对于string字段，两个最重要的映射参数是index和analyer。

index

index参数控制字符串以何种方式被索引。它包含以下三个值当中的一个：

值	解释
analyzed	首先分析这个字符串，然后索引。换言之，以全文形式索引此字段。
not_analyzed	索引这个字段，使之可以被搜索，但是索引内容和指定值一样。不分析此字段。
no	不索引这个字段。这个字段不能为搜索到。
string类型字段默认值是analyzed。如果我们想映射字段为确切值，我们需要设置它为not_analyzed：

{
    "tag": {
        "type":     "string",
        "index":    "not_analyzed"
    }
}
其他简单类型（long、double、date等等）也接受index参数，但相应的值只能是no和not_analyzed，它们的值不能被分析。
分析

对于analyzed类型的字符串字段，使用analyzer参数来指定哪一种分析器将在搜索和索引的时候使用。默认的，Elasticsearch使用standard分析器，但是你可以通过指定一个内建的分析器来更改它，例如whitespace、simple或english。

{
    "tweet": {
        "type":     "string",
        "analyzer": "english"
    }
}
在《自定义分析器》章节我们将告诉你如何定义和使用自定义的分析器。

更新映射

你可以在第一次创建索引的时候指定映射的类型。此外，你也可以晚些时候为新类型添加映射（或者为已有的类型更新映射）。

重要

你可以向已有映射中增加字段，但你不能修改它。如果一个字段在映射中已经存在，这可能意味着那个字段的数据已经被索引。如果你改变了字段映射，那已经被索引的数据将错误并且不能被正确的搜索到。
我们可以更新一个映射来增加一个新字段，但是不能把已有字段的类型那个从analyzed改到not_analyzed。

为了演示两个指定的映射方法，让我们首先删除索引gb：

DELETE /gb
然后创建一个新索引，指定tweet字段的分析器为english：

PUT /gb <1>
{
  "mappings": {
    "tweet" : {
      "properties" : {
        "tweet" : {
          "type" :    "string",
          "analyzer": "english"
        },
        "date" : {
          "type" :   "date"
        },
        "name" : {
          "type" :   "string"
        },
        "user_id" : {
          "type" :   "long"
        }
      }
    }
  }
}
<1> 这将创建包含mappings的索引，映射在请求体中指定。

再后来，我们决定在tweet的映射中增加一个新的not_analyzed类型的文本字段，叫做tag，使用_mapping后缀:

PUT /gb/_mapping/tweet
{
  "properties" : {
    "tag" : {
      "type" :    "string",
      "index":    "not_analyzed"
    }
  }
}
注意到我们不再需要列出所有的已经存在的字段，因为我们没法修改他们。我们的新字段已经被合并至存在的那个映射中。

测试映射

你可以通过名字使用analyze API测试字符串字段的映射。对比这两个请求的输出：

GET /gb/_analyze?field=tweet&text=Black-cats <1>

GET /gb/_analyze?field=tag&text=Black-cats <2>
<1> <2> 我们想要分析的文本被放在请求体中。

tweet字段产生两个词，"black"和"cat",tag字段产生单独的一个词"Black-cats"。换言之，我们的映射工作正常。



复合核心字段类型
除了之前提到的简单的标量类型，JSON还有null值，数组和对象，所有这些Elasticsearch都支持：

多值字段

我们想让tag字段包含多个字段，这非常有可能发生。我们可以索引一个标签数组来代替单一字符串：

{ "tag": [ "search", "nosql" ]}
对于数组不需要特殊的映射。任何一个字段可以包含零个、一个或多个值，同样对于全文字段将被分析并产生多个词。

言外之意，这意味着数组中所有值必须为同一类型。你不能把日期和字符窜混合。如果你创建一个新字段，这个字段索引了一个数组，Elasticsearch将使用第一个值的类型来确定这个新字段的类型。

当你从Elasticsearch中取回一个文档，任何一个数组的顺序和你索引它们的顺序一致。你取回的_source字段的顺序同样与索引它们的顺序相同。

然而，数组是做为多值字段被索引的，它们没有顺序。在搜索阶段你不能指定“第一个值”或者“最后一个值”。倒不如把数组当作一个值集合(bag of values)
空字段

当然数组可以是空的。这等价于有零个值。事实上，Lucene没法存放null值，所以一个null值的字段被认为是空字段。

这四个字段将被识别为空字段而不被索引：

"empty_string":             "",
"null_value":               null,
"empty_array":              [],
"array_with_null_value":    [ null ]
多层对象

我们需要讨论的最后一个自然JSON数据类型是对象(object)——在其它语言中叫做hash、hashmap、dictionary 或者 associative array.

内部对象(inner objects)经常用于在另一个对象中嵌入一个实体或对象。例如，做为在tweet文档中user_name和user_id的替代，我们可以这样写：

{
    "tweet":            "Elasticsearch is very flexible",
    "user": {
        "id":           "@johnsmith",
        "gender":       "male",
        "age":          26,
        "name": {
            "full":     "John Smith",
            "first":    "John",
            "last":     "Smith"
        }
    }
}
内部对象的映射

Elasticsearch 会动态的检测新对象的字段，并且映射它们为 object 类型，将每个字段加到 properties 字段下

{
  "gb": {
    "tweet": { <1>
      "properties": {
        "tweet":            { "type": "string" },
        "user": { <2>
          "type":             "object",
          "properties": {
            "id":           { "type": "string" },
            "gender":       { "type": "string" },
            "age":          { "type": "long"   },
            "name":   { <3>
              "type":         "object",
              "properties": {
                "full":     { "type": "string" },
                "first":    { "type": "string" },
                "last":     { "type": "string" }
              }
            }
          }
        }
      }
    }
  }
}
根对象. 内部对象. 对`user`和`name`字段的映射与`tweet`类型自己很相似。事实上，`type`映射只是`object`映射的一种特殊类型，我们将 `object` 称为_根对象_。它与其他对象一模一样，除非它有一些特殊的顶层字段，比如 `_source`, `_all` 等等。 ### 内部对象是怎样被索引的 Lucene 并不了解内部对象。 一个 Lucene 文件包含一个键-值对应的扁平表单。 为了让 Elasticsearch 可以有效的索引内部对象，将文件转换为以下格式： ```javascript { "tweet": [elasticsearch, flexible, very], "user.id": [@johnsmith], "user.gender": [male], "user.age": [26], "user.name.full": [john, smith], "user.name.first": [john], "user.name.last": [smith] } ``` _内部栏位_可被归类至name，例如`"first"`。 为了区别两个拥有相同名字的栏位，我们可以使用完整_路径_，例如`"user.name.first"` 或甚至`类型`名称加上路径：`"tweet.user.name.first"`。 > 注意： 在以上扁平化文件中，并没有栏位叫作`user`也没有栏位叫作`user.name`。 Lucene 只索引阶层或简单的值，而不会索引复杂的资料结构。 ## 对象-数组 ### 内部对象数组 最后，一个包含内部对象的数组如何索引。 我们有个数组如下所示： ```json { "followers": [ { "age": 35, "name": "Mary White"}, { "age": 26, "name": "Alex Jones"}, { "age": 19, "name": "Lisa Smith"} ] } ``` 此文件会如我们以上所说的被扁平化，但其结果会像如此： ```json { "followers.age": [19, 26, 35], "followers.name": [alex, jones, lisa, smith, mary, white] } ``` `{age: 35}`与`{name: Mary White}`之间的关联会消失，因每个多值的栏位会变成一个值集合，而非有序的阵列。 这让我们可以知道： * _是否有26岁的追随者？_ 但我们无法取得准确的资料如： * _是否有26岁的追随者**且名字叫Alex Jones？**_ 关联内部对象可解决此类问题，我们称之为_嵌套_对象，我们之後会在嵌套对象中提到它。










请求体查询
简单查询语句(lite)是一种有效的命令行_adhoc_查询。但是，如果你想要善用搜索，你必须使用请求体查询(request body search)API。之所以这么称呼，是因为大多数的参数以JSON格式所容纳而非查询字符串。

请求体查询(下文简称查询)，并不仅仅用来处理查询，而且还可以高亮返回结果中的片段，并且给出帮助你的用户找寻最好结果的相关数据建议。

空查询

我们以最简单的 search API开始，空查询将会返回索引中所有的文档。

GET /_search
{} <1>
这是一个空查询数据。
同字符串查询一样，你可以查询一个，多个或_all索引(indices)或类型(types)：

GET /index_2014*/type1,type2/_search
{}
你可以使用from及size参数进行分页：

GET /_search
{
  "from": 30,
  "size": 10
}
携带内容的GET请求？

任何一种语言(特别是js)的HTTP库都不允许GET请求中携带交互数据。 事实上，有些用户很惊讶GET请求中居然会允许携带交互数据。

真实情况是，http://tools.ietf.org/html/rfc7231#page-24[RFC 7231]， 一份规定HTTP语义及内容的RFC中并未规定GET请求中允许携带交互数据！ 所以，有些HTTP服务允许这种行为，而另一些(特别是缓存代理)，则不允许这种行为。

Elasticsearch的作者们倾向于使用GET提交查询请求，因为他们觉得这个词相比POST来说，能更好的描述这种行为。 然而，因为携带交互数据的GET请求并不被广泛支持，所以searchAPI同样支持POST请求，类似于这样：

POST /_search
{
  "from": 30,
  "size": 10
}
这个原理同样应用于其他携带交互数据的GETAPI请求中。

我们将在后续的章节中讨论聚合查询，但是现在我们把关注点仅放在查询语义上。

相对于神秘的查询字符串方法，请求体查询允许我们使用结构化查询Query DSL(Query Domain Specific Language)

结构化查询 Query DSL
结构化查询是一种灵活的，多表现形式的查询语言。 Elasticsearch在一个简单的JSON接口中用结构化查询来展现Lucene绝大多数能力。 你应当在你的产品中采用这种方式进行查询。它使得你的查询更加灵活，精准，易于阅读并且易于debug。

使用结构化查询，你需要传递query参数：

GET /_search
{
    "query": YOUR_QUERY_HERE
}
空查询 - {} - 在功能上等同于使用match_all查询子句，正如其名字一样，匹配所有的文档：

GET /_search
{
    "query": {
        "match_all": {}
    }
}
查询子句

一个查询子句一般使用这种结构：

{
    QUERY_NAME: {
        ARGUMENT: VALUE,
        ARGUMENT: VALUE,...
    }
}
或指向一个指定的字段：

{
    QUERY_NAME: {
        FIELD_NAME: {
            ARGUMENT: VALUE,
            ARGUMENT: VALUE,...
        }
    }
}
例如，你可以使用match查询子句用来找寻在tweet字段中找寻包含elasticsearch的成员：

{
    "match": {
        "tweet": "elasticsearch"
    }
}
完整的查询请求会是这样：

GET /_search
{
    "query": {
        "match": {
            "tweet": "elasticsearch"
        }
    }
}
合并多子句

查询子句就像是搭积木一样，可以合并简单的子句为一个复杂的查询语句，比如：

叶子子句(leaf clauses)(比如match子句)用以在将查询字符串与一个字段(或多字段)进行比较

复合子句(compound)用以合并其他的子句。例如，bool子句允许你合并其他的合法子句，must，must_not或者should，如果可能的话：
{
    "bool": {
        "must":     { "match": { "tweet": "elasticsearch" }},
        "must_not": { "match": { "name":  "mary" }},
        "should":   { "match": { "tweet": "full text" }}
    }
}
复合子句能合并 任意其他查询子句，包括其他的复合子句。 这就意味着复合子句可以相互嵌套，从而实现非常复杂的逻辑。

以下实例查询的是邮件正文中含有“business opportunity”字样的星标邮件或收件箱中正文中含有“business opportunity”字样的非垃圾邮件：

{
    "bool": {
        "must": { "match":      { "email": "business opportunity" }},
        "should": [
             { "match":         { "starred": true }},
             { "bool": {
                   "must":      { "folder": "inbox" }},
                   "must_not":  { "spam": true }}
             }}
        ],
        "minimum_should_match": 1
    }
}
不用担心这个例子的细节，我们将在后面详细解释它。 重点是复合子句可以合并多种子句为一个单一的查询，无论是叶子子句还是其他的复合子句。

查询与过滤
前面我们讲到的是关于结构化查询语句，事实上我们可以使用两种结构化语句： 结构化查询（Query DSL）和结构化过滤（Filter DSL）。 查询与过滤语句非常相似，但是它们由于使用目的不同而稍有差异。

一条过滤语句会询问每个文档的字段值是否包含着特定值：

created 的日期范围是否在 2013 到 2014 ?

status 字段中是否包含单词 "published" ?

lat_lon 字段中的地理位置与目标点相距是否不超过10km ?
一条查询语句与过滤语句相似，但问法不同：

查询语句会询问每个文档的字段值与特定值的匹配程度如何？

查询语句的典型用法是为了找到文档：

查找与 full text search 这个词语最佳匹配的文档

查找包含单词 run ，但是也包含runs, running, jog 或 sprint的文档

同时包含着 quick, brown 和 fox --- 单词间离得越近，该文档的相关性越高

标识着 lucene, search 或 java --- 标识词越多，该文档的相关性越高
一条查询语句会计算每个文档与查询语句的相关性，会给出一个相关性评分 _score，并且 按照相关性对匹配到的文档进行排序。 这种评分方式非常适用于一个没有完全配置结果的全文本搜索。

性能差异
使用过滤语句得到的结果集 -- 一个简单的文档列表，快速匹配运算并存入内存是十分方便的， 每个文档仅需要1个字节。这些缓存的过滤结果集与后续请求的结合使用是非常高效的。

查询语句不仅要查找相匹配的文档，还需要计算每个文档的相关性，所以一般来说查询语句要比 过滤语句更耗时，并且查询结果也不可缓存。

幸亏有了倒排索引，一个只匹配少量文档的简单查询语句在百万级文档中的查询效率会与一条经过缓存 的过滤语句旗鼓相当，甚至略占上风。 但是一般情况下，一条经过缓存的过滤查询要远胜一条查询语句的执行效率。

过滤语句的目的就是缩小匹配的文档结果集，所以需要仔细检查过滤条件。

什么情况下使用
原则上来说，使用查询语句做全文本搜索或其他需要进行相关性评分的时候，剩下的全部用过滤语句

最重要的查询过滤语句
Elasticsearch 提供了丰富的查询过滤语句，而有一些是我们较常用到的。 我们将会在后续的《深入搜索》中展开讨论，现在我们快速的介绍一下 这些最常用到的查询过滤语句。

term 过滤
term主要用于精确匹配哪些值，比如数字，日期，布尔值或 not_analyzed的字符串(未经分析的文本数据类型)：

    { "term": { "age":    26           }}
    { "term": { "date":   "2014-09-01" }}
    { "term": { "public": true         }}
    { "term": { "tag":    "full_text"  }}
terms 过滤
terms 跟 term 有点类似，但 terms 允许指定多个匹配条件。 如果某个字段指定了多个值，那么文档需要一起去做匹配：

{
    "terms": {
        "tag": [ "search", "full_text", "nosql" ]
        }
}
range 过滤
range过滤允许我们按照指定范围查找一批数据：

{
    "range": {
        "age": {
            "gte":  20,
            "lt":   30
        }
    }
}
范围操作符包含：

gt :: 大于

gte:: 大于等于

lt :: 小于

lte:: 小于等于

exists 和 missing 过滤
exists 和 missing 过滤可以用于查找文档中是否包含指定字段或没有某个字段，类似于SQL语句中的IS_NULL条件

{
    "exists":   {
        "field":    "title"
    }
}
这两个过滤只是针对已经查出一批数据来，但是想区分出某个字段是否存在的时候使用。

bool 过滤
bool 过滤可以用来合并多个过滤条件查询结果的布尔逻辑，它包含一下操作符：

must :: 多个查询条件的完全匹配,相当于 and。

must_not :: 多个查询条件的相反匹配，相当于 not。

should :: 至少有一个查询条件匹配, 相当于 or。

这些参数可以分别继承一个过滤条件或者一个过滤条件的数组：

{
    "bool": {
        "must":     { "term": { "folder": "inbox" }},
        "must_not": { "term": { "tag":    "spam"  }},
        "should": [
                    { "term": { "starred": true   }},
                    { "term": { "unread":  true   }}
        ]
    }
}
match_all 查询
使用match_all 可以查询到所有文档，是没有查询条件下的默认语句。

{
    "match_all": {}
}
此查询常用于合并过滤条件。 比如说你需要检索所有的邮箱,所有的文档相关性都是相同的，所以得到的_score为1

match 查询
match查询是一个标准查询，不管你需要全文本查询还是精确查询基本上都要用到它。

如果你使用 match 查询一个全文本字段，它会在真正查询之前用分析器先分析match一下查询字符：

{
    "match": {
        "tweet": "About Search"
    }
}
如果用match下指定了一个确切值，在遇到数字，日期，布尔值或者not_analyzed 的字符串时，它将为你搜索你给定的值：

{ "match": { "age":    26           }}
{ "match": { "date":   "2014-09-01" }}
{ "match": { "public": true         }}
{ "match": { "tag":    "full_text"  }}
提示： 做精确匹配搜索时，你最好用过滤语句，因为过滤语句可以缓存数据。
不像我们在《简单搜索》中介绍的字符查询，match查询不可以用类似"+usid:2 +tweet:search"这样的语句。 它只能就指定某个确切字段某个确切的值进行搜索，而你要做的就是为它指定正确的字段名以避免语法错误。

multi_match 查询
multi_match查询允许你做match查询的基础上同时搜索多个字段：

{
    "multi_match": {
        "query":    "full text search",
        "fields":   [ "title", "body" ]
    }
}
bool 查询
bool 查询与 bool 过滤相似，用于合并多个查询子句。不同的是，bool 过滤可以直接给出是否匹配成功， 而bool 查询要计算每一个查询子句的 _score （相关性分值）。

must:: 查询指定文档一定要被包含。

must_not:: 查询指定文档一定不要被包含。

should:: 查询指定文档，有则可以为文档相关性加分。

以下查询将会找到 title 字段中包含 "how to make millions"，并且 "tag" 字段没有被标为 spam。 如果有标识为 "starred" 或者发布日期为2014年之前，那么这些匹配的文档将比同类网站等级高：

{
    "bool": {
        "must":     { "match": { "title": "how to make millions" }},
        "must_not": { "match": { "tag":   "spam" }},
        "should": [
            { "match": { "tag": "starred" }},
            { "range": { "date": { "gte": "2014-01-01" }}}
        ]
    }
}
提示： 如果bool 查询下没有must子句，那至少应该有一个should子句。但是 如果有must子句，那么没有should子句也可以进行查询。


查询与过滤条件的合并
查询语句和过滤语句可以放在各自的上下文中。 在 ElasticSearch API 中我们会看到许多带有 query 或 filter 的语句。 这些语句既可以包含单条 query 语句，也可以包含一条 filter 子句。 换句话说，这些语句需要首先创建一个query或filter的上下文关系。

复合查询语句可以加入其他查询子句，复合过滤语句也可以加入其他过滤子句。 通常情况下，一条查询语句需要过滤语句的辅助，全文本搜索除外。

所以说，查询语句可以包含过滤子句，反之亦然。 以便于我们切换 query 或 filter 的上下文。这就要求我们在读懂需求的同时构造正确有效的语句。

带过滤的查询语句
过滤一条查询语句

比如说我们有这样一条查询语句:

{
    "match": {
        "email": "business opportunity"
    }
}
然后我们想要让这条语句加入 term 过滤，在收信箱中匹配邮件：

{
    "term": {
        "folder": "inbox"
    }
}
search API中只能包含 query 语句，所以我们需要用 filtered 来同时包含 "query" 和 "filter" 子句：

{
    "filtered": {
        "query":  { "match": { "email": "business opportunity" }},
        "filter": { "term":  { "folder": "inbox" }}
    }
}
我们在外层再加入 query 的上下文关系：

GET /_search
{
    "query": {
        "filtered": {
            "query":  { "match": { "email": "business opportunity" }},
            "filter": { "term": { "folder": "inbox" }}
        }
    }
}
单条过滤语句
在 query 上下文中，如果你只需要一条过滤语句，比如在匹配全部邮件的时候，你可以 省略 query 子句：

GET /_search
{
    "query": {
        "filtered": {
            "filter":   { "term": { "folder": "inbox" }}
        }
    }
}
如果一条查询语句没有指定查询范围，那么它默认使用 match_all 查询，所以上面语句 的完整形式如下：

GET /_search
{
    "query": {
        "filtered": {
            "query":    { "match_all": {}},
            "filter":   { "term": { "folder": "inbox" }}
        }
    }
}
查询语句中的过滤
有时候，你需要在 filter 的上下文中使用一个 query 子句。下面的语句就是一条带有查询功能 的过滤语句， 这条语句可以过滤掉看起来像垃圾邮件的文档：

GET /_search
{
    "query": {
        "filtered": {
            "filter":   {
                "bool": {
                    "must":     { "term":  { "folder": "inbox" }},
                    "must_not": {
                        "query": { <1>
                            "match": { "email": "urgent business proposal" }
                        }
                    }
                }
            }
        }
    }
}
过滤语句中可以使用`query`查询的方式代替 `bool` 过滤子句。 >**提示**： >我们很少用到的过滤语句中包含查询，保留这种用法只是为了语法的完整性。 >只有在过滤中用到全文本匹配的时候才会使用这种结构。


验证查询
查询语句可以变得非常复杂，特别是与不同的分析器和字段映射相结合后，就会有些难度。

validate API 可以验证一条查询语句是否合法。

GET /gb/tweet/_validate/query
{
   "query": {
      "tweet" : {
         "match" : "really powerful"
      }
   }
}
以上请求的返回值告诉我们这条语句是非法的：

{
  "valid" :         false,
  "_shards" : {
    "total" :       1,
    "successful" :  1,
    "failed" :      0
  }
}
理解错误信息
想知道语句非法的具体错误信息，需要加上 explain 参数：

GET /gb/tweet/_validate/query?explain <1>
{
   "query": {
      "tweet" : {
         "match" : "really powerful"
      }
   }
}
`explain` 参数可以提供语句错误的更多详情。 很显然，我们把 query 语句的 `match` 与字段名位置弄反了： ```Javascript { "valid" : false, "_shards" : { ... }, "explanations" : [ { "index" : "gb", "valid" : false, "error" : "org.elasticsearch.index.query.QueryParsingException: [gb] No query registered for [tweet]" } ] } ``` ## 理解查询语句 如果是合法语句的话，使用 `explain` 参数可以返回一个带有查询语句的可阅读描述， 可以帮助了解查询语句在ES中是如何执行的： ```Javascript GET /_validate/query?explain { "query": { "match" : { "tweet" : "really powerful" } } } ``` `explanation` 会为每一个索引返回一段描述，因为每个索引会有不同的映射关系和分析器： ```Javascript { "valid" : true, "_shards" : { ... }, "explanations" : [ { "index" : "us", "valid" : true, "explanation" : "tweet:really tweet:powerful" }, { "index" : "gb", "valid" : true, "explanation" : "tweet:really tweet:power" } ] } ``` 从返回的 `explanation` 你会看到 `match` 是如何为查询字符串 `"really powerful"` 进行查询的， 首先，它被拆分成两个独立的词分别在 `tweet` 字段中进行查询。 而且，在索引`us`中这两个词为`"really"`和`"powerful"`，在索引`gb`中被拆分成`"really"` 和 `"power"`。 这是因为我们在索引`gb`中使用了`english`分析器。











相关性排序
默认情况下，结果集会按照相关性进行排序 -- 相关性越高，排名越靠前。 这一章我们会讲述相关性是什么以及它是如何计算的。 在此之前，我们先看一下sort参数的使用方法。

排序方式
为了使结果可以按照相关性进行排序，我们需要一个相关性的值。在ElasticSearch的查询结果中， 相关性分值会用_score字段来给出一个浮点型的数值，所以默认情况下，结果集以_score进行倒序排列。

有时，即便如此，你还是没有一个有意义的相关性分值。比如，以下语句返回所有tweets中 user_id 是否 包含值 1：

GET /_search
{
    "query" : {
        "filtered" : {
            "filter" : {
                "term" : {
                    "user_id" : 1
                }
            }
        }
    }
}
过滤语句与 _score 没有关系，但是有隐含的查询条件 match_all 为所有的文档的 _score 设值为 1。 也就相当于所有的文档相关性是相同的。

字段值排序
下面例子中，对结果集按照时间排序，这也是最常见的情形，将最新的文档排列靠前。 我们使用 sort 参数进行排序：

GET /_search
{
    "query" : {
        "filtered" : {
            "filter" : { "term" : { "user_id" : 1 }}
        }
    },
    "sort": { "date": { "order": "desc" }}
}
你会发现这里有两个不同点：

"hits" : {
    "total" :           6,
    "max_score" :       null, <1>
    "hits" : [ {
        "_index" :      "us",
        "_type" :       "tweet",
        "_id" :         "14",
        "_score" :      null, <1>
        "_source" :     {
             "date":    "2014-09-24",
             ...
        },
        "sort" :        [ 1411516800000 ] <2>
    },
    ...
}
`_score` 字段没有经过计算，因为它没有用作排序。 `date` 字段被转为毫秒当作排序依据。 首先，在每个结果中增加了一个 `sort` 字段，它所包含的值是用来排序的。 在这个例子当中 `date` 字段在内部被转为毫秒，即长整型数字`1411516800000`等同于日期字符串 `2014-09-24 00:00:00 UTC`。 其次就是 `_score` 和 `max_score` 字段都为 `null`。计算 `_score` 是比较消耗性能的, 而且通常主要用作排序 -- 我们不是用相关性进行排序的时候，就不需要统计其相关性。 如果你想强制计算其相关性，可以设置`track_scores`为 `true`。 ## 默认排序 **** 作为缩写，你可以只指定要排序的字段名称： ```Javascript "sort": "number_of_children" ``` 字段值默认以顺序排列，而 `_score` 默认以倒序排列。 **** ## 多级排序 如果我们想要合并一个查询语句，并且展示所有匹配的结果集使用第一排序是`date`，第二排序是 `_score`： ```Javascript GET /_search { "query" : { "filtered" : { "query": { "match": { "tweet": "manage text search" }}, "filter" : { "term" : { "user_id" : 2 }} } }, "sort": [ { "date": { "order": "desc" }}, { "_score": { "order": "desc" }} ] } ``` 排序是很重要的。结果集会先用第一排序字段来排序，当用用作第一字段排序的值相同的时候， 然后再用第二字段对第一排序值相同的文档进行排序，以此类推。 多级排序不需要包含 `_score` -- 你可以使用几个不同的字段，如位置距离或者自定义数值。 ## 字符串参数排序 **** 字符查询也支持自定义排序，在查询字符串使用`sort`参数就可以： ```Javascript GET /_search?sort=date:desc&sort=_score&q=search ``` **** ## 为多值字段排序 在为一个字段的多个值进行排序的时候， 其实这些值本来是没有固定的排序的-- 一个拥有多值的字段就是一个集合， 你准备以哪一个作为排序依据呢？ 对于数字和日期，你可以从多个值中取出一个来进行排序，你可以使用`min`, `max`, `avg` 或 `sum`这些模式。 比说你可以在 `dates` 字段中用最早的日期来进行排序： ```Javascript "sort": { "dates": { "order": "asc", "mode": "min" } } ```

多值字段字符串排序
译者注: 多值字段是指同一个字段在ES索引中可以有多个含义，即可使用多个分析器(analyser)进行分词与排序，也可以不添加分析器，保留原值。
被分析器(analyser)处理过的字符称为analyzed field(译者注：即已被分词并排序的字段，所有写入ES中的字段默认圴会被analyzed), analyzed字符串字段同时也是多值字段，在这些字段上排序往往得不到你想要的值。 比如你分析一个字符 "fine old art",它最终会得到三个值。例如我们想要按照第一个词首字母排序， 如果第一个单词相同的话，再用第二个词的首字母排序，以此类推，可惜 ElasticSearch 在进行排序时 是得不到这些信息的。

当然你可以使用 min 和 max 模式来排（默认使用的是 min 模式）但它是依据art 或者 old排序， 而不是我们所期望的那样。

为了使一个string字段可以进行排序，它必须只包含一个词：即完整的not_analyzed字符串(译者注：未经分析器分词并排序的原字符串)。 当然我们需要对字段进行全文本搜索的时候还必须使用被 analyzed 标记的字段。

在 _source 下相同的字符串上排序两次会造成不必要的资源浪费。 而我们想要的是同一个字段中同时包含这两种索引方式，我们只需要改变索引(index)的mapping即可。 方法是在所有核心字段类型上，使用通用参数 fields对mapping进行修改。 比如，我们原有mapping如下：

"tweet": {
    "type":     "string",
    "analyzer": "english"
}
改变后的多值字段mapping如下：

"tweet": { <1>
    "type":     "string",
    "analyzer": "english",
    "fields": {
        "raw": { <2>
            "type":  "string",
            "index": "not_analyzed"
        }
    }
}
`tweet` 字段用于全文本的 `analyzed` 索引方式不变。 新增的 `tweet.raw` 子字段索引方式是 `not_analyzed`。 现在，在给数据重建索引后，我们既可以使用 `tweet` 字段进行全文本搜索，也可以用`tweet.raw`字段进行排序： ```Javascript GET /_search { "query": { "match": { "tweet": "elasticsearch" } }, "sort": "tweet.raw" } ``` >**警告**： >对 `analyzed` 字段进行强制排序会消耗大量内存。 >详情请查阅《字段类型简介》相关内容。

相关性简介
我们曾经讲过，默认情况下，返回结果是按相关性倒序排列的。 但是什么是相关性？ 相关性如何计算？

每个文档都有相关性评分，用一个相对的浮点数字段 _score 来表示 -- _score 的评分越高，相关性越高。

查询语句会为每个文档添加一个 _score 字段。评分的计算方式取决于不同的查询类型 -- 不同的查询语句用于不同的目的：fuzzy 查询会计算与关键词的拼写相似程度，terms查询会计算 找到的内容与关键词组成部分匹配的百分比，但是一般意义上我们说的全文本搜索是指计算内容与关键词的类似程度。

ElasticSearch的相似度算法被定义为 TF/IDF，即检索词频率/反向文档频率，包括一下内容：

检索词频率::

检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过5次要比只出现过1次的相关性高。

反向文档频率::

每个检索词在索引中出现的频率？频率越高，相关性越低。 检索词出现在多数文档中会比出现在少数文档中的权重更低， 即检验一个检索词在文档中的普遍重要性。

字段长度准则::

字段的长度是多少？长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段。

单个查询可以使用TF/IDF评分标准或其他方式，比如短语查询中检索词的距离或模糊查询里的检索词相似度。

相关性并不只是全文本检索的专利。也适用于yes|no的子句，匹配的子句越多，相关性评分越高。

如果多条查询子句被合并为一条复合查询语句，比如 bool 查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中。

理解评分标准
当调试一条复杂的查询语句时，想要理解相关性评分 _score 是比较困难的。ElasticSearch 在 每个查询语句中都有一个explain参数，将 explain 设为 true 就可以得到更详细的信息。

GET /_search?explain <1>
{
   "query"   : { "match" : { "tweet" : "honeymoon" }}
}
`explain` 参数可以让返回结果添加一个 `_score` 评分的得来依据。 **** 增加一个 `explain` 参数会为每个匹配到的文档产生一大堆额外内容，但是花时间去理解它是很有意义的。 如果现在看不明白也没关系 -- 等你需要的时候再来回顾这一节就行。下面我们来一点点的了解这块知识点。 **** 首先，我们看一下普通查询返回的元数据： ```Javascript { "_index" : "us", "_type" : "tweet", "_id" : "12", "_score" : 0.076713204, "_source" : { ... trimmed ... }, } ``` 这里加入了该文档来自于哪个节点哪个分片上的信息，这对我们是比较有帮助的，因为词频率和 文档频率是在每个分片中计算出来的，而不是每个索引中： ```Javascript "_shard" : 1, "_node" : "mzIVYCsqSWCG_M_ZffSs9Q", ``` 然后返回值中的 `_explanation` 会包含在每一个入口，告诉你采用了哪种计算方式，并让你知道计算的结果以及其他详情： ```Javascript "_explanation": { "description": "weight(tweet:honeymoon in 0) [PerFieldSimilarity], result of:", "value": 0.076713204, "details": [ { "description": "fieldWeight in 0, product of:", "value": 0.076713204, "details": [ { "description": "tf(freq=1.0), with freq of:", "value": 1, "details": [ { "description": "termFreq=1.0", "value": 1 } ] }, { "description": "idf(docFreq=1, maxDocs=1)", "value": 0.30685282 }, { "description": "fieldNorm(doc=0)", "value": 0.25, } ] } ] } ``` `honeymoon` 相关性评分计算的总结 检索词频率 反向文档频率 字段长度准则 >**重要**： > 输出 `explain` 结果代价是十分昂贵的，它只能用作调试工具 > --千万不要用于生产环境。 第一部分是关于计算的总结。告诉了我们 `"honeymoon"` 在 `tweet`字段中的检索词频率/反向文档频率或 TF/IDF， （这里的文档 `0` 是一个内部的ID，跟我们没有关系，可以忽略。） 然后解释了计算的权重是如何计算出来的： 检索词频率: 检索词 `honeymoon` 在 `tweet` 字段中的出现次数。 反向文档频率: 检索词 `honeymoon` 在 `tweet` 字段在当前文档出现次数与索引中其他文档的出现总数的比率。 字段长度准则: 文档中 `tweet` 字段内容的长度 -- 内容越长，值越小。 复杂的查询语句解释也非常复杂，但是包含的内容与上面例子大致相同。 通过这段描述我们可以了解搜索结果是如何产生的。 >**提示**： >JSON形式的explain描述是难以阅读的 >但是转成 YAML 会好很多，只需要在参数中加上 `format=yaml` ## Explain Api #### 文档是如何被匹配到的 当`explain`选项加到某一文档上时，它会告诉你为何这个文档会被匹配，以及一个文档为何没有被匹配。 请求路径为 `/index/type/id/_explain`, 如下所示： ```Javascript GET /us/tweet/12/_explain { "query" : { "filtered" : { "filter" : { "term" : { "user_id" : 2 }}, "query" : { "match" : { "tweet" : "honeymoon" }} } } } ``` 除了上面我们看到的完整描述外，我们还可以看到这样的描述： ```Javascript "failure to match filter: cache(user_id:[2 TO 2])" ``` 也就是说我们的 `user_id` 过滤子句使该文档不能匹配到。


数据字段
本章的目的在于介绍关于ElasticSearch内部的一些运行情况。在这里我们先不介绍新的知识点， 数据字段是我们要经常查阅的内容之一，但我们使用的时候不必太在意。

当你对一个字段进行排序时，ElasticSearch 需要进入每个匹配到的文档得到相关的值。 倒排索引在用于搜索时是非常卓越的，但却不是理想的排序结构。

当搜索的时候，我们需要用检索词去遍历所有的文档。

当排序的时候，我们需要遍历文档中所有的值，我们需要做反倒序排列操作。
为了提高排序效率，ElasticSearch 会将所有字段的值加载到内存中，这就叫做"数据字段"。

重要： ElasticSearch将所有字段数据加载到内存中并不是匹配到的那部分数据。 而是索引下所有文档中的值，包括所有类型。
将所有字段数据加载到内存中是因为从硬盘反向倒排索引是非常缓慢的。尽管你这次请求需要的是某些文档中的部分数据， 但你下个请求却需要另外的数据，所以将所有字段数据一次性加载到内存中是十分必要的。

ElasticSearch中的字段数据常被应用到以下场景：

对一个字段进行排序
对一个字段进行聚合
某些过滤，比如地理位置过滤
某些与字段相关的脚本计算
毫无疑问，这会消耗掉很多内存，尤其是大量的字符串数据 -- string字段可能包含很多不同的值，比如邮件内容。 值得庆幸的是，内存不足是可以通过横向扩展解决的，我们可以增加更多的节点到集群。

现在，你只需要知道字段数据是什么，和什么时候内存不足就可以了。 稍后我们会讲述字段数据到底消耗了多少内存，如何限制ElasticSearch可以使用的内存，以及如何预加载字段数据以提高用户体验。










分布式搜索的执行方式

在继续之前，我们将绕道讲一下搜索是如何在分布式环境中执行的。 它比我们之前讲的基础的增删改查(create-read-update-delete ，CRUD)请求要复杂一些。

注意：

本章的信息只是出于兴趣阅读，使用Elasticsearch并不需要理解和记住这里的所有细节。

阅读这一章只是增加对系统如何工作的了解，并让你知道这些信息以备以后参考，所以别淹没在细节里。
一个CRUD操作只处理一个单独的文档。文档的唯一性由_index, _type和routing-value（通常默认是该文档的_id）的组合来确定。这意味着我们可以准确知道集群中的哪个分片持有这个文档。

由于不知道哪个文档会匹配查询（文档可能存放在集群中的任意分片上），所以搜索需要一个更复杂的模型。一个搜索不得不通过查询每一个我们感兴趣的索引的分片副本，来看是否含有任何匹配的文档。

但是，找到所有匹配的文档只完成了这件事的一半。在搜索（search）API返回一页结果前，来自多个分片的结果必须被组合放到一个有序列表中。因此，搜索的执行过程分两个阶段，称为查询然后取回（query then fetch）。

查询阶段
在初始化查询阶段（query phase），查询被向索引中的每个分片副本（原本或副本）广播。每个分片在本地执行搜索并且建立了匹配document的优先队列（priority queue）。

优先队列

一个优先队列（priority queue is）只是一个存有前n个（top-n）匹配document的有序列表。这个优先队列的大小由分页参数from和size决定。例如，下面这个例子中的搜索请求要求优先队列要能够容纳100个document
GET /_search
{
    "from": 90,
    "size": 10
}
这个查询的过程被描述在图分布式搜索查询阶段中。

Query phase of distributed search

图1 分布式搜索查询阶段

查询阶段包含以下三步：

1.客户端发送一个search（搜索）请求给Node 3,Node 3创建了一个长度为from+size的空优先级队列。 2.Node 3 转发这个搜索请求到索引中每个分片的原本或副本。每个分片在本地执行这个查询并且结果将结果到一个大小为from+size的有序本地优先队列里去。 3.每个分片返回document的ID和它优先队列里的所有document的排序值给协调节点Node 3。Node 3把这些值合并到自己的优先队列里产生全局排序结果。

当一个搜索请求被发送到一个节点Node，这个节点就变成了协调节点。这个节点的工作是向所有相关的分片广播搜索请求并且把它们的响应整合成一个全局的有序结果集。这个结果集会被返回给客户端。

第一步是向索引里的每个节点的分片副本广播请求。就像document的GET请求一样，搜索请求可以被每个分片的原本或任意副本处理。这就是更多的副本（当结合更多的硬件时）如何提高搜索的吞吐量的方法。对于后续请求，协调节点会轮询所有的分片副本以分摊负载。

每一个分片在本地执行查询和建立一个长度为from+size的有序优先队列——这个长度意味着它自己的结果数量就足够满足全局的请求要求。分片返回一个轻量级的结果列表给协调节点。只包含documentID值和排序需要用到的值，例如_score。

协调节点将这些分片级的结果合并到自己的有序优先队列里。这个就代表了最终的全局有序结果集。到这里，查询阶段结束。

注意

一个索引可以由一个或多个原始分片组成，所以一个对于单个索引的搜索请求也需要能够把来自多个分片的结果组合起来。一个对于 _多（multiple）_或_全部（all）_索引的搜索的工作机制和这完全一致——仅仅是多了一些分片而已。


取回阶段
查询阶段辨别出那些满足搜索请求的document，但我们仍然需要取回那些document本身。这就是取回阶段的工作，如图分布式搜索的取回阶段所示。

Fetch phase of distributed search

图2 分布式搜索取回阶段

分发阶段由以下步骤构成：

1.协调节点辨别出哪个document需要取回，并且向相关分片发出GET请求。

2.每个分片加载document并且根据需要_丰富（enrich）_它们，然后再将document返回协调节点。

3.一旦所有的document都被取回，协调节点会将结果返回给客户端。

协调节点先决定哪些document是_实际（actually）_需要取回的。例如，我们指定查询{ "from": 90, "size": 10 }，那么前90条将会被丢弃，只有之后的10条会需要取回。这些document可能来自与原始查询请求相关的某个、某些或者全部分片。

协调节点为每个持有相关document的分片建立多点get请求然后发送请求到处理查询阶段的分片副本。

分片加载document主体——_source field。如果需要，还会根据元数据丰富结果和高亮搜索片断。一旦协调节点收到所有结果，会将它们汇集到单一的回答响应里，这个响应将会返回给客户端。

深分页

查询然后取回过程虽然支持通过使用from和size参数进行分页，但是要在有限范围内（within limited）。还记得每个分片必须构造一个长度为from+size的优先队列吧，所有这些都要传回协调节点。这意味着协调节点要通过对分片数量 * (from + size)个document进行排序来找到正确的size个document。

根据document的数量，分片的数量以及所使用的硬件，对10,000到50,000条结果（1,000到5,000页）深分页是可行的。但是对于足够大的from值，排序过程将会变得非常繁重，会使用巨大量的CPU，内存和带宽。因此，强烈不建议使用深分页。

在实际中，“深分页者”也是很少的一部人。一般人会在翻了两三页后就停止翻页，并会更改搜索标准。那些不正常情况通常是机器人或者网络爬虫的行为。它们会持续不断地一页接着一页地获取页面直到服务器到崩溃的边缘。

如果你确实需要从集群里获取大量documents，你可以通过设置搜索类型scan禁用排序，来高效地做这件事。这一点将在后面的章节讨论。

搜索选项
一些查询字符串（query-string）可选参数能够影响搜索过程。

preference（偏爱）

preference参数允许你控制使用哪个分片或节点来处理搜索请求。她接受如下一些参数 _primary， _primary_first， _local， _only_node:xyz， _prefer_node:xyz和_shards:2,3。这些参数在文档搜索偏好（search preference）里有详细描述。

然而通常最有用的值是一些随机字符串，它们可以避免结果震荡问题（the bouncing results problem）。

结果震荡（Bouncing Results）

想像一下，你正在按照timestamp字段来对你的结果排序，并且有两个document有相同的timestamp。由于搜索请求是在所有有效的分片副本间轮询的，这两个document可能在原始分片里是一种顺序，在副本分片里是另一种顺序。

这就是被称为_结果震荡（bouncing results）_的问题：用户每次刷新页面，结果顺序会发生变化。避免这个问题方法是对于同一个用户总是使用同一个分片。方法就是使用一个随机字符串例如用户的会话ID（session ID）来设置preference参数。
timeout（超时）

通常，协调节点会等待接收所有分片的回答。如果有一个节点遇到问题，它会拖慢整个搜索请求。

timeout参数告诉协调节点最多等待多久，就可以放弃等待而将已有结果返回。返回部分结果总比什么都没有好。

搜索请求的返回将会指出这个搜索是否超时，以及有多少分片成功答复了：

    ...
    "timed_out":     true,  (1)
    "_shards": {
       "total":      5,
       "successful": 4,
       "failed":     1     (2)
    },
    ...
(1) 搜索请求超时。

(2) 五个分片中有一个没在超时时间内答复。

如果一个分片的所有副本都因为其他原因失败了——也许是因为硬件故障——这个也同样会反映在该答复的_shards部分里。

routing（路由选择）

在路由值那节里，我们解释了如何在建立索引时提供一个自定义的routing参数来保证所有相关的document（如属于单个用户的document）被存放在一个单独的分片中。在搜索时，你可以指定一个或多个routing 值来限制只搜索那些分片而不是搜索index里的全部分片：

GET /_search?routing=user_1,user2
这个技术在设计非常大的搜索系统时就会派上用场了。我们在规模（scale）那一章里详细讨论它。

search_type（搜索类型）

虽然query_then_fetch是默认的搜索类型，但也可以根据特定目的指定其它的搜索类型，例如：

GET /_search?search_type=count
count（计数）

count（计数）搜索类型只有一个query（查询）的阶段。当不需要搜索结果只需要知道满足查询的document的数量时，可以使用这个查询类型。

_query_andfetch（查询并且取回）

query_and_fetch（查询并且取回）搜索类型将查询和取回阶段合并成一个步骤。这是一个内部优化选项，当搜索请求的目标只是一个分片时可以使用，例如指定了routing（路由选择）值时。虽然你可以手动选择使用这个搜索类型，但是这么做基本上不会有什么效果。

_dfs_query_then_fetch__ 和 dfs_query_and_fetch___

dfs搜索类型有一个预查询的阶段，它会从全部相关的分片里取回项目频数来计算全局的项目频数。我们将在relevance-is-broken（相关性被破坏）里进一步讨论这个。

scan（扫描）

scan（扫描）搜索类型是和scroll（滚屏）API连在一起使用的，可以高效地取回巨大数量的结果。它是通过禁用排序来实现的。我们将在下一节_scan-and-scroll（扫描和滚屏）_里讨论它。

扫描和滚屏
scan（扫描）搜索类型是和scroll（滚屏）API一起使用来从Elasticsearch里高效地取回巨大数量的结果而不需要付出深分页的代价。

scroll（滚屏）

一个滚屏搜索允许我们做一个初始阶段搜索并且持续批量从Elasticsearch里拉取结果直到没有结果剩下。这有点像传统数据库里的cursors（游标）。

滚屏搜索会及时制作快照。这个快照不会包含任何在初始阶段搜索请求后对index做的修改。它通过将旧的数据文件保存在手边，所以可以保护index的样子看起来像搜索开始时的样子。

scan（扫描）

深度分页代价最高的部分是对结果的全局排序，但如果禁用排序，就能以很低的代价获得全部返回结果。为达成这个目的，可以采用scan（扫描）搜索模式。扫描模式让Elasticsearch不排序，只要分片里还有结果可以返回，就返回一批结果。

为了使用scan-and-scroll（扫描和滚屏），需要执行一个搜索请求，将search_type 设置成scan，并且传递一个scroll参数来告诉Elasticsearch滚屏应该持续多长时间。

GET /old_index/_search?search_type=scan&scroll=1m (1)
{
    "query": { "match_all": {}},
    "size":  1000
}
（1）保持滚屏开启1分钟。

这个请求的应答没有包含任何命中的结果，但是包含了一个Base-64编码的_scroll_id（滚屏id）字符串。现在我们可以将_scroll_id 传递给_search/scroll末端来获取第一批结果：

GET /_search/scroll?scroll=1m      (1)
c2Nhbjs1OzExODpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzExOTpRNV9aY1VyUVM4U0 <2>
NMd2pjWlJ3YWlBOzExNjpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzExNzpRNV9aY1Vy
UVM4U0NMd2pjWlJ3YWlBOzEyMDpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzE7dG90YW
xfaGl0czoxOw==
(1) 保持滚屏开启另一分钟。

(2) _scroll_id 可以在body或者URL里传递，也可以被当做查询参数传递。

注意，要再次指定?scroll=1m。滚屏的终止时间会在我们每次执行滚屏请求时刷新，所以他只需要给我们足够的时间来处理当前批次的结果而不是所有的匹配查询的document。

这个滚屏请求的应答包含了第一批次的结果。虽然指定了一个1000的size ，但是获得了更多的document。当扫描时，size被应用到每一个分片上，所以我们在每个批次里最多或获得size * number_of_primary_shards（size*主分片数）个document。

注意：

滚屏请求也会返回一个_新_的_scroll_id。每次做下一个滚屏请求时，必须传递前一次请求返回的_scroll_id。
如果没有更多的命中结果返回，就处理完了所有的命中匹配的document。

提示：

一些Elasticsearch官方客户端提供_扫描和滚屏_的小助手。小助手提供了一个对这个功能的简单封装。









索引管理
我们已经看到Elasticsearch如何在不需要任何预先计划和设置的情况下，轻松地开发一个新的应用。并且，在你想调整索引和搜索过程来更好地适应你特殊的使用需求前，不会花较长的时间。它包含几乎所有的和索引及类型相关的定制选项。在这一章，将介绍管理索引和类型映射的API以及最重要的设置。

创建索引

迄今为止，我们简单的通过添加一个文档的方式创建了一个索引。这个索引使用默认设置，新的属性通过动态映射添加到分类中。现在我们需要对这个过程有更多的控制：我们需要确保索引被创建在适当数量的分片上，在索引数据_之前_设置好分析器和类型映射。

为了达到目标，我们需要手动创建索引，在请求中加入所有设置和类型映射，如下所示：

PUT /my_index
{
    "settings": { ... any settings ... },
    "mappings": {
        "type_one": { ... any mappings ... },
        "type_two": { ... any mappings ... },
        ...
    }
事实上，你可以通过在 config/elasticsearch.yml 中添加下面的配置来防止自动创建索引。

action.auto_create_index: false
NOTE

今后，我们将介绍怎样用【索引模板】来自动预先配置索引。这在索引日志数据时尤其有效： 你将日志数据索引在一个以日期结尾的索引上，第二天，一个新的配置好的索引会自动创建好。
删除索引

使用以下的请求来删除索引：

DELETE /my_index
你也可以用下面的方式删除多个索引

DELETE /index_one,index_two
DELETE /index_*
你甚至可以删除所有索引

DELETE /_all


索引设置

你可以通过很多种方式来自定义索引行为，你可以阅读Index Modules reference documentation，但是：

提示: Elasticsearch 提供了优化好的默认配置。除非你明白这些配置的行为和为什么要这么做，请不要修改这些配置。

下面是两个最重要的设置：

number_of_shards

定义一个索引的主分片个数，默认值是 `5`。这个配置在索引创建后不能修改。
number_of_replicas

每个主分片的复制分片个数，默认是 `1`。这个配置可以随时在活跃的索引上修改。
例如，我们可以创建只有一个主分片，没有复制分片的小索引。

PUT /my_temp_index
{
    "settings": {
        "number_of_shards" :   1,
        "number_of_replicas" : 0
    }
}
然后，我们可以用 update-index-settings API 动态修改复制分片个数：

PUT /my_temp_index/_settings
{
    "number_of_replicas": 1
}

配置分析器

第三个重要的索引设置是 analysis 部分，用来配置已存在的分析器或创建自定义分析器来定制化你的索引。

在【分析器介绍】中，我们介绍了一些内置的分析器，用于将全文字符串转换为适合搜索的倒排索引。

standard 分析器是用于全文字段的默认分析器，对于大部分西方语系来说是一个不错的选择。它考虑了以下几点：

standard 分词器，在词层级上分割输入的文本。
standard 标记过滤器，被设计用来整理分词器触发的所有标记（但是目前什么都没做）。
lowercase 标记过滤器，将所有标记转换为小写。
stop 标记过滤器，删除所有可能会造成搜索歧义的停用词，如 a，the，and，is。
默认情况下，停用词过滤器是被禁用的。如需启用它，你可以通过创建一个基于 standard 分析器的自定义分析器，并且设置 stopwords 参数。可以提供一个停用词列表，或者使用一个特定语言的预定停用词列表。

在下面的例子中，我们创建了一个新的分析器，叫做 es_std，并使用预定义的西班牙语停用词：

PUT /spanish_docs
{
    "settings": {
        "analysis": {
            "analyzer": {
                "es_std": {
                    "type":      "standard",
                    "stopwords": "_spanish_"
                }
            }
        }
    }
}
es_std 分析器不是全局的，它仅仅存在于我们定义的 spanish_docs 索引中。为了用 analyze API 来测试它，我们需要使用特定的索引名。

GET /spanish_docs/_analyze?analyzer=es_std
El veloz zorro marrón
下面简化的结果中显示停用词 El 被正确的删除了：

{
  "tokens" : [
    { "token" :    "veloz",   "position" : 2 },
    { "token" :    "zorro",   "position" : 3 },
    { "token" :    "marrón",  "position" : 4 }
  ]
}


自定义分析器

虽然 Elasticsearch 内置了一系列的分析器，但是真正的强大之处在于定制你自己的分析器。你可以通过在配置文件中组合字符过滤器，分词器和标记过滤器，来满足特定数据的需求。

在 【分析器介绍】 中，我们提到 分析器 是三个顺序执行的组件的结合（字符过滤器，分词器，标记过滤器）。

字符过滤器

字符过滤器是让字符串在被分词前变得更加“整洁”。例如，如果我们的文本是 HTML 格式，它可能会包含一些我们不想被索引的 HTML 标签，诸如 <p> 或 <div>。

我们可以使用 html_strip 字符过滤器 来删除所有的 HTML 标签，并且将 HTML 实体转换成对应的 Unicode 字符，比如将 &Aacute; 转成 Á。

一个分析器可能包含零到多个字符过滤器。
分词器

一个分析器 必须 包含一个分词器。分词器将字符串分割成单独的词（terms）或标记（tokens）。standard 分析器使用 standard 分词器将字符串分割成单独的字词，删除大部分标点符号，但是现存的其他分词器会有不同的行为特征。

例如，keyword 分词器输出和它接收到的相同的字符串，不做任何分词处理。[whitespace 分词器]只通过空格来分割文本。[pattern 分词器]可以通过正则表达式来分割文本。
标记过滤器

分词结果的 标记流 会根据各自的情况，传递给特定的标记过滤器。

标记过滤器可能修改，添加或删除标记。我们已经提过 lowercase 和 stop 标记过滤器，但是 Elasticsearch 中有更多的选择。stemmer 标记过滤器将单词转化为他们的根形态（root form）。ascii_folding 标记过滤器会删除变音符号，比如从 très 转为 tres。 ngram 和 edge_ngram 可以让标记更适合特殊匹配情况或自动完成。
在【深入搜索】中，我们将举例介绍如何使用这些分词器和过滤器。但是首先，我们需要阐述一下如何创建一个自定义分析器

创建自定义分析器

与索引设置一样，我们预先配置好 es_std 分析器，我们可以再 analysis 字段下配置字符过滤器，分词器和标记过滤器：

PUT /my_index
{
    "settings": {
        "analysis": {
            "char_filter": { ... custom character filters ... },
            "tokenizer":   { ...    custom tokenizers     ... },
            "filter":      { ...   custom token filters   ... },
            "analyzer":    { ...    custom analyzers      ... }
        }
    }
}
作为例子，我们来配置一个这样的分析器：

用 html_strip 字符过滤器去除所有的 HTML 标签

将 & 替换成 and，使用一个自定义的 mapping 字符过滤器
"char_filter": {
    "&_to_and": {
        "type":       "mapping",
        "mappings": [ "&=> and "]
    }
}
使用 standard 分词器分割单词

使用 lowercase 标记过滤器将词转为小写

用 stop 标记过滤器去除一些自定义停用词。
"filter": {
    "my_stopwords": {
        "type":        "stop",
        "stopwords": [ "the", "a" ]
    }
}
根据以上描述来将预定义好的分词器和过滤器组合成我们的分析器：

"analyzer": {
    "my_analyzer": {
        "type":           "custom",
        "char_filter":  [ "html_strip", "&_to_and" ],
        "tokenizer":      "standard",
        "filter":       [ "lowercase", "my_stopwords" ]
    }
}
用下面的方式可以将以上请求合并成一条：

PUT /my_index
{
    "settings": {
        "analysis": {
            "char_filter": {
                "&_to_and": {
                    "type":       "mapping",
                    "mappings": [ "&=> and "]
            }},
            "filter": {
                "my_stopwords": {
                    "type":       "stop",
                    "stopwords": [ "the", "a" ]
            }},
            "analyzer": {
                "my_analyzer": {
                    "type":         "custom",
                    "char_filter":  [ "html_strip", "&_to_and" ],
                    "tokenizer":    "standard",
                    "filter":       [ "lowercase", "my_stopwords" ]
            }}
}}}
创建索引后，用 analyze API 来测试新的分析器：

GET /my_index/_analyze?analyzer=my_analyzer
The quick & brown fox
下面的结果证明我们的分析器能正常工作了：

{
  "tokens" : [
      { "token" :   "quick",    "position" : 2 },
      { "token" :   "and",      "position" : 3 },
      { "token" :   "brown",    "position" : 4 },
      { "token" :   "fox",      "position" : 5 }
    ]
}
除非我们告诉 Elasticsearch 在哪里使用，否则分析器不会起作用。我们可以通过下面的映射将它应用在一个 string 类型的字段上：

PUT /my_index/_mapping/my_type
{
    "properties": {
        "title": {
            "type":      "string",
            "analyzer":  "my_analyzer"
        }
    }
}


类型和映射

类型 在 Elasticsearch 中表示一组相似的文档。类型 由一个 名称（比如 user 或 blogpost）和一个类似数据库表结构的映射组成，描述了文档中可能包含的每个字段的 属性，数据类型（比如 string, integer 或 date），和是否这些字段需要被 Lucene 索引或储存。

在【文档】一章中，我们说过类型类似关系型数据库中的表格，一开始你可以这样做类比，但是现在值得更深入阐释一下什么是类型，且在 Lucene 中是怎么实现的。

Lucene 如何处理文档

Lucene 中，一个文档由一组简单的键值对组成，一个字段至少需要有一个值，但是任何字段都可以有多个值。类似的，一个单独的字符串可能在分析过程中被转换成多个值。Lucene 不关心这些值是字符串，数字或日期，所有的值都被当成 不透明字节

当我们在 Lucene 中索引一个文档时，每个字段的值都被加到相关字段的倒排索引中。你也可以选择将原始数据 储存 起来以备今后取回。

类型是怎么实现的

Elasticsearch 类型是在这个简单基础上实现的。一个索引可能包含多个类型，每个类型有各自的映射和文档，保存在同一个索引中。

因为 Lucene 没有文档类型的概念，每个文档的类型名被储存在一个叫 _type 的元数据字段上。当我们搜索一种特殊类型的文档时，Elasticsearch 简单的通过 _type 字段来过滤出这些文档。

Lucene 同样没有映射的概念。映射是 Elasticsearch 将复杂 JSON 文档_映射_成 Lucene 需要的扁平化数据的方式。

例如，user 类型中 name 字段的映射声明这个字段是一个 string 类型，在被加入倒排索引之前，它的数据需要通过 whitespace 分析器来分析。

"name": {
    "type":     "string",
    "analyzer": "whitespace"
}
预防类型陷阱

事实上不同类型的文档可以被加到同一个索引里带来了一些预想不到的困难。

想象一下我们的索引中有两种类型：blog_en 表示英语版的博客，blog_es 表示西班牙语版的博客。两种类型都有 title 字段，但是其中一种类型使用 english 分析器，另一种使用 spanish 分析器。

使用下面的查询就会遇到问题：

GET /_search
{
    "query": {
        "match": {
            "title": "The quick brown fox"
        }
    }
}
我们在两种类型中搜索 title 字段，首先需要分析查询语句，但是应该使用哪种分析器呢，spanish 还是 english？Elasticsearch 会采用第一个被找到的 title 字段使用的分析器，这对于这个字段的文档来说是正确的，但对另一个来说却是错误的。

我们可以通过给字段取不同的名字来避免这种错误 —— 比如，用 title_en 和 title_es。或者在查询中明确包含各自的类型名。

GET /_search
{
    "query": {
        "multi_match": { <1>
            "query":    "The quick brown fox",
            "fields": [ "blog_en.title", "blog_es.title" ]
        }
    }
}
`multi_match` 查询在多个字段上执行 `match` 查询并一起返回结果。 新的查询中 `english` 分析器用于 `blog_en.title` 字段，`spanish` 分析器用于 `blog_es.title` 字段，然后通过综合得分组合两种字段的结果。 这种办法对具有相同数据类型的字段有帮助，但是想象一下如果你将下面两个文档加入同一个索引，会发生什么： * 类型: user ``` { "login": "john_smith" } ``` * 类型: event ``` { "login": "2014-06-01" } ``` Lucene 不在乎一个字段是字符串而另一个字段是日期，它会一视同仁的索引这两个字段。 然而，假如我们试图 _排序_ `event.login` 字段，Elasticsearch 需要将 `login` 字段的值加载到内存中。像我们在 【字段数据介绍】中提到的，它将 _任意文档_ 的值加入索引而不管它们的类型。 它会尝试加载这些值为字符串或日期，取决于它遇到的第一个 `login` 字段。这可能会导致预想不到的结果或者以失败告终。 提示：为了保证你不会遇到这些冲突，建议在同一个索引的每一个类型中，确保用_同样的方式_映射_同名_的字段


根对象

映射的最高一层被称为 根对象，它可能包含下面几项：

一个 properties 节点，列出了文档中可能包含的每个字段的映射

多个元数据字段，每一个都以下划线开头，例如 _type, _id 和 _source

设置项，控制如何动态处理新的字段，例如 analyzer, dynamic_date_formats 和 dynamic_templates。

其他设置，可以同时应用在根对象和其他 object 类型的字段上，例如 enabled, dynamic 和 include_in_all
属性

我们已经在【核心字段】和【复合核心字段】章节中介绍过文档字段和属性的三个最重要的设置：

type： 字段的数据类型，例如 string 和 date

index： 字段是否应当被当成全文来搜索（analyzed），或被当成一个准确的值（not_analyzed），还是完全不可被搜索（no）

analyzer： 确定在索引和或搜索时全文字段使用的 分析器。

我们将在下面的章节中介绍其他字段，例如 ip, geo_point 和 geo_shape


元数据：_source 字段

默认情况下，Elasticsearch 用 JSON 字符串来表示文档主体保存在 _source 字段中。像其他保存的字段一样，_source 字段也会在写入硬盘前压缩。

这几乎始终是需要的功能，因为：

搜索结果中能得到完整的文档 —— 不需要额外去别的数据源中查询文档

如果缺少 _source 字段，部分 更新 请求不会起作用

当你的映射有变化，而且你需要重新索引数据时，你可以直接在 Elasticsearch 中操作而不需要重新从别的数据源中取回数据。

你可以从 _source 中通过 get 或 search 请求取回部分字段，而不是整个文档。

这样更容易排查错误，因为你可以准确的看到每个文档中包含的内容，而不是只能从一堆 ID 中猜测他们的内容。
即便如此，存储 _source 字段还是要占用硬盘空间的。假如上面的理由对你来说不重要，你可以用下面的映射禁用 _source 字段：

PUT /my_index
{
    "mappings": {
        "my_type": {
            "_source": {
                "enabled":  false
            }
        }
    }
}
在搜索请求中你可以通过限定 _source 字段来请求指定字段：

GET /_search
{
    "query":   { "match_all": {}},
    "_source": [ "title", "created" ]
}
这些字段会从 _source 中提取出来，而不是返回整个 _source 字段。

储存字段

除了索引字段的值，你也可以选择 储存 字段的原始值以备日后取回。使用 Lucene 做后端的用户用_储存字段_来选择搜索结果的返回值，事实上，_source 字段就是一个储存字段。

在 Elasticsearch 中，单独设置储存字段不是一个好做法。完整的文档已经被保存在 _source 字段中。通常最好的办法会是使用 _source 参数来过滤你需要的字段。


元数据：_all 字段

在【简单搜索】中，我们介绍了 _all 字段：一个所有其他字段值的特殊字符串字段。query_string 在没有指定字段时默认用 _all 字段查询。

_all 字段在新应用的探索阶段比较管用，当你还不清楚最终文档的结构时，可以将任何查询用于这个字段，就有机会得到你想要的文档：

GET /_search
{
    "match": {
        "_all": "john smith marketing"
    }
}
随着你应用的发展，搜索需求会变得更加精准。你会越来越少的使用 _all 字段。_all 是一种简单粗暴的搜索方式。通过查询独立的字段，你能更灵活，强大和精准的控制搜索结果，提高相关性。

提示

【相关性算法】考虑的一个最重要的原则是字段的长度：字段越短，就越重要。在较短的 title 字段中的短语会比较长的 content 字段中的短语显得更重要。而字段间的这种差异在 _all 字段中就不会出现

如果你决定不再使用 _all 字段，你可以通过下面的映射禁用它：

PUT /my_index/_mapping/my_type
{
    "my_type": {
        "_all": { "enabled": false }
    }
}
通过 include_in_all 选项可以控制字段是否要被包含在 _all 字段中，默认值是 true。在一个对象上设置 include_in_all 可以修改这个对象所有字段的默认行为。

你可能想要保留 _all 字段来查询所有特定的全文字段，例如 title, overview, summary 和 tags。相对于完全禁用 _all 字段，你可以先默认禁用 include_in_all 选项，而选定字段上启用 include_in_all。

PUT /my_index/my_type/_mapping
{
    "my_type": {
        "include_in_all": false,
        "properties": {
            "title": {
                "type":           "string",
                "include_in_all": true
            },
            ...
        }
    }
}
谨记 _all 字段仅仅是一个经过分析的 string 字段。它使用默认的分析器来分析它的值，而不管这值本来所在的字段指定的分析器。而且像所有 string 类型字段一样，你可以配置 _all 字段使用的分析器：

PUT /my_index/my_type/_mapping
{
    "my_type": {
        "_all": { "analyzer": "whitespace" }
    }
}

文档 ID

文档唯一标识由四个元数据字段组成：

_id：文档的字符串 ID

_type：文档的类型名

_index：文档所在的索引

_uid：_type 和 _id 连接成的 type#id

默认情况下，_uid 是被保存（可取回）和索引（可搜索）的。_type 字段被索引但是没有保存，_id 和 _index 字段则既没有索引也没有储存，它们并不是真实存在的。

尽管如此，你仍然可以像真实字段一样查询 _id 字段。Elasticsearch 使用 _uid 字段来追溯 _id。虽然你可以修改这些字段的 index 和 store 设置，但是基本上不需要这么做。

_id 字段有一个你可能用得到的设置：path 设置告诉 Elasticsearch 它需要从文档本身的哪个字段中生成 _id

PUT /my_index
{
    "mappings": {
        "my_type": {
            "_id": {
                "path": "doc_id" <1>
            },
            "properties": {
                "doc_id": {
                    "type":   "string",
                    "index":  "not_analyzed"
                }
            }
        }
    }
}
从 `doc_id` 字段生成 `_id` 然后，当你索引一个文档时： ``` POST /my_index/my_type { "doc_id": "123" } ``` `_id` 值由文档主体的 `doc_id` 字段生成。 ``` { "_index": "my_index", "_type": "my_type", "_id": "123", "_version": 1, "created": true } ``` `_id` 正确的生成了。 警告：虽然这样很方便，但是注意它对 `bulk` 请求（见【bulk 格式】）有个轻微的性能影响。处理请求的节点将不能仅靠解析元数据行来决定将请求分配给哪一个分片，而需要解析整个文档主体。


动态映射

当 Elasticsearch 遭遇一个位置的字段时，它通过【动态映射】来确定字段的数据类型且自动将该字段加到类型映射中。

有时这是理想的行为，有时却不是。或许你不知道今后会有哪些字段加到文档中，但是你希望它们能自动被索引。或许你仅仅想忽略它们。特别是当你使用 Elasticsearch 作为主数据源时，你希望未知字段能抛出一个异常来警示你。

幸运的是，你可以通过 dynamic 设置来控制这些行为，它接受下面几个选项：

true：自动添加字段（默认）

false：忽略字段

strict：当遇到未知字段时抛出异常

dynamic 设置可以用在根对象或任何 object 对象上。你可以将 dynamic 默认设置为 strict，而在特定内部对象上启用它：

PUT /my_index
{
    "mappings": {
        "my_type": {
            "dynamic":      "strict", <1>
            "properties": {
                "title":  { "type": "string"},
                "stash":  {
                    "type":     "object",
                    "dynamic":  true <2>
                }
            }
        }
    }
}
当遇到未知字段时，`my_type` 对象将会抛出异常 `stash` 对象会自动创建字段 通过这个映射，你可以添加一个新的可搜索字段到 `stash` 对象中： ``` PUT /my_index/my_type/1 { "title": "This doc adds a new field", "stash": { "new_field": "Success!" } } ``` 但是在顶层做同样的操作则会失败： ``` PUT /my_index/my_type/1 { "title": "This throws a StrictDynamicMappingException", "new_field": "Fail!" } ``` 备注：将 `dynamic` 设置成 `false` 完全不会修改 `_source` 字段的内容。`_source` 将仍旧保持你索引时的完整 JSON 文档。然而，没有被添加到映射的未知字段将不可被搜索。


自定义动态索引

如果你想在运行时的增加新的字段，你可能会开启动态索引。虽然有时动态映射的 规则 显得不那么智能，幸运的是我们可以通过设置来自定义这些规则。

日期检测

当 Elasticsearch 遇到一个新的字符串字段时，它会检测这个字段是否包含一个可识别的日期，比如 2014-01-01。如果它看起来像一个日期，这个字段会被作为 date 类型添加，否则，它会被作为 string 类型添加。

有些时候这个规则可能导致一些问题。想象你有一个文档长这样：

{ "note": "2014-01-01" }
假设这是第一次见到 note 字段，它会被添加为 date 字段，但是如果下一个文档像这样：

{ "note": "Logged out" }
这显然不是一个日期，但为时已晚。这个字段已经被添加为日期类型，这个 不合法的日期 将引发异常。

日期检测可以通过在根对象上设置 date_detection 为 false 来关闭：

PUT /my_index
{
    "mappings": {
        "my_type": {
            "date_detection": false
        }
    }
}
使用这个映射，字符串将始终是 string 类型。假如你需要一个 date 字段，你得手动添加它。

提示：

Elasticsearch 判断字符串为日期的规则可以通过 dynamic_date_formats 配置 来修改。

动态模板

使用 dynamic_templates，你可以完全控制新字段的映射，你设置可以通过字段名或数据类型应用一个完全不同的映射。

每个模板都有一个名字用于描述这个模板的用途，一个 mapping 字段用于指明这个映射怎么使用，和至少一个参数（例如 match）来定义这个模板适用于哪个字段。

模板按照顺序来检测，第一个匹配的模板会被启用。例如，我们给 string 类型字段定义两个模板：

es: 字段名以 _es 结尾需要使用 spanish 分析器。
en: 所有其他字段使用 english 分析器。
我们将 es 模板放在第一位，因为它比匹配所有字符串的 en 模板更特殊一点

PUT /my_index
{
    "mappings": {
        "my_type": {
            "dynamic_templates": [
                { "es": {
                      "match":              "*_es", <1>
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "spanish"
                      }
                }},
                { "en": {
                      "match":              "*", <2>
                      "match_mapping_type": "string",
                      "mapping": {
                          "type":           "string",
                          "analyzer":       "english"
                      }
                }}
            ]
}}}
匹配字段名以 `_es` 结尾的字段. 匹配所有字符串类型字段。 `match_mapping_type` 允许你限制模板只能使用在特定的类型上，就像由标准动态映射规则检测的一样，（例如 `strong` 和 `long`） `match` 参数只匹配字段名，`path_match` 参数则匹配字段在一个对象中的完整路径，所以 `address.*.name` 规则将匹配一个这样的字段： ``` { "address": { "city": { "name": "New York" } } } ``` `unmatch` 和 `path_unmatch` 规则将用于排除未被匹配的字段。 更多选项见[根对象参考文档](http://bit.ly/1wdHOzG)


默认映射

通常，一个索引中的所有类型具有共享的字段和设置。用 _default_ 映射来指定公用设置会更加方便，而不是每次创建新的类型时重复操作。_default 映射像新类型的模板。所有在 _default_ 映射 之后 的类型将包含所有的默认设置，除非在自己的类型映射中明确覆盖这些配置。

例如，我们可以使用 _default_ 映射对所有类型禁用 _all 字段，而只在 blog 字段上开启它：

PUT /my_index
{
    "mappings": {
        "_default_": {
            "_all": { "enabled":  false }
        },
        "blog": {
            "_all": { "enabled":  true  }
        }
    }
}
_default_ 映射也是定义索引级别的动态模板的好地方。

重新索引数据

虽然你可以给索引添加新的类型，或给类型添加新的字段，但是你不能添加新的分析器或修改已有字段。假如你这样做，已被索引的数据会变得不正确而你的搜索也不会正常工作。

修改在已存在的数据最简单的方法是重新索引：创建一个新配置好的索引，然后将所有的文档从旧的索引复制到新的上。

_source 字段的一个最大的好处是你已经在 Elasticsearch 中有了完整的文档，你不再需要从数据库中重建你的索引，这样通常会比较慢。

为了更高效的索引旧索引中的文档，使用【scan-scoll】来批量读取旧索引的文档，然后将通过【bulk API】来将它们推送给新的索引。

批量重新索引：

你可以在同一时间执行多个重新索引的任务，但是你显然不愿意它们的结果有重叠。所以，可以将重建大索引的任务通过日期或时间戳字段拆分成较小的任务：

GET /old_index/_search?search_type=scan&scroll=1m
{
    "query": {
        "range": {
            "date": {
                "gte":  "2014-01-01",
                "lt":   "2014-02-01"
            }
        }
    },
    "size":  1000
}
假如你继续在旧索引上做修改，你可能想确保新增的文档被加到了新的索引中。这可以通过重新运行重建索引程序来完成，但是记得只要过滤出上次执行后新增的文档就行了。

索引别名和零停机时间

前面提到的重新索引过程中的问题是必须更新你的应用，来使用另一个索引名。索引别名正是用来解决这个问题的！

索引 别名 就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何需要索引名的 API 使用。别名带给我们极大的灵活性，允许我们做到：

在一个运行的集群上无缝的从一个索引切换到另一个
给多个索引分类（例如，last_three_months）
给索引的一个子集创建 视图
我们以后会讨论更多别名的使用场景。现在我们将介绍用它们怎么在零停机时间内从旧的索引切换到新的索引。

这里有两种管理别名的途径：_alias 用于单个操作，_aliases 用于原子化多个操作。

在这一章中，我们假设你的应用采用一个叫 my_index 的索引。而事实上，my_index 是一个指向当前真实索引的别名。真实的索引名将包含一个版本号：my_index_v1, my_index_v2 等等。

开始，我们创建一个索引 my_index_v1，然后将别名 my_index 指向它：

PUT /my_index_v1 <1>
PUT /my_index_v1/_alias/my_index <2>
创建索引 `my_index_v1`。 将别名 `my_index` 指向 `my_index_v1`。 你可以检测这个别名指向哪个索引： ``` GET /*/_alias/my_index ``` 或哪些别名指向这个索引： ``` GET /my_index_v1/_alias/* ``` 两者都将返回下列值： ``` { "my_index_v1" : { "aliases" : { "my_index" : { } } } } ``` 然后，我们决定修改索引中一个字段的映射。当然我们不能修改现存的映射，索引我们需要重新索引数据。首先，我们创建有新的映射的索引 `my_index_v2`。 ``` PUT /my_index_v2 { "mappings": { "my_type": { "properties": { "tags": { "type": "string", "index": "not_analyzed" } } } } } ``` 然后我们从将数据从 `my_index_v1` 迁移到 `my_index_v2`，下面的过程在【重新索引】中描述过了。一旦我们认为数据已经被正确的索引了，我们就将别名指向新的索引。 别名可以指向多个索引，所以我们需要在新索引中添加别名的同时从旧索引中删除它。这个操作需要原子化，所以我们需要用 `_aliases` 操作： ``` POST /_aliases { "actions": [ { "remove": { "index": "my_index_v1", "alias": "my_index" }}, { "add": { "index": "my_index_v2", "alias": "my_index" }} ] } ``` 这样，你的应用就从旧索引迁移到了新的，而没有停机时间。 提示： 即使你认为现在的索引设计已经是完美的了，当你的应用在生产环境使用时，还是有可能在今后有一些改变的。 所以请做好准备：在应用中使用别名而不是索引。然后你就可以在任何时候重建索引。别名的开销很小，应当广泛使用。











深入分片

在分布式集群中，我们介绍了分片，把它描述为底层的工作单元。但分片到底是什么，它怎样工作？在这章节，我们将回答这些问题：

为什么搜索是近实时的？
为什么文档的CRUD操作是实时的？
ES怎样保证更新持久化，即使断电也不会丢失？
为什么删除文档不会立即释放空间？
什么是refresh，flush, optimize API，以及什么时候你该使用它们？
为了理解分片如何工作，最简单的方式是从一堂历史课开始。我们将会看下，为了提供一个有近实时搜索和分析功能的分布式、持久化的搜索引擎需要解决哪些问题。

内容提示：

这章的内容是为了满足你的兴趣。为了使用ES，你不需要懂得并记住所有细节。阅读这章是为了感受下ES内部是如何运转的以及相关信息在哪，以备不时之需。但是不要被这些细节吓到。



使文本可以被搜索

第一个不得不解决的挑战是如何让文本变得可搜索。在传统的数据库中，一个字段存一个值，但是这对于全文搜索是不足的。想要让文本中的每个单词都可以被搜索，这意味这数据库需要存多个值。

支持一个字段多个值的最佳数据结构是倒排索引。倒排索引包含了出现在所有文档中唯一的值或词的有序列表，以及每个词所属的文档列表。

 Term  | Doc 1 | Doc 2 | Doc 3 | ...
 ------------------------------------
 brown |   X   |       |  X    | ...
 fox   |   X   |   X   |  X    | ...
 quick |   X   |   X   |       | ...
 the   |   X   |       |  X    | ...
注意

当讨论倒排索引时，我们说的是把文档加入索引。因为之前，一个倒排索引是用来索引整个非结构化的文本文档。ES的中文档是一个结构化的JSON文档。实际上，每一个JSON文档中被索引的字段都有它自己的倒排索引。
倒排索引存储了比包含了一个特定term的文档列表多地多的信息。它可能存储包含每个term的文档数量，一个term出现在指定文档中的频次，每个文档中term的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息让Elasticsearch知道哪些term更重要，哪些文档更重要，也就是相关性。

需要意识到，为了实现倒排索引预期的功能，它必须要知道集合中所有的文档。

在全文检索的早些时候，会为整个文档集合建立一个大索引，并且写入磁盘。只有新的索引准备好了，它就会替代旧的索引，最近的修改才可以被检索。

不可变性
写入磁盘的倒排索引是不可变的，它有如下好处：

不需要锁。如果从来不需要更新一个索引，就不必担心多个程序同时尝试修改。
一旦索引被读入文件系统的缓存(译者:在内存)，它就一直在那儿，因为不会改变。只要文件系统缓存有足够的空间，大部分的读会直接访问内存而不是磁盘。这有助于性能提升。
在索引的声明周期内，所有的其他缓存都可用。它们不需要在每次数据变化了都重建，因为数据不会变。
写入单个大的倒排索引，可以压缩数据，较少磁盘IO和需要缓存索引的内存大小。
当然，不可变的索引有它的缺点，首先是它不可变！你不能改变它。如果想要搜索一个新文档，必须重见整个索引。这不仅严重限制了一个索引所能装下的数据，还有一个索引可以被更新的频次。

动态索引

下一个需要解决的问题是如何在保持不可变好处的同时更新倒排索引。答案是，使用多个索引。

不是重写整个倒排索引，而是增加额外的索引反映最近的变化。每个倒排索引都可以按顺序查询，从最老的开始，最后把结果聚合。

Elasticsearch底层依赖的Lucene，引入了per-segment search的概念。一个段(segment)是有完整功能的倒排索引，但是现在Lucene中的索引指的是段的集合，再加上提交点(commit point，包括所有段的文件)，如图1所示。新的文档，在被写入磁盘的段之前，首先写入内存区的索引缓存，如图2、图3所示。

图1：一个提交点和三个索引的Lucene

一个提交点和三个索引的Lucene

索引vs分片

为了避免混淆，需要说明，Lucene索引是Elasticsearch中的分片，Elasticsearch中的索引是分片的集合。当Elasticsearch搜索索引时，它发送查询请求给该索引下的所有分片，然后过滤这些结果，聚合成全局的结果。
一个per-segment search如下工作:

新的文档首先写入内存区的索引缓存。
不时，这些buffer被提交：
一个新的段——额外的倒排索引——写入磁盘。
新的提交点写入磁盘，包括新段的名称。
磁盘是fsync’ed(文件同步)——所有写操作等待文件系统缓存同步到磁盘，确保它们可以被物理写入。
新段被打开，它包含的文档可以被检索
内存的缓存被清除，等待接受新的文档。
图2：内存缓存区有即将提交文档的Lucene索引

内存缓存区有即将提交文档的Lucene索引

图3：提交后，新的段加到了提交点，缓存被清空

提交后，新的段加到了提交点，缓存被清空

当一个请求被接受，所有段依次查询。所有段上的Term统计信息被聚合，确保每个term和文档的相关性被正确计算。通过这种方式，新的文档以较小的代价加入索引。

删除和更新
段是不可变的，所以文档既不能从旧的段中移除，旧的段也不能更新以反映文档最新的版本。相反，每一个提交点包括一个.del文件，包含了段上已经被删除的文档。

当一个文档被删除，它实际上只是在.del文件中被标记为删除，依然可以匹配查询，但是最终返回之前会被从结果中删除。

文档的更新操作是类似的：当一个文档被更新，旧版本的文档被标记为删除，新版本的文档在新的段中索引。也许该文档的不同版本都会匹配一个查询，但是更老版本会从结果中删除。

在合并段这节，我们会展示删除的文件是如何从文件系统中清除的。



近实时搜索

因为per-segment search机制，索引和搜索一个文档之间是有延迟的。新的文档会在几分钟内可以搜索，但是这依然不够快。

磁盘是瓶颈。提交一个新的段到磁盘需要fsync操作，确保段被物理地写入磁盘，即时电源失效也不会丢失数据。但是fsync是昂贵的，它不能在每个文档被索引的时就触发。

所以需要一种更轻量级的方式使新的文档可以被搜索，这意味这移除fsync。

位于Elasticsearch和磁盘间的是文件系统缓存。如前所说，在内存索引缓存中的文档（图1）被写入新的段（图2），但是新的段首先写入文件系统缓存，这代价很低，之后会被同步到磁盘，这个代价很大。但是一旦一个文件被缓存，它也可以被打开和读取，就像其他文件一样。

图1：内存缓存区有新文档的Lucene索引 内存缓存区有新文档的Lucene索引

Lucene允许新段写入打开，好让它们包括的文档可搜索，而不用执行一次全量提交。这是比提交更轻量的过程，可以经常操作，而不会影响性能。

图2：缓存内容已经写到段中，但是还没提交 缓存内容已经写到段中，但是还没提交

refeash API
在Elesticsearch中，这种写入打开一个新段的轻量级过程，叫做refresh。默认情况下，每个分片每秒自动刷新一次。这就是为什么说Elasticsearch是近实时的搜索了：文档的改动不会立即被搜索，但是会在一秒内可见。

这会困扰新用户：他们索引了个文档，尝试搜索它，但是搜不到。解决办法就是执行一次手动刷新，通过API:

POST /_refresh <1>
POST /blogs/_refresh <2>
<1> refresh所有索引
<2> 只refresh 索引blogs
虽然刷新比提交更轻量，但是它依然有消耗。人工刷新在测试写的时有用，但是不要在生产环境中每写一次就执行刷新，这会影响性能。相反，你的应用需要意识到ES近实时搜索的本质，并且容忍它。
不是所有的用户都需要每秒刷新一次。也许你使用ES索引百万日志文件，你更想要优化索引的速度，而不是进实时搜索。你可以通过修改配置项refresh_interval减少刷新的频率：

PUT /my_logs
{
  "settings": {
    "refresh_interval": "30s" <1>
  }
}
<1> 每30s refresh一次my_logs
refresh_interval可以在存在的索引上动态更新。你在创建大索引的时候可以关闭自动刷新，在要使用索引的时候再打开它。

PUT /my_logs/_settings
{ "refresh_interval": -1 } <1>

PUT /my_logs/_settings
{ "refresh_interval": "1s" } <2>
<1> 禁用所有自动refresh
<2> 每秒自动refresh


持久化变更

没用fsync同步文件系统缓存到磁盘，我们不能确保电源失效，甚至正常退出应用后，数据的安全。为了ES的可靠性，需要确保变更持久化到磁盘。

我们说过一次全提交同步段到磁盘，写提交点，这会列出所有的已知的段。在重启，或重新打开索引时，ES使用这次提交点决定哪些段属于当前的分片。

当我们通过每秒的刷新获得近实时的搜索，我们依然需要定时地执行全提交确保能从失败中恢复。但是提交之间的文档怎么办？我们也不想丢失它们。

ES增加了事务日志（translog），来记录每次操作。有了事务日志，过程现在如下：

当一个文档被索引，它被加入到内存缓存，同时加到事务日志。

图1：新的文档加入到内存缓存，同时写入事务日志 新的文档加入到内存缓存，同时写入事务日志

refresh使得分片的进入如下图描述的状态。每秒分片都进行refeash：

内存缓冲区的文档写入到段中，但没有fsync。
段被打开，使得新的文档可以搜索。
缓存被清除

图2：经过一次refresh，缓存被清除，但事务日志没有 经过一次refresh，缓存被清除，但事务日志没有

随着更多的文档加入到缓存区，写入日志，这个过程会继续

图3：事务日志会记录增长的文档 事务日志会记录增长的文档

不时地，比如日志很大了，新的日志会创建，会进行一次全提交：
内存缓存区的所有文档会写入到新段中。
清除缓存
一个提交点写入硬盘
文件系统缓存通过fsync操作flush到硬盘
事务日志被清除
事务日志记录了没有flush到硬盘的所有操作。当故障重启后，ES会用最近一次提交点从硬盘恢复所有已知的段，并且从日志里恢复所有的操作。

事务日志还用来提供实时的CRUD操作。当你尝试用ID进行CRUD时，它在检索相关段内的文档前会首先检查日志最新的改动。这意味着ES可以实时地获取文档的最新版本。

图4：flush过后，段被全提交，事务日志清除 flush过后，段被全提交，事务日志清除

flush API
在ES中，进行一次提交并删除事务日志的操作叫做 flush。分片每30分钟，或事务日志过大会进行一次flush操作。

flush API可用来进行一次手动flush：

POST /blogs/_flush <1> 

POST /_flush?wait_for_ongoing  <2>
<1> flush索引blogs
<2> flush所有索引，等待操作结束再返回
你很少需要手动flush，通常自动的就够了。

当你要重启或关闭一个索引，flush该索引是很有用的。当ES尝试恢复或者重新打开一个索引时，它必须重放所有事务日志中的操作，所以日志越小，恢复速度越快。

合并段

通过每秒自动刷新创建新的段，用不了多久段的数量就爆炸了。有太多的段是一个问题。每个段消费文件句柄，内存，cpu资源。更重要的是，每次搜索请求都需要依次检查每个段。段越多，查询越慢。

ES通过后台合并段解决这个问题。小段被合并成大段，再合并成更大的段。

这是旧的文档从文件系统删除的时候。旧的段不会再复制到更大的新段中。

这个过程你不必做什么。当你在索引和搜索时ES会自动处理。这个过程如图：两个提交的段和一个未提交的段合并为了一个更大的段所示：

索引过程中，refresh会创建新的段，并打开它。
合并过程会在后台选择一些小的段合并成大的段，这个过程不会中断索引和搜索。

图1：两个提交的段和一个未提交的段合并为了一个更大的段 两个提交的段和一个未提交的段合并为了一个更大的段

下图描述了合并后的操作：

新的段flush到了硬盘。
新的提交点写入新的段，排除旧的段。
新的段打开供搜索。
旧的段被删除。

图2：段合并完后，旧的段被删除 段合并完后，旧的段被删除

合并大的段会消耗很多IO和CPU，如果不检查会影响到搜素性能。默认情况下，ES会限制合并过程，这样搜索就可以有足够的资源进行。

optimize API
optimize API最好描述为强制合并段API。它强制分片合并段以达到指定max_num_segments参数。这是为了减少段的数量（通常为1）达到提高搜索性能的目的。

警告

不要在动态的索引（正在活跃更新）上使用optimize API。后台的合并处理已经做的很好了，优化命令会阻碍它的工作。不要干涉！
在特定的环境下，optimize API是有用的。典型的场景是记录日志，这中情况下日志是按照每天，周，月存入索引。旧的索引一般是只可读的，它们是不可能修改的。 这种情况下，把每个索引的段降至1是有效的。搜索过程就会用到更少的资源，性能更好:

POST /logstash-2014-10/_optimize?max_num_segments=1 <1>
<1> 把索引中的每个分片都合并成一个段








结构化搜索

结构化搜索 是指查询包含内部结构的数据。日期，时间，和数字都是结构化的：它们有明确的格式给你执行逻辑操作。一般包括比较数字或日期的范围，或确定两个值哪个大。

文本也可以被结构化。一包蜡笔有不同的颜色：红色，绿色，蓝色。一篇博客可能被打上 分布式 和 搜索的标签。电子商务产品有商品统一代码（UPCs） 或其他有着严格格式的标识。

通过结构化搜索，你的查询结果_始终_是 是或非；是否应该属于集合。结构化搜索不关心文档的相关性或分数，它只是简单的包含或排除文档。

这必须是有意义的逻辑，一个数字不能比同一个范围中的其他数字 更多。它只能包含在一个范围中 —— 或不在其中。类似的，对于结构化文本，一个值必须相等或不等。这里没有 更匹配 的概念。





查找准确值

对于准确值，你需要使用过滤器。过滤器的重要性在于它们非常的快。它们不计算相关性（避过所有计分阶段）而且很容易被缓存。我们今后再来讨论过滤器的性能优势【过滤器缓存】，现在，请先记住尽可能多的使用过滤器。

用于数字的 term 过滤器

我们下面将介绍 term 过滤器，首先因为你可能经常会用到它，这个过滤器旨在处理数字，布尔值，日期，和文本。

我们来看一下例子，一些产品最初用数字来索引，包含两个字段 price 和 productID：

POST /my_store/products/_bulk
{ "index": { "_id": 1 }}
{ "price" : 10, "productID" : "XHDK-A-1293-#fJ3" }
{ "index": { "_id": 2 }}
{ "price" : 20, "productID" : "KDKE-B-9947-#kL5" }
{ "index": { "_id": 3 }}
{ "price" : 30, "productID" : "JODL-X-1937-#pV7" }
{ "index": { "_id": 4 }}
{ "price" : 30, "productID" : "QQPX-R-3956-#aD8" }
我们的目标是找出特定价格的产品。假如你有关系型数据库背景，可能用 SQL 来表现这次查询比较熟悉，它看起来像这样：

SELECT document
FROM   products
WHERE  price = 20
在 Elasticsearch DSL 中，我们使用 term 过滤器来实现同样的事。term 过滤器会查找我们设定的准确值。term 过滤器本身很简单，它接受一个字段名和我们希望查找的值：

{
    "term" : {
        "price" : 20
    }
}
term 过滤器本身并不能起作用。像在【查询 DSL】中介绍的一样，搜索 API 需要得到一个查询语句，而不是一个 过滤器。为了使用 term 过滤器，我们需要将它包含在一个过滤查询语句中：

GET /my_store/products/_search
{
    "query" : {
        "filtered" : { <1>
            "query" : {
                "match_all" : {} <2>
            },
            "filter" : {
                "term" : { <3>
                    "price" : 20
                }
            }
        }
    }
}
`filtered` 查询同时接受接受 `query` 与 `filter`。 `match_all` 用来匹配所有文档，这是默认行为，所以在以后的例子中我们将省略掉 `query` 部分。 这是我们上面见过的 `term` 过滤器。注意它在 `filter` 分句中的位置。 执行之后，你将得到预期的搜索结果：只能文档 2 被返回了（因为只有 `2` 的价格是 `20`）： ```json "hits" : [ { "_index" : "my_store", "_type" : "products", "_id" : "2", "_score" : 1.0, "_source" : { "price" : 20, "productID" : "KDKE-B-9947-#kL5" } } ] ``` 过滤器不会执行计分和计算相关性。分值由 `match_all` 查询产生，所有文档一视同仁，所有每个结果的分值都是 `1` #### 用于文本的 `term` 过滤器 像我们在开头提到的，`term` 过滤器可以像匹配数字一样轻松的匹配字符串。让我们通过特定 UPC 标识码来找出产品，而不是通过价格。如果用 SQL 来实现，我们可能会使用下面的查询： ```sql SELECT product FROM products WHERE productID = "XHDK-A-1293-#fJ3" ``` 转到查询 DSL，我们用 `term` 过滤器来构造一个类似的查询： ```json GET /my_store/products/_search { "query" : { "filtered" : { "filter" : { "term" : { "productID" : "XHDK-A-1293-#fJ3" } } } } } ``` 有点出乎意料：我们没有得到任何结果值！为什么呢？问题不在于 `term` 查询；而在于数据被索引的方式。如果我们使用 `analyze` API，我们可以看到 UPC 被分解成短小的表征： ```json GET /my_store/_analyze?field=productID XHDK-A-1293-#fJ3 ``` ```json { "tokens" : [ { "token" : "xhdk", "start_offset" : 0, "end_offset" : 4, "type" : "", "position" : 1 }, { "token" : "a", "start_offset" : 5, "end_offset" : 6, "type" : "", "position" : 2 }, { "token" : "1293", "start_offset" : 7, "end_offset" : 11, "type" : "", "position" : 3 }, { "token" : "fj3", "start_offset" : 13, "end_offset" : 16, "type" : "", "position" : 4 } ] } ``` 这里有一些要点： * 我们得到了四个分开的标记，而不是一个完整的标记来表示 UPC。 * 所有的字符都被转为了小写。 * 我们失去了连字符和 `#` 符号。 所以当我们用 `XHDK-A-1293-#fJ3` 来查找时，得不到任何结果，因为这个标记不在我们的倒排索引中。相反，那里有上面列出的四个标记。 显然，在处理唯一标识码，或其他枚举值时，这不是我们想要的结果。 为了避免这种情况发生，我们需要通过设置这个字段为 `not_analyzed` 来告诉 Elasticsearch 它包含一个准确值。我们曾在【自定义字段映射】中见过它。为了实现目标，我们要先删除旧索引（因为它包含了错误的映射），并创建一个正确映射的索引： ```json DELETE /my_store PUT /my_store { "mappings" : { "products" : { "properties" : { "productID" : { "type" : "string", "index" : "not_analyzed" } } } } } ``` 必须首先删除索引，因为我们不能修改已经存在的映射。 删除后，我们可以用自定义的映射来创建它。 这里我们明确表示不希望 `productID` 被分析。 现在我们可以继续重新索引文档： ```json POST /my_store/products/_bulk { "index": { "_id": 1 }} { "price" : 10, "productID" : "XHDK-A-1293-#fJ3" } { "index": { "_id": 2 }} { "price" : 20, "productID" : "KDKE-B-9947-#kL5" } { "index": { "_id": 3 }} { "price" : 30, "productID" : "JODL-X-1937-#pV7" } { "index": { "_id": 4 }} { "price" : 30, "productID" : "QQPX-R-3956-#aD8" } ``` 现在我们的 `term` 过滤器将按预期工作。让我们在新索引的数据上再试一次（注意，查询和过滤都没有修改，只是数据被重新映射了）。 ```json GET /my_store/products/_search { "query" : { "filtered" : { "filter" : { "term" : { "productID" : "XHDK-A-1293-#fJ3" } } } } } ``` `productID` 字段没有经过分析，`term` 过滤器也没有执行分析，所以这条查询找到了准确匹配的值，如期返回了文档 1。 #### 内部过滤操作 Elasticsearch 在内部会通过一些操作来执行一次过滤： 1. _查找匹配文档_。 `term` 过滤器在倒排索引中查找词 `XHDK-A-1293-#fJ3`，然后返回包含那个词的文档列表。在这个例子中，只有文档 1 有我们想要的词。 2. _创建字节集_ 然后过滤器将创建一个 _字节集_ —— 一个由 1 和 0 组成的数组 —— 描述哪些文档包含这个词。匹配的文档得到 `1` 字节，在我们的例子中，字节集将是 `[1,0,0,0]` 3. _缓存字节集_ 最后，字节集被储存在内存中，以使我们能用它来跳过步骤 1 和 2。这大大的提升了性能，让过滤变得非常的快。 当执行 `filtered` 查询时，`filter` 会比 `query` 早执行。结果字节集会被传给 `query` 来跳过已经被排除的文档。这种过滤器提升性能的方式，查询更少的文档意味着更快的速度。



组合过滤

前面的两个例子展示了单个过滤器的使用。现实中，你可能需要过滤多个值或字段，例如，想在 Elasticsearch 中表达这句 SQL 吗？

SELECT product
FROM   products
WHERE  (price = 20 OR productID = "XHDK-A-1293-#fJ3")
  AND  (price != 30)
这些情况下，你需要 bool 过滤器。这是以其他过滤器作为参数的组合过滤器，将它们结合成多种布尔组合。

布尔过滤器

bool 过滤器由三部分组成：

{
   "bool" : {
      "must" :     [],
      "should" :   [],
      "must_not" : [],
   }
}
must：所有分句都_必须_匹配，与 AND 相同。

must_not：所有分句都_必须不_匹配，与 NOT 相同。

should：至少有一个分句匹配，与 OR 相同。

这样就行了！假如你需要多个过滤器，将他们放入 bool 过滤器就行。

提示： bool 过滤器的每个部分都是可选的（例如，你可以只保留一个 must 分句），而且每个部分可以包含一到多个过滤器

为了复制上面的 SQL 示例，我们将两个 term 过滤器放在 bool 过滤器的 should 分句下，然后用另一个分句来处理 NOT 条件：

GET /my_store/products/_search
{
   "query" : {
      "filtered" : { <1>
         "filter" : {
            "bool" : {
              "should" : [
                 { "term" : {"price" : 20}}, <2>
                 { "term" : {"productID" : "XHDK-A-1293-#fJ3"}} <2>
              ],
              "must_not" : {
                 "term" : {"price" : 30} <3>
              }
           }
         }
      }
   }
}
注意我们仍然需要用 `filtered` 查询来包裹所有条件。 这两个 `term` 过滤器是 `bool` 过滤器的_子节点_，因为它们被放在 `should` 分句下，所以至少他们要有一个条件符合。 如果一个产品价值 `30`，它就会被自动排除掉，因为它匹配了 `must_not` 分句。 我们的搜索结果返回了两个结果，分别满足了 `bool` 过滤器中的不同分句： ```json "hits" : [ { "_id" : "1", "_score" : 1.0, "_source" : { "price" : 10, "productID" : "XHDK-A-1293-#fJ3" } }, { "_id" : "2", "_score" : 1.0, "_source" : { "price" : 20, "productID" : "KDKE-B-9947-#kL5" } } ] ``` 匹配 `term` 过滤器 `productID = "XHDK-A-1293-#fJ3"` 匹配 `term` 过滤器 `price = 20` #### 嵌套布尔过滤器 虽然 `bool` 是一个组合过滤器而且接受子过滤器，需明白它自己仍然只是一个过滤器。这意味着你可以在 `bool` 过滤器中嵌套 `bool` 过滤器，让你实现更复杂的布尔逻辑。 下面先给出 SQL 语句： ```sql SELECT document FROM products WHERE productID = "KDKE-B-9947-#kL5" OR ( productID = "JODL-X-1937-#pV7" AND price = 30 ) ``` 我们可以将它翻译成一对嵌套的 `bool` 过滤器： ```json GET /my_store/products/_search { "query" : { "filtered" : { "filter" : { "bool" : { "should" : [ { "term" : {"productID" : "KDKE-B-9947-#kL5"}}, { "bool" : { "must" : [ { "term" : {"productID" : "JODL-X-1937-#pV7"}}, { "term" : {"price" : 30}} ] }} ] } } } } } ``` 因为 `term` 和 `bool` 在第一个 `should` 分句中是平级的，至少需要匹配其中的一个过滤器。 `must` 分句中有两个平级的 `term` 分句，所以他们俩都需要匹配。 结果得到两个文档，分别匹配一个 `should` 分句： ```json "hits" : [ { "_id" : "2", "_score" : 1.0, "_source" : { "price" : 20, "productID" : "KDKE-B-9947-#kL5" } }, { "_id" : "3", "_score" : 1.0, "_source" : { "price" : 30, "productID" : "JODL-X-1937-#pV7" } } ] ``` `productID` 匹配第一个 `bool` 中的 `term` 过滤器。 这两个字段匹配嵌套的 `bool` 中的 `term` 过滤器。 这只是一个简单的例子，但是它展示了该怎样用布尔过滤器来构造复杂的逻辑条件。

查询多个准确值

term 过滤器在查询单个值时很好用，但是你可能经常需要搜索多个值。比如你想寻找 20 或 30 元的文档，该怎么做呢？

比起使用多个 term 过滤器，你可以用一个 terms 过滤器。terms 过滤器是 term 过滤器的复数版本。

它用起来和 term 差不多，我们现在来指定一组数值，而不是单一价格：

{
    "terms" : {
        "price" : [20, 30]
    }
}
像 term 过滤器一样，我们将它放在 filtered 查询中：

GET /my_store/products/_search
{
    "query" : {
        "filtered" : {
            "filter" : {
                "terms" : { <1>
                    "price" : [20, 30]
                }
            }
        }
    }
}
这是前面提到的 `terms` 过滤器，放置在 `filtered` 查询中 这条查询将返回第二，第三和第四个文档： ```json "hits" : [ { "_id" : "2", "_score" : 1.0, "_source" : { "price" : 20, "productID" : "KDKE-B-9947-#kL5" } }, { "_id" : "3", "_score" : 1.0, "_source" : { "price" : 30, "productID" : "JODL-X-1937-#pV7" } }, { "_id": "4", "_score": 1.0, "_source": { "price": 30, "productID": "QQPX-R-3956-#aD8" } } ] ```



包含，而不是相等

理解 term 和 terms 是_包含_操作，而不是_相等_操作，这点非常重要。这意味着什么？

假如你有一个 term 过滤器 { "term" : { "tags" : "search" } }，它将匹配下面两个文档：

{ "tags" : ["search"] }
{ "tags" : ["search", "open_source"] } <1>
虽然这个文档除了 `search` 还有其他短语，它还是被返回了 回顾一下 `term` 过滤器是怎么工作的：它检查倒排索引中所有具有短语的文档，然后组成一个字节集。在我们简单的示例中，我们有下面的倒排索引： | Token | DocIDs | | ------------ | ------- | |`open_source` | `2` | |`search` | `1`,`2` | 当执行 `term` 过滤器来查询 `search` 时，它直接在倒排索引中匹配值并找出相关的 ID。如你所见，文档 1 和文档 2 都包含 `search`，所以他们都作为结果集返回。 提示： 倒排索引的特性让完全匹配一个字段变得非常困难。你将如何确定一个文档_只能_包含你请求的短语？你将在索引中找出这个短语，解出所有相关文档 ID，然后扫描 _索引中每一行_来确定文档是否包含其他值。 由此可见，这将变得非常低效和开销巨大。因此，`term` 和 `terms` 是 _必须包含_ 操作，而不是 _必须相等_。 #### 完全匹配 假如你真的需要完全匹配这种行为，最好是通过添加另一个字段来实现。在这个字段中，你索引原字段包含值的个数。引用上面的两个文档，我们现在包含一个字段来记录标签的个数： ```json { "tags" : ["search"], "tag_count" : 1 } { "tags" : ["search", "open_source"], "tag_count" : 2 } ``` 一旦你索引了标签个数，你可以构造一个 `bool` 过滤器来限制短语个数： ```json GET /my_index/my_type/_search { "query": { "filtered" : { "filter" : { "bool" : { "must" : [ { "term" : { "tags" : "search" } }, { "term" : { "tag_count" : 1 } } ] } } } } } ``` 找出所有包含 `search` 短语的文档 但是确保文档只有一个标签 这将匹配只有一个 `search` 标签的文档，而不是匹配所有包含了 `search` 标签的文档。

范围

我们到现在只搜索过准确的数字，现实中，通过范围来过滤更为有用。例如，你可能希望找到所有价格高于 20 元而低于 40 元的产品。

在 SQL 语法中，范围可以如下表示：

SELECT document
FROM   products
WHERE  price BETWEEN 20 AND 40
Elasticsearch 有一个 range 过滤器，让你可以根据范围过滤：

"range" : {
    "price" : {
        "gt" : 20,
        "lt" : 40
    }
}
range 过滤器既能包含也能排除范围，通过下面的选项：

gt: > 大于
lt: < 小于
gte: >= 大于或等于
lte: <= 小于或等于
下面是范围过滤器的一个示例：

GET /my_store/products/_search
{
    "query" : {
        "filtered" : {
            "filter" : {
                "range" : {
                    "price" : {
                        "gte" : 20,
                        "lt"  : 40
                    }
                }
            }
        }
    }
}
假如你需要不设限的范围，去掉一边的限制就可以了：

"range" : {
    "price" : {
        "gt" : 20
    }
}
日期范围

range 过滤器也可以用于日期字段：

"range" : {
    "timestamp" : {
        "gt" : "2014-01-01 00:00:00",
        "lt" : "2014-01-07 00:00:00"
    }
}
当用于日期字段时，range 过滤器支持_日期数学_操作。例如，我们想找到所有最近一个小时的文档：

"range" : {
    "timestamp" : {
        "gt" : "now-1h"
    }
}
这个过滤器将始终能找出所有时间戳大于当前时间减 1 小时的文档，让这个过滤器像_移窗_一样通过你的文档。

日期计算也能用于实际的日期，而不是仅仅是一个像 now 一样的占位符。只要在日期后加上双竖线 ||，就能使用日期数学表达式了。

"range" : {
    "timestamp" : {
        "gt" : "2014-01-01 00:00:00",
        "lt" : "2014-01-01 00:00:00||+1M" <1>
    }
}
早于 2014 年 1 月 1 号加一个月 日期计算是与_日历相关_的，所以它知道每个月的天数，每年的天数，等等。更详细的关于日期的信息可以在这里找到 [日期格式手册](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html) #### 字符串范围 `range` 过滤器也可以用于字符串。字符串范围根据_字典_或字母顺序来计算。例如，这些值按照字典顺序排序： * 5, 50, 6, B, C, a, ab, abb, abc, b 提示：倒排索引中的短语按照字典顺序排序，也是为什么字符串范围使用这个顺序。 假如我们想让范围从 `a` 开始而不包含 `b`，我们可以用类似的 `range` 过滤器语法： ```json "range" : { "title" : { "gte" : "a", "lt" : "b" } } ``` 当心基数： 数字和日期字段的索引方式让他们在计算范围时十分高效。但对于字符串来说却不是这样。为了在字符串上执行范围操作，Elasticsearch 会在这个范围内的每个短语执行 `term` 操作。这比日期或数字的范围操作慢得多。 字符串范围适用于一个基数较小的字段，一个唯一短语个数较少的字段。你的唯一短语数越多，搜索就越慢。

处理 Null 值

回到我们早期的示例，在文档中有一个多值的字段 tags，一个文档可能包含一个或多个标签，或根本没有标签。如果一个字段没有值，它是怎么储存在倒排索引中的？

这是一个取巧的问题，因为答案是它根本没有存储。让我们从看一下前几节的倒排索引：

Token	DocIDs
open_source	2
search	1,2
你怎么可能储存一个在数据结构不存在的字段呢？倒排索引是标记和包含它们的文档的一个简单列表。假如一个字段不存在，它就没有任何标记，也就意味着它无法被倒排索引的数据结构表达出来。

本质上来说，null，[]（空数组）和 [null] 是相等的。它们都不存在于倒排索引中！

显然，这个世界却没有那么简单，数据经常会缺失字段，或包含空值或空数组。为了应对这些情形，Elasticsearch 有一些工具来处理空值或缺失的字段。

exists 过滤器

工具箱中的第一个利器是 exists 过滤器，这个过滤器将返回任何包含这个字段的文档，让我们用标签来举例，索引一些示例文档：

POST /my_index/posts/_bulk
{ "index": { "_id": "1"              }}
{ "tags" : ["search"]                }  <1>
{ "index": { "_id": "2"              }}
{ "tags" : ["search", "open_source"] }  <2>
{ "index": { "_id": "3"              }}
{ "other_field" : "some data"        }  <3>
{ "index": { "_id": "4"              }}
{ "tags" : null                      }  <4>
{ "index": { "_id": "5"              }}
{ "tags" : ["search", null]          }  <5>
`tags` 字段有一个值 `tags` 字段有两个值 `tags` 字段不存在 `tags` 字段被设为 `null` `tags` 字段有一个值和一个 `null` 结果我们 `tags` 字段的倒排索引看起来将是这样： | Token | DocIDs | |--------------|-------------| |`open_source` | `2` | |`search` | `1`,`2`,`5` | 我们的目标是找出所有设置了标签的文档，我们不关心这个标签是什么，只要它存在于文档中就行。在 SQL 语法中，我们可以用 `IS NOT NULL` 查询： ```sql SELECT tags FROM posts WHERE tags IS NOT NULL ``` 在 Elasticsearch 中，我们使用 `exists` 过滤器： ```json GET /my_index/posts/_search { "query" : { "filtered" : { "filter" : { "exists" : { "field" : "tags" } } } } } ``` 查询返回三个文档： ```json "hits" : [ { "_id" : "1", "_score" : 1.0, "_source" : { "tags" : ["search"] } }, { "_id" : "5", "_score" : 1.0, "_source" : { "tags" : ["search", null] } }, { "_id" : "2", "_score" : 1.0, "_source" : { "tags" : ["search", "open source"] } } ] ``` 文档 5 虽然包含了一个 `null` 值，仍被返回了。这个字段存在是因为一个有值的标签被索引了，所以 `null` 对这个过滤器没有影响 结果很容易理解，所以在 `tags` 字段中有值的文档都被返回了。只排除了文档 3 和 4。 #### `missing` 过滤器 `missing` 过滤器本质上是 `exists` 的反义词：它返回没有特定字段值的文档，像这条 SQL 一样： ```sql SELECT tags FROM posts WHERE tags IS NULL ``` 让我们在前面的例子中用 `missing` 过滤器来取代 `exists`： ```json GET /my_index/posts/_search { "query" : { "filtered" : { "filter": { "missing" : { "field" : "tags" } } } } } ``` 如你所愿，我们得到了两个没有包含标签字段的文档： ```json "hits" : [ { "_id" : "3", "_score" : 1.0, "_source" : { "other_field" : "some data" } }, { "_id" : "4", "_score" : 1.0, "_source" : { "tags" : null } } ] ``` 什么时候 null 才表示 null 有时你需要能区分一个字段是没有值，还是被设置为 `null`。用上面见到的默认行为无法区分这一点，数据都不存在了。幸运的是，我们可以将明确的 `null` 值用我们选择的_占位符_来代替 当指定字符串，数字，布尔值或日期字段的映射时，你可以设置一个 `null_value` 来处理明确的 `null` 值。没有值的字段仍将被排除在倒排索引外。 当选定一个合适的 `null_value` 时，确保以下几点： * 它与字段的类型匹配，你不能在 `date` 类型的字段中使用字符串 `null_value` * 它需要能与这个字段可能包含的正常值区分开来，以避免真实值和 `null` 值混淆 #### 对象的 `exists/missing` `exists` 和 `missing` 过滤器同样能在内联对象上工作，而不仅仅是核心类型。例如下面的文档： ```json { "name" : { "first" : "John", "last" : "Smith" } } ``` 你可以检查 `name.first` 和 `name.last` 的存在性，也可以检查 `name` 的。然而，在【映射】中，我们提到对象在内部被转成扁平化的键值结构，像下面所示： ```json { "name.first" : "John", "name.last" : "Smith" } ``` 所以我们是怎么使用 `exists` 或 `missing` 来检测 `name` 字段的呢，这个字段并没有真正存在于倒排索引中。 原因是像这样的一个过滤器 ```json { "exists" : { "field" : "name" } } ``` 实际是这样执行的 ```json { "bool": { "should": [ { "exists": { "field": { "name.first" }}}, { "exists": { "field": { "name.last" }}} ] } } ``` 同样这意味着假如 `first` 和 `last` 都为空，那么 `name` 就是不存在的。

关于缓存

在【内部过滤操作】章节中，我们简单提到过过滤器是怎么计算的。它们的核心是一个字节集来表示哪些文档符合这个过滤器。Elasticsearch 主动缓存了这些字节集留作以后使用。一旦缓存后，当遇到相同的过滤时，这些字节集就可以被重用，而不需要重新运算整个过滤。

缓存的字节集很“聪明”：他们会增量更新。你索引中添加了新的文档，只有这些新文档需要被添加到已存的字节集中，而不是一遍遍重新计算整个缓存的过滤器。过滤器和整个系统的其他部分一样是实时的，你不需要关心缓存的过期时间。

独立的过滤缓存

每个过滤器都被独立计算和缓存，而不管它们在哪里使用。如果两个不同的查询使用相同的过滤器，则会使用相同的字节集。同样，如果一个查询在多处使用同样的过滤器，只有一个字节集会被计算和重用。

让我们看一下示例，查找符合下列条件的邮箱：

在收件箱而且没有被读取过
_不在_收件箱但是被标记为重要
"bool": {
   "should": [
      { "bool": {
            "must": [
               { "term": { "folder": "inbox" }}, <1>
               { "term": { "read": false }}
            ]
      }},
      { "bool": {
            "must_not": {
               "term": { "folder": "inbox" } <1>
            },
            "must": {
               "term": { "important": true }
            }
      }}
   ]
}
这两个过滤器相同，而且会使用同一个字节集。 虽然一个收件箱条件是 `must` 而另一个是 `must_not`，这两个条件本身是相等的。这意味着字节集会在第一个条件执行时计算一次，然后作为缓存被另一个条件使用。而第二次执行这条查询时，收件箱的过滤已经被缓存了，所以两个条件都能使用缓存的字节集。 这与查询 DSL 的组合型紧密相关。移动过滤器或在相同查询中多处重用相同的过滤器非常简单。这不仅仅是方便了开发者 —— 对于性能也有很大的提升 #### 控制缓存 大部分直接处理字段的_枝叶过滤器_（例如 `term`）会被缓存，而像 `bool` 这类的组合过滤器则不会被缓存。 【提示】 枝叶过滤器需要在硬盘中检索倒排索引，所以缓存它们是有意义的。另一方面来说，组合过滤器使用快捷的字节逻辑来组合它们内部条件生成的字节集结果，所以每次重新计算它们也是很高效的。 然而，有部分枝叶过滤器，默认不会被缓存，因为它们这样做没有意义： 脚本过滤器： 脚本过滤器的结果不能被缓存因为脚本的意义对于 Elasticsearch 来说是不透明的。 Geo 过滤器： 定位过滤器（我们会在【geoloc】中更详细的介绍），通常被用于过滤基于特定用户地理位置的结果。因为每个用户都有一个唯一的定位，geo 过滤器看起来不太会重用，所以缓存它们没有意义。 日期范围： 使用 `now` 方法的日期范围（例如 `"now-1h"`），结果值精确到毫秒。每次这个过滤器执行时，`now` 返回一个新的值。老的过滤器将不再被使用，所以默认缓存是被禁用的。然而，当 `now` 被取整时（例如，`now/d` 取最近一天），缓存默认是被启用的。 有时候默认的缓存测试并不正确。可能你希望一个复杂的 `bool` 表达式可以在相同的查询中重复使用，或你想要禁用一个 `date` 字段的过滤器缓存。你可以通过 `_cache` 标记来覆盖几乎所有过滤器的默认缓存策略 ```json { "range" : { "timestamp" : { "gt" : "2014-01-02 16:15:14" }, "_cache": false } } ``` 看起来我们不会再使用这个精确时间戳 在这个过滤器上禁用缓存 以后的章节将提供一些例子来说明哪些时候覆盖默认缓存策略是有意义的。

过滤顺序

在 bool 条件中过滤器的顺序对性能有很大的影响。更详细的过滤条件应该被放置在其他过滤器之前，以便在更早的排除更多的文档。

假如条件 A 匹配 1000 万个文档，而 B 只匹配 100 个文档，那么需要将 B 放在 A 前面。

缓存的过滤器非常快，所以它们需要被放在不能缓存的过滤器之前。想象一下我们有一个索引包含了一个月的日志事件，然而，我们只对近一个小时的事件感兴趣：

GET /logs/2014-01/_search
{
    "query" : {
        "filtered" : {
            "filter" : {
                "range" : {
                    "timestamp" : {
                        "gt" : "now-1h"
                    }
                }
            }
        }
    }
}
这个过滤条件没有被缓存，因为它使用了 now 方法，这个值每毫秒都在变化。这意味着我们需要每次执行这条查询时都检测一整个月的日志事件。

我们可以通过组合一个缓存的过滤器来让这变得更有效率：我们可以添加一个含固定时间的过滤器来排除掉这个月的大部分数据，例如昨晚凌晨：

"bool": {
    "must": [
        { "range" : {
            "timestamp" : {
                "gt" : "now-1h/d" <1>
            }
        }},
        { "range" : {
            "timestamp" : {
                "gt" : "now-1h" <2>
            }
        }}
    ]
}
这个过滤器被缓存了，因为它使用了取整到昨夜凌晨 `now` 条件。 这个过滤器没有被缓存，因为它没有对 `now` 取整。 `now-1h/d` 条件取整到昨夜凌晨，所以所有今天之前的文档都被排除掉了。这个结果的字节集被缓存了，因为 `now` 被取整了，意味着它只需要每天当_昨夜凌晨_的值改变时被执行一次。`now-1h` 条件没有被缓存，因为 `now` 表示最近一毫秒的时间。然而，得益于第一个过滤器，第二个过滤器只需要检测当天的文档就行。 这些条件的排序很重要。上面的实现能正常工作是因为_自从昨晚凌晨_条件比_最近一小时_条件位置更前。假如它们用别的方式组合，那么_最近一小时_条件还是需要检测所有的文档，而不仅仅是昨夜以来的文档。












地理坐标点
地理坐标点（geo-point） 是指地球表面可以用经纬度描述的一个点。地理坐标点可以用来计算两个坐标位置间的距离，或者判断一个点是否在一个区域中。

地理坐标点不能被动态映射（dynamic mapping）自动检测，而是需要显式声明对应字段类型为 geo_point 。

PUT /attractions
{
  "mappings": {
    "restaurant": {
      "properties": {
        "name": {
          "type": "string"
        },
        "location": {
          "type": "geo_point"
        }
      }
    }
  }
}
经纬度坐标格式

如上例，location 被声明为 geo_point 后，我们就可以索引包含了经纬度信息的文档了。经纬度信息的形式可以是字符串，数组或者对象。

PUT /attractions/restaurant/1
{
  "name":     "Chipotle Mexican Grill",
  "location": "40.715, -74.011" <1>
}

PUT /attractions/restaurant/2
{
  "name":     "Pala Pizza",
  "location": { <2>
    "lat":     40.722,
    "lon":    -73.989
  }
}

PUT /attractions/restaurant/3
{
  "name":     "Mini Munchies Pizza",
  "location": [ -73.983, 40.719 ] <3>
}
以半角逗号分割的字符串形式 `"lat,lon"`；
明确以 `lat` 和 `lon` 作为属性的对象；
数组形式表示 `[lon,lat]`。
注意

可能所有人都至少踩过一次这个坑：地理坐标点用字符串形式表示时是经度在前，维度在后（"latitude,longitude"），而数组形式表示时刚好相反，是纬度在前，经度在后（[longitude,latitude]）。

其实，在 Elasticesearch 内部，不管字符串形式还是数组形式，都是经度在前，纬度在后。不过早期为了适配 GeoJSON 的格式规范，调整了数组形式的表示方式。

因此，在使用地理位置（geolocation）的路上就出现了这么一个“捕熊器”，专坑那些不了解这个陷阱的使用者。


通过地理坐标点过滤
有四种地理坐标点相关的过滤方式可以用来选中或者排除文档：

geo_bounding_box::

找出落在指定矩形框中的坐标点

geo_distance::

找出与指定位置在给定距离内的点

geo_distance_range::

找出与指定点距离在给定最小距离和最大距离之间的点

geo_polygon::

找出落在多边形中的点。这个过滤器使用代价很大。当你觉得自己需要使用它，最好先看看 geo-shapes

所有这些过滤器的工作方式都相似： 把 索引中所有文档（而不仅仅是查询中匹配到的部分文档，见 fielddata-intro）的经纬度信息都载入内存，然后每个过滤器执行一个轻量级的计算去判断当前点是否落在指定区域。

提示

地理坐标过滤器使用代价昂贵 —— 所以最好在文档集合尽可能少的场景使用。 你可以先使用那些简单快捷的过滤器，比如 term 或者 range，来过滤掉尽可能多的文档，最后才交给地理坐标过滤器处理。

布尔型过滤器（bool filter）会自动帮你做这件事。 它会优先让那些基于“bitset”的简单过滤器(见 filter-caching)来过滤掉尽可能多的文档，然后依次才是地理坐标过滤器或者脚本类的过滤器。


地理坐标盒模型过滤器
这是目前为止最有效的 地理坐标过滤器了，因为它计算起来非常简单。 你指定一个矩形的 顶部（top）, 底部（bottom）, 左边界（left）, 和 右边界（right）， 然后它只需判断坐标的经度是否在左右边界之间，纬度是否在上下边界之间。（译注：原文似乎写反了）


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geo_bounding_box": {
          "location": { <1>
            "top_left": {
              "lat":  40.8,
              "lon": -74.0
            },
            "bottom_right": {
              "lat":  40.7,
              "lon": -73.0
            }
          }
        }
      }
    }
  }
}
盒模型信息也可以用 `bottom_left`（左下方点）和 `top_right`（右上方点） 来表示。
优化盒模型

地理坐标盒模型过滤器不需要把所有坐标点都加载到内存里。 因为它要做的只是简单判断 纬度 和 经度 坐标数值是否在给定的范围内，所以它可以用倒排索引来做一个范围（range）过滤。

要使用这种优化方式，需要把 geo_point 字段用 纬度（lat）和经度（lon）方式表示并分别索引。

PUT /attractions
{
  "mappings": {
    "restaurant": {
      "properties": {
        "name": {
          "type": "string"
        },
        "location": {
          "type":    "geo_point",
          "lat_lon": true <1>
        }
      }
    }
  }
}
`location.lat` 和 `location.lon` 字段将被分别索引。它们可以被用于检索，但是不会在检索结果中返回。
然后，查询时你需要告诉 Elasticesearch 使用已索引的 lat和lon。


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geo_bounding_box": {
          "type":    "indexed", <1>
          "location": {
            "top_left": {
              "lat":  40.8,
              "lon": -74.0
            },
            "bottom_right": {
              "lat":  40.7,
              "lon":  -73.0
            }
          }
        }
      }
    }
  }
}
设置 `type` 参数为 `indexed` (默认为 `memory`) 来明确告诉 Elasticsearch 对这个过滤器使用倒排索引。
注意：

geo_point 类型可以包含多个地理坐标点，但是针对经度纬度分别索引的这种优化方式（lat_lon）只对单个坐标点的方式有效。



地理距离过滤器
地理距离过滤器（geo_distance）以给定位置为圆心画一个圆，来找出那些位置落在其中的文档：


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geo_distance": {
          "distance": "1km", <1>
          "location": { <2>
            "lat":  40.715,
            "lon": -73.988
          }
        }
      }
    }
  }
}
找出所有与指定点距离在1公里（`1km`）内的 `location` 字段。访问 [Distance Units](http://bit.ly/1ynS64j) 查看所支持的距离表示单位

中心点可以表示为字符串，数组或者（如示例中的）对象。详见 [lat-lon-formats](/lat-lon-formats)。
地理距离过滤器计算代价昂贵。 为了优化性能，Elasticsearch 先画一个矩形框（边长为2倍距离）来围住整个圆形， 这样就可以用消耗较少的盒模型计算方式来排除掉那些不在盒子内（自然也不在圆形内）的文档， 然后只对落在盒模型内的这部分点用地理坐标计算方式处理。

提示

你需要判断你的使用场景，是否需要如此精确的使用圆模型来做距离过滤？ 通常使用矩形模型是更高效的方式，并且往往也能满足应用需求。
更快的地理距离计算

两点间的距离计算，有多种性能换精度的算法：

arc::

最慢但是最精确是弧形（arc）计算方式，这种方式把世界当作是球体来处理。 不过这种方式精度还是有限，因为这个世界并不是完全的球体。

plane::

平面（plane）计算方式，((("plane distance calculation")))把地球当成是平坦的。 这种方式快一些但是精度略逊；在赤道附近位置精度最好，而靠近两极则变差。

sloppy_arc::

如此命名，是因为它使用了 Lucene 的 SloppyMath 类。 这是一种用精度换取速度的计算方式，它使用 Haversine formula 来计算距离； 它比弧形（arc）计算方式快4~5倍, 并且距离精度达99.9%。这也是默认的计算方式。

你可以参考下例来指定不同的计算方式：


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geo_distance": {
          "distance":      "1km",
          "distance_type": "plane", <1>
          "location": {
            "lat":  40.715,
            "lon": -73.988
          }
        }
      }
    }
  }
}
使用更快但精度稍差的`平面`（`plane`）计算方式。
提示： 你的用户真的会在意一个宾馆落在指定圆形区域数米之外了吗？ 一些地理位置相关的应用会有较高的精度要求；但大部分实际应用场景中，使用精度较低但响应更快的计算方式可能就挺好。
地理距离区间过滤器

地理距离过滤器（geo_distance）和地理距离区间过滤器（geo_distance_range）的唯一差别在于后者是一个环状的，它会排除掉落在内圈中的那部分文档。

指定到中心点的距离也可以换一种表示方式： 指定一个最小距离（使用 gt或者gte）和最大距离（使用lt或者lte），就像使用区间（range）过滤器一样。


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geo_distance_range": {
          "gte":    "1km", <1>
          "lt":     "2km", <1>
          "location": {
            "lat":  40.715,
            "lon": -73.988
          }
        }
      }
    }
  }
}
匹配那些距离中心点超过`1公里`而小于`2公里`的位置。


缓存地理位置过滤器
因为如下两个原因，地理位置过滤器默认是不被缓存的：

地理位置过滤器通常是用于查找用户当前位置附近的东西。但是用户是在移动的，并且没有两个用户的位置完全相同，因此缓存的过滤器基本不会被重复使用到。

过滤器是被缓存为比特位集合来表示段（segment）内的文档。假如我们的查询排除了几乎所有文档，只剩一个保存在这个特别的段内。一个未缓存的地理位置过滤器只需要检查这一个文档就行了，但是一个缓存的地理位置过滤器则需要检查所有在段内的文档。
缓存对于地理位置过滤器也可以很有效。 假设你的索引里包含了所有美国的宾馆。一个在纽约的用户是不会对旧金山的宾馆感兴趣的。 所以我们可以认为纽约是一个热点（hot spot），然后画一个边框把它和附近的区域围起来。

如果这个地理盒模型过滤器（geo_bounding_box）被缓存起来，那么当有位于纽约市的用户访问时它就可以被重复使用了。 它可以直接排除国内其它区域的宾馆。然后我们使用未缓存的，更加明确的地理盒模型过滤器（geo_bounding_box）或者地理距离过滤器（geo_distance）来在剩下的结果集中把范围进一步缩小到用户附近：


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "geo_bounding_box": {
                "type": "indexed",
                "_cache": true, <1>
                "location": {
                  "top_left": {
                    "lat":  40,8,
                    "lon": -74.1
                  },
                  "bottom_right": {
                    "lat":  40.4,
                    "lon": -73.7
                  }
                }
              }
            },
            {
              "geo_distance": { <2>
                "distance": "1km",
                "location": {
                  "lat":  40.715,
                  "lon": -73.988
                }
              }
            }
          ]
        }
      }
    }
  }
}
缓存的`地理盒模型过滤器`把结果集缩小到了纽约市。
代价更高的`地理距离过滤器`（`geo_distance`）让结果集缩小到1km内的用户。


减少内存占用
每一个 经纬度（lat/lon）组合需要占用16个字节的内存。要知道内存可是供不应求的。 使用这种占用16字节内存的方式可以得到非常精确的结果。不过就像之前提到的一样，实际应用中几乎都不需要这么精确。

你可以通过这种方式来减少内存使用量： 设置一个压缩的（compressed）数据字段格式并明确指定你的地理坐标点所需的精度。 即使只是将精度降低到1毫米（1mm）级别，也可以减少1/3的内存使用。 更实际的，将精度设置到3米（3m）内存占用可以减少62%，而设置到1公里（1km）则节省75%之多。

这个设置项可以通过 update-mapping API 来对实时索引进行调整：

POST /attractions/_mapping/restaurant
{
  "location": {
    "type": "geo_point",
    "fielddata": {
      "format":    "compressed",
      "precision": "1km" <1>
    }
  }
}
每一个`经纬度`（`lat/lon`）组合现在只需要4个字节，而不是16个。
另外，你还可以这样做来避免把所有地理坐标点全部同时加载到内存中： 使用在优化盒模型（optimize-bounding-box）中提到的技术， 或者把地理坐标点当作文档值（doc values）来存储。


PUT /attractions
{
  "mappings": {
    "restaurant": {
      "properties": {
        "name": {
          "type": "string"
        },
        "location": {
          "type":       "geo_point",
          "doc_values": true <1>
        }
      }
    }
  }
}
地理坐标点现在不会被加载到内存，而是保存在磁盘中。
将地理坐标点映射为文档值的方式只能是在这个字段第一次被创建时。 相比使用字段值，使用文档值会有一些小的性能代价，不过考虑到它对内存的节省，这种方式通常是还值得的。


按距离排序
检索结果可以按跟指定点的距离排序：

提示 当你可以（can）按距离排序时，按距离打分（scoring-by-distance）通常是一个更好的解决方案。
GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geo_bounding_box": {
          "type":       "indexed",
          "location": {
            "top_left": {
              "lat":  40,8,
              "lon": -74.0
            },
            "bottom_right": {
              "lat":  40.4,
              "lon": -73.0
            }
          }
        }
      }
    }
  },
  "sort": [
    {
      "_geo_distance": {
        "location": { <1>
          "lat":  40.715,
          "lon": -73.998
        },
        "order":         "asc",
        "unit":          "km", <2>
        "distance_type": "plane" <3>
      }
    }
  ]
}
计算每个文档中 `location` 字段与指定的 `lat/lon` 点间的距离。
以 `公里`（`km`）为单位，将距离设置到每个返回结果的 `sort` 键中。
使用快速但精度略差的`平面`（`plane`）计算方式。
你可能想问：为什么要制定距离的单位（unit）呢？ 用于排序的话，我们并不关心比较距离的尺度是英里，公里还是光年。 原因是，这个用于排序的值会设置在每个返回结果的 sort 元素中。


...
  "hits": [
     {
        "_index": "attractions",
        "_type": "restaurant",
        "_id": "2",
        "_score": null,
        "_source": {
           "name": "New Malaysia",
           "location": {
              "lat": 40.715,
              "lon": -73.997
           }
        },
        "sort": [
           0.08425653647614346 <1>
        ]
     },
...
宾馆距离我们的指定位置距离是 0.084km。
你可以通过设置单位（unit）来让返回值的形式跟你应用中想要的匹配。
提示

地理距离排序可以对多个坐标点来使用，不管（这些坐标点）是在文档中还是排序参数中。 使用 sort_mode 来指定是否需要使用位置集合的 最小（min），最大（max）或者平均（avg）距离。 这样就可以返回离我的工作地和家最近的朋友这样的结果了。
按距离打分

有可能距离只是决定返回结果排序的唯一重要因素，不过更常见的情况是距离会和其它因素， 比如全文检索匹配度，流行程度或者价格一起决定排序结果。

遇到这种场景你需要在查询分值计算（function_score query）中指定方式让我们把这些因子处理得到一个综合分。 decay-functions中有个一个例子就是地理距离影响排序得分的。

另外按距离排序还有个缺点就是性能：需要对每一个匹配到的文档都进行距离计算。 而 function_score请求，在 rescore phase阶段有可能只需要对前 n 个结果进行计算处理。











Geohashes
Geohashes 是一种将 经纬度坐标对（lat/lon）编码成字符串的方式。 最开始这么做只是为了让地理位置在url上呈现的形式更加友好， 不过现在geohash已经变成一种在数据库中有效索引地理坐标点和地理形状的方式。

Geohashes 把整个世界分为32个单元的格子--4行8列--每一个格子都用一个字母或者数字标识。 比如 g 这个单元覆盖了半个格林兰，冰岛的全部和大不列颠的大部分。 每一个单元还可以进一步被分解成新的32个单元，这些单元又可以继续被分解成32个更小的单元，不断重复下去。 gc 这个单元覆盖了爱尔兰和英格兰，gcp覆盖了伦敦的大部分和部分南英格兰， gcpuuz94k是伯明翰宫的入口，精确到了约5米。

换句话说，geohash的长度越长，它的精度就越高。 如果两个geohash有一个共同的前缀，如 gcpuuz，就表示他们挨得很紧。 共同的前缀越长，距离就越近。

但那也就是说，两个刚好相邻的位置，会可能有完全不同的geohash。 一个实例，伦敦的 Millenium Dome 的geohash是 u10hbp， 因为它落在了 u这个大单元里，而紧挨着它东边的最大的单元是 g。

地理坐标点可以自动关联到他们对应的 geohash。 需要注意的是，他们会被索引到了所有（各个层级）的 geohash 前缀（prefixes）。 例：索引伯明翰宫的门口--坐标纬度 51.501568，经度-0.141257--会在各种尺寸精度的 geohash 上建立索引， 如下表：

Geohash	Level	Dimensions
g	1	~ 5,004km x 5,004km
gc	2	~ 1,251km x 625km
gcp	3	~ 156km x 156km
gcpu	4	~ 39km x 19.5km
gcpuu	5	~ 4.9km x 4.9km
gcpuuz	6	~ 1.2km x 0.61km
gcpuuz9	7	~ 152.8m x 152.8m
gcpuuz94	8	~ 38.2m x 19.1m
gcpuuz94k	9	~ 4.78m x 4.78m
gcpuuz94kk	10	~ 1.19m x 0.60m
gcpuuz94kkp	11	~ 14.9cm x 14.9cm
gcpuuz94kkp5	12	~ 3.7cm x 1.8cm
geohash单元过滤器（ geohash_cell filter ）可以使用这些geohash前缀来找出与指定坐标点（lat/lon）相邻的位置。


Geohashes
Geohashes 是一种将 经纬度坐标对（lat/lon）编码成字符串的方式。 最开始这么做只是为了让地理位置在url上呈现的形式更加友好， 不过现在geohash已经变成一种在数据库中有效索引地理坐标点和地理形状的方式。

Geohashes 把整个世界分为32个单元的格子--4行8列--每一个格子都用一个字母或者数字标识。 比如 g 这个单元覆盖了半个格林兰，冰岛的全部和大不列颠的大部分。 每一个单元还可以进一步被分解成新的32个单元，这些单元又可以继续被分解成32个更小的单元，不断重复下去。 gc 这个单元覆盖了爱尔兰和英格兰，gcp覆盖了伦敦的大部分和部分南英格兰， gcpuuz94k是伯明翰宫的入口，精确到了约5米。

换句话说，geohash的长度越长，它的精度就越高。 如果两个geohash有一个共同的前缀，如 gcpuuz，就表示他们挨得很紧。 共同的前缀越长，距离就越近。

但那也就是说，两个刚好相邻的位置，会可能有完全不同的geohash。 一个实例，伦敦的 Millenium Dome 的geohash是 u10hbp， 因为它落在了 u这个大单元里，而紧挨着它东边的最大的单元是 g。

地理坐标点可以自动关联到他们对应的 geohash。 需要注意的是，他们会被索引到了所有（各个层级）的 geohash 前缀（prefixes）。 例：索引伯明翰宫的门口--坐标纬度 51.501568，经度-0.141257--会在各种尺寸精度的 geohash 上建立索引， 如下表：

Geohash	Level	Dimensions
g	1	~ 5,004km x 5,004km
gc	2	~ 1,251km x 625km
gcp	3	~ 156km x 156km
gcpu	4	~ 39km x 19.5km
gcpuu	5	~ 4.9km x 4.9km
gcpuuz	6	~ 1.2km x 0.61km
gcpuuz9	7	~ 152.8m x 152.8m
gcpuuz94	8	~ 38.2m x 19.1m
gcpuuz94k	9	~ 4.78m x 4.78m
gcpuuz94kk	10	~ 1.19m x 0.60m
gcpuuz94kkp	11	~ 14.9cm x 14.9cm
gcpuuz94kkp5	12	~ 3.7cm x 1.8cm
geohash单元过滤器（ geohash_cell filter ）可以使用这些geohash前缀来找出与指定坐标点（lat/lon）相邻的位置。


geohash单元过滤器
geohash单元过滤器做的事情非常简单： 把经纬度坐标位置根据指定精度转换成一个geohash，然后查找落在同一个geohash中的位置--这实在是非常高效的过滤器。


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geohash_cell": {
          "location": {
            "lat":  40.718,
            "lon": -73.983
          },
          "precision": "2km" <1>
        }
      }
    }
  }
}
`precision` 字段设置的精度不能高于geohash精度映射时的设定。
这个过滤器将坐标点转换成对应长度的geohash--本例中为dr5rsk--然后查找位于同一个组中的所有位置。

然而，如上例中的写法可能不会返回5km内所有的宾馆。 要知道每个 geohash 实际上仅是一个矩形，而指定的点可能位于这个矩形中的任何位置。 有可能这个点刚好落在了geohash单元的边缘附近，但过滤器会排除那些（挨得很近却）落在相邻单元里的宾馆。

为了修正这点，我们可以告诉过滤器，把周围的单元也包含进来。 通过设置neighbors 参数为 true：


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geohash_cell": {
          "location": {
            "lat":  40.718,
            "lon": -73.983
          },
          "neighbors": true, <1>
          "precision": "2km"
        }
      }
    }
  }
}
过滤器将会查找对应的geohash和包围它的（8个）geohash。
明显的，2km精度的geohash再加上周围的单元，会导致结果实际在一个更大的检索范围。 这个过滤器不是为精度而生的，但是它非常有效率，可以用于更高精度的地理位置过滤器的前置过滤器。

提示

将 precision 参数设置为一个距离可能会有误导性。 比如将 precision 设置为 2km 将会转换成长度为6的geohash。但实际上它的尺寸是约 1.2km * 0.6 km。 你可能会发现这还不如自己明确的设置一个长度 5 或者 6 来得更容易理解。
这个过滤器有一个比地理盒模型过滤器（geo_bounding_box）更好的优点，就是它支持一个字段中有多个坐标位置的情况。 我们在设置优化盒模型过滤器（ optimize-bounding-box ）讲过，设置 lat_lon 选项也是一个很有效的方式， 但是它只对字段中的单个坐标点情况有效。









地理位置聚合
虽然地理位置过滤或评分功能很有用，不过更有用得是将信息再地图上呈现给用户。 检索的结果集可能很多而不能将每个点都一一呈现，这时候就可以使用地理位置聚合来把这些位置点分布到更加可控的桶（buckets）里。

有三种聚合器可以作用于 geo_point 类型的字段：

geo_distance

将文档按以指定中心点为圆心的圆环分组

geohash_grid

将文档按 geohash 单元分组，以便在地图上呈现

geo_bounds

返回包含一系列矩形框的经纬坐标对，这些矩形框包含了所有的坐标点。 这种方式对于要在地图上选择一个合适的缩放等级（zoom level）时很实用。


按距离聚合
按距离聚合对于类似“找出距我1公里内的所有pizza店”这样的检索场景很适合。 检索结果需要确实地只返回距离用户1km内的文档，不过我们可以再加上一个“1-2km内的结果集”：


GET /attractions/restaurant/_search
{
  "query": {
    "filtered": {
      "query": {
        "match": { <1>
          "name": "pizza"
        }
      },
      "filter": {
        "geo_bounding_box": {
          "location": { <2>
            "top_left": {
              "lat":  40,8,
              "lon": -74.1
            },
            "bottom_right": {
              "lat":  40.4,
              "lon": -73.7
            }
          }
        }
      }
    }
  },
  "aggs": {
    "per_ring": {
      "geo_distance": { <3>
        "field":    "location",
        "unit":     "km",
        "origin": {
          "lat":    40.712,
          "lon":   -73.988
        },
        "ranges": [
          { "from": 0, "to": 1 },
          { "from": 1, "to": 2 }
        ]
      }
    }
  },
  "post_filter": { <4>
    "geo_distance": {
      "distance":   "1km",
      "location": {
        "lat":      40.712,
        "lon":     -73.988
      }
    }
  }
}
主查询查找饭店名中包含了 “pizza” 的文档。
矩形框过滤器让结果集缩小到纽约区域。
距离聚合器计算距用户1km和1km-2km的结果数。
最后，后置过滤器（`post_filter`)再把结果缩小到距离用户1km的饭店。
上例请求的返回结果如下：


"hits": {
  "total":     1,
  "max_score": 0.15342641,
  "hits": [ <1>
     {
        "_index": "attractions",
        "_type":  "restaurant",
        "_id":    "3",
        "_score": 0.15342641,
        "_source": {
           "name": "Mini Munchies Pizza",
           "location": [
              -73.983,
              40.719
           ]
        }
     }
  ]
},
"aggregations": {
  "per_ring": { <2>
     "buckets": [
        {
           "key":       "*-1.0",
           "from":      0,
           "to":        1,
           "doc_count": 1
        },
        {
           "key":       "1.0-2.0",
           "from":      1,
           "to":        2,
           "doc_count": 1
        }
     ]
  }
}
后置过滤器（`post_filter`）已经结果集缩小到满足“距离用户1km”条件下的唯一一个pizza店。
聚合器包含了"距离用户2km"的pizza店的检索结果。
这个例子中，我们统计了落到各个环形区域中的饭店数。 当然，我们也可以使用子聚合器再在每个环形区域中进一步计算它们的平均价格，最流行，等等。

geohash单元聚合器
一个查询返回的结果集中可能包含很多的点，以至于不能在地图上全部单独显示。 geohash单元聚合器可以按照你指定的精度计算每个点的geohash并将相邻的点聚合到一起。

返回结果是一个个单元格，每个单元格对应一个可以在地图上展示的 geohash。 通过改变 geohash 的精度，你可以统计全球、某个国家，或者一个城市级别的综述信息。

聚合结果是稀疏（sparse）的，因为它只返回包含了文档集合的单元。 如果你的geohash精度太细，导致生成了太多的结果集，它默认只会返回包含结果最多的10000个单元 -- 它们包含了大部分文档集合。 然后，为了找出这排在前10000的单元，它还是需要先生成所有的结果集。 你可以通过如下方式控制生成的单元的数目：

使用一个矩形过滤器来限制结果集。
对应该矩形，选择一个合适的精度。

GET /attractions/restaurant/_search?search_type=count
{
  "query": {
    "filtered": {
      "filter": {
        "geo_bounding_box": {
          "location": { <1>
            "top_left": {
              "lat":  40,8,
              "lon": -74.1
            },
            "bottom_right": {
              "lat":  40.4,
              "lon": -73.7
            }
          }
        }
      }
    }
  },
  "aggs": {
    "new_york": {
      "geohash_grid": { <2>
        "field":     "location",
        "precision": 5
      }
    }
  }
}
矩形框将检索限制在纽约区域。
使用精度为 `5` 的geohash，精度大约是 5km x 5km.
每个精度为 5 的 geohash 覆盖约 25平方公里，那 10000 个单元就能覆盖 25万平方公里。 我们指定的矩形框覆盖面积约 44km * 33km，也就是大概 1452平方公里。 所以这肯定在一个安全的限度内，我们不会因此浪费大量内存来生成太多单元。

上例请求的返回如下：


...
"aggregations": {
  "new_york": {
     "buckets": [ <1>
        {
           "key": "dr5rs",
           "doc_count": 2
        },
        {
           "key": "dr5re",
           "doc_count": 1
        }
     ]
  }
}
...
每个单元以一个 geohash 作为 `key`。
Again, we didn't specify any subaggregations, so all we got back was the document count. We could have asked for popular restaurant types, average price, or other details. 同样的，我们没有指定子聚合器，所以我们的返回结果是文档数目。 我们也可以（指定子聚合器来）得到流行的饭店类型，平均价格，或者其它详细信息。

提示

为了将这些单元放置在地图上展示，我们需要一个类库来将geohash解析为对于的矩形框或者中心点。 Javascript和一些语言中有现成的类库，不过你也可以根据 geo-bounds-agg 的信息自己来实现。


范围（边界）聚合器
在geohash聚合器的例子中，我们使用了一个矩形框过滤器来将结果限制在纽约区域。 然而，我们的结果都分布在曼哈顿。 当在地图上呈现给用户时，合理的方式是可以缩放到有数据的区域；地图上有大量空白区域是没有任何点分布的。

范围过滤器是这么做得： 它计算出一个个小矩形框来覆盖到所有的坐标点。


GET /attractions/restaurant/_search?search_type=count
{
  "query": {
    "filtered": {
      "filter": {
        "geo_bounding_box": {
          "location": {
            "top_left": {
              "lat":  40,8,
              "lon": -74.1
            },
            "bottom_right": {
              "lat":  40.4,
              "lon": -73.9
            }
          }
        }
      }
    }
  },
  "aggs": {
    "new_york": {
      "geohash_grid": {
        "field":     "location",
        "precision": 5
      }
    },
    "map_zoom": { <1>
      "geo_bounds": {
        "field":     "location"
      }
    }
  }
}
范围聚合器会计算出一个最小的矩形框来覆盖查询结果的所有文档。
返回结果包含了一个可以用来在地图上缩放的矩形框：


...
"aggregations": {
  "map_zoom": {
     "bounds": {
        "top_left": {
           "lat":  40.722,
           "lon": -74.011
        },
        "bottom_right": {
           "lat":  40.715,
           "lon": -73.983
        }
     }
  },
...
实际上，我们可以把矩形聚合器放到每一个 geohash 单元里，因为有坐标点的单元只占了所有单元的一部分：


GET /attractions/restaurant/_search?search_type=count
{
  "query": {
    "filtered": {
      "filter": {
        "geo_bounding_box": {
          "location": {
            "top_left": {
              "lat":  40,8,
              "lon": -74.1
            },
            "bottom_right": {
              "lat":  40.4,
              "lon": -73.9
            }
          }
        }
      }
    }
  },
  "aggs": {
    "new_york": {
      "geohash_grid": {
        "field":     "location",
        "precision": 5
      },
      "aggs": {
        "cell": { <1>
          "geo_bounds": {
            "field": "location"
          }
        }
      }
    }
  }
}
子聚合器 `cell_bounds` 会作用于每个 geohash 单元。
现在落在每个geohash单元中的点都有了一个所在的矩形框区域：


...
"aggregations": {
  "new_york": {
     "buckets": [
        {
           "key": "dr5rs",
           "doc_count": 2,
           "cell": {
              "bounds": {
                 "top_left": {
                    "lat":  40.722,
                    "lon": -73.989
                 },
                 "bottom_right": {
                    "lat":  40.719,
                    "lon": -73.983
                 }
              }
           }
        },
...










地理形状
地理形状（geo-shapes）使用一种与地理坐标点完全不同的方法。 我们在计算机屏幕上看到的圆形并不是由完美的连续的线组成的；而是用一个个连续的像素点来画出的一个近似圆。 地理形状的工作方式就与此相似。

复杂的形状 -- 比如 点集，线，多边形，多多变形，中空多边形等 -- 都是通过一个个 geohash单元来画出的。 这些形状会转化为一个被它所覆盖到的 geohash 集合。

注意

实际上，有两种类型的格子模式能用于地理星座： 默认是使用我们之前讨论过的 geohash；另外还有一种是 象限4叉树（quad trees）。 象限4叉树和geohash类似，只不过它每个层级都是4个单元（而不是像geohash一样的32个）。 这种不同取决于编码方式的选择。
组成一个形状的 geohash 都作为一个组索引在一起。 有这些信息，通过查看是否有相同的geohash 单元，就可以很轻易地检查两个形状是否有交集。

地理形状有这些用处：判断查询的形状与索引的形状的关系；这些关系可能是以下之一：

intersects::

查询的形状与索引形状有重叠（默认）。

disjoint::

查询的形状与索引的形状完全不重叠。

within::

索引的形状完全被包含在查询形状中。

注意

地理形状不能用语计算距离、排序、打分以及聚集。

映射地理形状
与 geo_point类型的字段相似，地理形状也需要在使用前明确映射：


PUT /attractions
{
  "mappings": {
    "landmark": {
      "properties": {
        "name": {
          "type": "string"
        },
        "location": {
          "type": "geo_shape"
        }
      }
    }
  }
}
你需要关注两个重要的设置项来调整精度（precision）和距离误差（distance_error_pct）。 There are two important settings that you should consider changing precision and distance_error_pct.

精度

精度（precision）参数用来控制组成地理形状的geohash的长度。 它的默认值是 9，等同于尺寸在 5m*5m 的geohash。这个精度可能比你需要的精确得多。

精度越低，需要索引的单元就越少，检索时也会更快。当然，精度越低，地理形状的准确性就越差。 你需要决定自己的地理形状所需精度 —— 即使减少1-2个等级的精度也能带来明显的消耗缩减收益。

你可以通过指定距离的方式来定制精度 —— 比如，50m或2km； 不过这些距离最终也是会转换成对应的geohash长度（见 geohashes）。

距离误差

当索引一个多边形时，中间连续区域很容易用一个短geohash来表示。 麻烦的是边缘部分，这些地方需要使用更精细的geohash才能表示。

当你在索引一个小地标时，你希望它的边界比较精确；（如果精度不够，）让这些纪念碑一个叠着一个可不好。 当索引整个国家时，你就不需要这么高的精度。误差个50米左右也没什么大不了。

距离误差（distance_error_pct）指定地理形状可以接受的最大错误率。它的默认值是 0.025，即 2.5%。 这也就是说，大的地理形状（比如国家）相比小的地理形状（比如纪念碑）来说，容许更加模糊的边界。

0.025是一个不错的初始值。不过如果我们容许更大的错误率，对应地理形状需要索引的单元就越少。

索引地理形状
地理形状通过GeoJSON来表示，这是一种开放的使用JSON实现的二维形状编码方式。 每个形状包含两个信息：形状类型：point, line, polygon, envelope；一个或多经纬度点集合的数组。

注意：

在 GeoJSON 里，经纬度表示方式通常是“纬度在前，经度在后”。
举例如下，我们用一个多边形来索引阿姆斯特丹达姆广场：


PUT /attractions/landmark/dam_square
{
    "name" : "Dam Square, Amsterdam",
    "location" : {
        "type" : "polygon", <1>
        "coordinates" : [[ <2>
          [ 4.89218, 52.37356 ],
          [ 4.89205, 52.37276 ],
          [ 4.89301, 52.37274 ],
          [ 4.89392, 52.37250 ],
          [ 4.89431, 52.37287 ],
          [ 4.89331, 52.37346 ],
          [ 4.89305, 52.37326 ],
          [ 4.89218, 52.37356 ]
        ]]
    }
}
`type`参数指明如何使用经纬度坐标集来表示对应形状。
用来表示多边形的经纬度坐标点列表。
上例中大量的方括号可能看起来让人困惑，不过实际上 GeoJSON 的语法非常简单：

用一个数组表示经纬度坐标点:

  [lon,lat]
一组坐标点放到一个数组来表示一个多边形：

  [[lon,lat],[lon,lat], ... ]
一个多边形（polygon）形状可以包含多个多边形；第一个表示多边形的外轮廓，后续的多边形表示第一个多边形内部的空洞：
  [
    [[lon,lat],[lon,lat], ... ],  # main polygon
    [[lon,lat],[lon,lat], ... ],  # hole in main polygon
    ...
  ]

查询地理形状
地理形状一个不寻常的地方在于它运行我们使用形状来做查询，而不仅仅是坐标点。

举个例子，当我们的用户刚刚迈出阿姆斯特丹中央火车站时，我们可以用如下方式，查询出方圆1km内所有的地标：


GET /attractions/landmark/_search
{
  "query": {
    "geo_shape": {
      "location": { <1>
        "shape": { <2>
          "type":   "circle", <3>
          "radius": "1km"
          "coordinates": [ <4>
            4.89994,
            52.37815
          ]
        }
      }
    }
  }
}
查询使用 `location` 字段中的地理形状；
查询中的形状是由`shape`键对应的内容表示；
形状是一个半径为1km的圆形；
安姆斯特丹中央火车站入口的坐标点。
默认，查询（或者过滤器 —— 工作方式相同）会从已索引的形状中寻找与指定形状有交集的形状。 此外，relation 也可以设置为 disjoint来查找与指定形状不相交的，或者设置为within来查找完全落在查询形状中的。

举个例子，我们查找所有落在阿姆斯特丹内的地标：


GET /attractions/landmark/_search
{
  "query": {
    "geo_shape": {
      "location": {
        "relation": "within", <1>
        "shape": {
          "type": "polygon",
          "coordinates": [[ <2>
              [4.88330,52.38617],
              [4.87463,52.37254],
              [4.87875,52.36369],
              [4.88939,52.35850],
              [4.89840,52.35755],
              [4.91909,52.36217],
              [4.92656,52.36594],
              [4.93368,52.36615],
              [4.93342,52.37275],
              [4.92690,52.37632],
              [4.88330,52.38617]
            ]]
        }
      }
    }
  }
}
只匹配完全落在查询形状中的（已索引）形状。
这个多边形表示安姆斯特丹中心。


在查询中使用已索引的形状
对于那些经常会在查询中使用的形状，可以把它们索引起来以便在查询中可以方便地直接引用名字。 以之前的阿姆斯特丹中央为例，我们可以把它存储为一个类型为 neighborhood 的文档。

首先，我们仿照之前设置 landmark 时的方式建立一个映射：


PUT /attractions/_mapping/neighborhood
{
  "properties": {
    "name": {
      "type": "string"
    },
    "location": {
      "type": "geo_shape"
    }
  }
}
然后我们索引阿姆斯特丹中央对应的形状：


PUT /attractions/neighborhood/central_amsterdam
{
  "name" : "Central Amsterdam",
  "location" : {
      "type" : "polygon",
      "coordinates" : [[
        [4.88330,52.38617],
        [4.87463,52.37254],
        [4.87875,52.36369],
        [4.88939,52.35850],
        [4.89840,52.35755],
        [4.91909,52.36217],
        [4.92656,52.36594],
        [4.93368,52.36615],
        [4.93342,52.37275],
        [4.92690,52.37632],
        [4.88330,52.38617]
      ]]
  }
}
形状索引好之后，我们就可以在查询中通过 index, type 和 id 来引用它了：


GET /attractions/landmark/_search
{
  "query": {
    "geo_shape": {
      "location": {
        "relation": "within",
        "indexed_shape": { <1>
          "index": "attractions",
          "type":  "neighborhood",
          "id":    "central_amsterdam",
          "path":  "location"
        }
      }
    }
  }
}
指定 `indexed_shape` 而不是 `shape`，Elasticesearch 就知道需要从指定的文档和路径检索出对应的形状了。 阿姆斯特丹中央这个形状没有什么特别的。同样地，我们也可以使用已经索引好的阿姆斯特丹达姆广场。 这个查询查找出与阿姆斯特丹达姆广场有交集的临近点： ```json GET /attractions/neighborhood/_search { "query": { "geo_shape": { "location": { "indexed_shape": { "index": "attractions", "type": "landmark", "id": "dam_square", "path": "location" } } } } } ```

地理形状的过滤与缓存
地理形状 的查询和过滤都是表现为相同功能。查询就是简单的表现为一个过滤：把所以匹配到的文档的 _score 标记为1。 查询结果不能被缓存，不过过滤结果可以。

结果默认是不被缓存的。与地理坐标点集类似，任何形状内坐标的变化都会导致 geohash 集合的变化，因此在缓存过滤结果几乎没有什么意义。 也就是说，除非你会重复的使用相同的形状来做过滤，它才是值得缓存起来的。 缓存方法是，把 _cache 设置为 true：


GET /attractions/neighborhood/_search
{
  "query": {
    "filtered": {
      "filter": {
        "geo_shape": {
          "_cache": true, <1>
          "location": {
            "indexed_shape": {
              "index": "attractions",
              "type":  "landmark",
              "id":    "dam_square",
              "path":  "location"
            }
          }
        }
      }
    }
  }
}
`geo_shape` 过滤器的结果将被缓存。









嵌套-对象
嵌套对象

事实上在Elasticsearch中，创建丶删除丶修改一个文档是是原子性的，因此我们可以在一个文档中储存密切关联的实体。举例来说，我们可以在一个文档中储存一笔订单及其所有内容，或是储存一个Blog文章及其所有回应，藉由传递一个comments阵列：

PUT /my_index/blogpost/1
{
  "title": "Nest eggs",
  "body":  "Making your money work...",
  "tags":  [ "cash", "shares" ],
  "comments": [ <1>
    {
      "name":    "John Smith",
      "comment": "Great article",
      "age":     28,
      "stars":   4,
      "date":    "2014-09-01"
    },
    {
      "name":    "Alice White",
      "comment": "More like this please",
      "age":     31,
      "stars":   5,
      "date":    "2014-10-22"
    }
  ]
}
如果我们依靠动态映射，`comments`栏位会被自动建立为一个`object`栏位。 因为所有内容都在同一个文档中，使搜寻时并不需要连接(join)blog文章与回应，因此搜寻表现更加优异。 问题在於以上的文档可能会如下所示的匹配一个搜寻： ```json GET /_search { "query": { "bool": { "must": [ { "match": { "name": "Alice" }}, { "match": { "age": 28 }} ] } } } ``` Alice是31岁，而不是28岁！ 造成跨对象配对的原因如同我们在对象阵列中所讨论到，在于我们优美结构的JSON文档在索引中被扁平化为下方的 键-值 形式： ```json { "title": [ eggs, nest ], "body": [ making, money, work, your ], "tags": [ cash, shares ], "comments.name": [ alice, john, smith, white ], "comments.comment": [ article, great, like, more, please, this ], "comments.age": [ 28, 31 ], "comments.stars": [ 4, 5 ], "comments.date": [ 2014-09-01, 2014-10-22 ] } ``` `Alice`与`31` 以及 `John`与`2014-09-01` 之间的关联已经无法挽回的消失了。 当`object`类型的栏位用于储存_单一_对象是非常有用的。 从搜寻的角度来看，对於排序一个对象阵列来说关联是不需要的东西。 这是_嵌套对象_被设计来解决的问题。 藉由映射`commments`栏位为`nested`类型而不是`object`类型， 每个嵌套对象会被索引为一个_隐藏分割文档_，例如： ```json { "comments.name": [ john, smith ], "comments.comment": [ article, great ], "comments.age": [ 28 ], "comments.stars": [ 4 ], "comments.date": [ 2014-09-01 ] } { "comments.name": [ alice, white ], "comments.comment": [ like, more, please, this ], "comments.age": [ 31 ], "comments.stars": [ 5 ], "comments.date": [ 2014-10-22 ] } { "title": [ eggs, nest ], "body": [ making, money, work, your ], "tags": [ cash, shares ] } ``` 第一个`嵌套`对象 第二个`嵌套`对象 根或是父文档 藉由分别索引每个嵌套对象，对象的栏位中保持了其关联。 我们的查询可以只在同一个嵌套对象都匹配时才回应。 不仅如此，因嵌套对象都被索引了，连接嵌套对象至根文档的查询速度非常快--几乎与查询单一文档一样快。 这些额外的嵌套对象被隐藏起来，我们无法直接访问他们。 为了要新增丶修改或移除一个嵌套对象，我们必须重新索引整个文档。 要牢记搜寻要求的结果并不是只有嵌套对象，而是整个文档。

嵌套对象映射

设定一个nested栏位很简单--在你会设定为object类型的地方，改为nested类型：

PUT /my_index
{
  "mappings": {
    "blogpost": {
      "properties": {
        "comments": {
          "type": "nested", <1>
          "properties": {
            "name":    { "type": "string"  },
            "comment": { "type": "string"  },
            "age":     { "type": "short"   },
            "stars":   { "type": "short"   },
            "date":    { "type": "date"    }
          }
        }
      }
    }
  }
}
一个`nested`栏位接受与`object`类型相同的参数。 所需仅此而已。 任何`comments`对象会被索引为分离嵌套对象。 参考更多 [`nested` type reference docs](http://bit.ly/1KNQEP9)。

查询嵌套对象

因嵌套对象(nested objects)会被索引为分离的隐藏文档，我们不能直接查询它们。而是使用 nested查询或 nested 过滤器来存取它们：

GET /my_index/blogpost/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "title": "eggs" }}, <1>
        {
          "nested": {
            "path": "comments", <2>
            "query": {
              "bool": {
                "must": [ <3>
                  { "match": { "comments.name": "john" }},
                  { "match": { "comments.age":  28     }}
                ]
        }}}}
      ]
}}}
`title`条件运作在根文档上 `nested`条件``深入``嵌套的`comments`栏位。它不会在存取根文档的栏位，或是其他嵌套文档的栏位。 `comments.name`以及`comments.age`运作在相同的嵌套文档。 >### TIP >一个`nested`栏位可以包含其他`nested`栏位。 相同的，一个`nested`查询可以包含其他`nested`查询。 嵌套阶层会如同你预期的运作。 当然，一个`nested`查询可以匹配多个嵌套文档。 每个文档的匹配会有各自的关联分数，但多个分数必须减少至单一分数才能应用至根文档。 在预设中，它会平均所有嵌套文档匹配的分数。这可以藉由设定`score_mode`参数为`avg`, `max`, `sum`或甚至`none`(为了防止根文档永远获得`1.0`的匹配分数时)来控制。 ```json GET /my_index/blogpost/_search { "query": { "bool": { "must": [ { "match": { "title": "eggs" }}, { "nested": { "path": "comments", "score_mode": "max", "query": { "bool": { "must": [ { "match": { "comments.name": "john" }}, { "match": { "comments.age": 28 }} ] }}}} ] }}} ``` 从最匹配的嵌套文档中给予根文档的`_score`值。 >### 注意 >`nested`过滤器类似於`nested`查询，除了无法使用`score_mode`参数。 只能使用在_filter context_—例如在`filtered`查询中--其作用类似其他的过滤器： 包含或不包含，但不评分。 >`nested`过滤器的结果本身不会缓存，通常缓存规则会被应用於`nested`过滤器_之中_的过滤器。


以嵌套栏位排序

我们可以依照嵌套栏位中的值来排序，甚至藉由分离嵌套文档中的值。为了使其结果更加有趣，我们加入另一个记录：

PUT /my_index/blogpost/2
{
  "title": "Investment secrets",
  "body":  "What they don't tell you ...",
  "tags":  [ "shares", "equities" ],
  "comments": [
    {
      "name":    "Mary Brown",
      "comment": "Lies, lies, lies",
      "age":     42,
      "stars":   1,
      "date":    "2014-10-18"
    },
    {
      "name":    "John Smith",
      "comment": "You're making it up!",
      "age":     28,
      "stars":   2,
      "date":    "2014-10-16"
    }
  ]
}
想像我们要取回在十月中有收到回应的blog文章，并依照所取回的各个blog文章中最少stars数量的顺序作排序。 这个搜寻请求如下：

GET /_search
{
  "query": {
    "nested": { <1>
      "path": "comments",
      "filter": {
        "range": {
          "comments.date": {
            "gte": "2014-10-01",
            "lt":  "2014-11-01"
          }
        }
      }
    }
  },
  "sort": {
    "comments.stars": { <2>
      "order": "asc",   <2>
      "mode":  "min",   <2>
      "nested_filter": { <3>
        "range": {
          "comments.date": {
            "gte": "2014-10-01",
            "lt":  "2014-11-01"
          }
        }
      }
    }
  }
}
`nested`查询限制了结果为十月份收到回应的blog文章。 结果在所有匹配的回应中依照`comment.stars`栏位的最小值(`min`)作递增(`asc`)的排序。 排序条件中的`nested_filter`与主查询`query`条件中的`nested`查询相同。 於下一个下方解释。 为什么我们要在`nested_filter`重复写上查询条件？ 原因是排序在於执行查询后才发生。 此查询匹配了在十月中有收到回应的blog文章，回传blog文章文档作为结果。 如果我们不加上`nested_filter`条件，我们最後会依照任何blog文章曾经收到过的回应作排序，而不是在十月份收到的。


如同我们在查询时需要使用nested查询来存取嵌套对象，专门的nested集合使我们可以取得嵌套对象中栏位的集合：

GET /my_index/blogpost/_search?search_type=count
{
  "aggs": {
    "comments": { <1>
      "nested": {
        "path": "comments"
      },
      "aggs": {
        "by_month": {
          "date_histogram": { <2>
            "field":    "comments.date",
            "interval": "month",
            "format":   "yyyy-MM"
          },
          "aggs": {
            "avg_stars": {
              "avg": { <3>
                "field": "comments.stars"
              }
            }
          }
        }
      }
    }
  }
}
`nested`集合`深入`嵌套对象的`comments`栏位 评论基於`comments.date`栏位被分至各个月份分段 每个月份分段单独计算星号的平均数 结果显示集合发生於嵌套文档层级： ```json ... "aggregations": { "comments": { "doc_count": 4, "by_month": { "buckets": [ { "key_as_string": "2014-09", "key": 1409529600000, "doc_count": 1, "avg_stars": { "value": 4 } }, { "key_as_string": "2014-10", "key": 1412121600000, "doc_count": 3, "avg_stars": { "value": 2.6666666666666665 } } ] } } } ... ``` 此处总共有四个`comments`: 一个在九月以及三个在十月 ## 反向-嵌套-集合 ### 反向嵌套-集合 一个`nested`集合只能存取嵌套文档中的栏位，而无法看见根文档或其他嵌套文档中的栏位。 然而，我们可以_跳出_嵌套区块，藉由`reverse_nested`集合回到父阶层。 举例来说，我们可以发现使用评论者的年龄为其加上`tags`很有趣。 `comment.age`是在嵌套栏位中，但是`tags`位於根文档： ```json GET /my_index/blogpost/_search?search_type=count { "aggs": { "comments": { "nested": { "path": "comments" }, "aggs": { "age_group": { "histogram": { "field": "comments.age", "interval": 10 }, "aggs": { "blogposts": { "reverse_nested": {}, "aggs": { "tags": { "terms": { "field": "tags" } } } } } } } } } } ``` `nested`集合深入`comments`对象 `histogram`集合以`comments.age`栏位聚集成每十年一个的分段 `reverse_nested`集合跳回到根文档 `terms`集合计算每个年龄分段的火红词语 简略的结果显示如下： ```json .. "aggregations": { "comments": { "doc_count": 4, "age_group": { "buckets": [ { "key": 20, "doc_count": 2, "blogposts": { "doc_count": 2, "tags": { "doc_count_error_upper_bound": 0, "buckets": [ { "key": "shares", "doc_count": 2 }, { "key": "cash", "doc_count": 1 }, { "key": "equities", "doc_count": 1 } ] } } }, ... ``` 共有四个评论 有两个评论的发表者年龄介於20至30之间 两个blog文章与这些评论相关 这些blog文章的火红标签是`shares`丶`cash`丶`equities` ### 什麽时候要使用嵌套对象 嵌套对象对於当有一个主要实体(如`blogpost`)，加上有限数量的紧密相关实体(如`comments`)是非常有用的。 有办法能以评论内容找到blog文章很有用，且`nested`查询及过滤器提供短查询时间连接(fast query-time joins)。 嵌套模型的缺点如下： * 如欲新增丶修改或删除一个嵌套文档，则必须重新索引整个文档。因此越多嵌套文档造成越多的成本。 * 搜寻请求回传整个文档，而非只有匹配的嵌套文档。 虽然有个进行中的计画要支持只回传根文档及最匹配的嵌套文档，但目前并未支持。 有时你需要完整分离主要文档及其关连实体。 _父-子关系_提供这一个功能。









segment、buffer和translog对实时性的影响

既然介绍数据流向，首先第一步就是：写入的数据是如何变成 Elasticsearch 里可以被检索和聚合的索引内容的？
以单文件的静态层面看，每个全文索引都是一个词元的倒排索引，具体涉及到全文索引的通用知识，这里不单独介绍，有兴趣的读者可以阅读《Lucene in Action》等书籍详细了解。
动态更新的 Lucene 索引

以在线动态服务的层面看，要做到实时更新条件下数据的可用和可靠，就需要在倒排索引的基础上，再做一系列更高级的处理。
其实总结一下 Lucene 的处理办法，很简单，就是一句话：新收到的数据写到新的索引文件里。
Lucene 把每次生成的倒排索引，叫做一个段(segment)。然后另外使用一个 commit 文件，记录索引内所有的 segment。而生成 segment 的数据来源，则是内存中的 buffer。也就是说，动态更新过程如下：
当前索引有 3 个 segment 可用。索引状态如图 2-1； A Lucene index with a commit point and three segments 图 2-1
新接收的数据进入内存 buffer。索引状态如图 2-2； A Lucene index with new documents in the in-memory buffer, ready to commit 图 2-2
内存 buffer 刷到磁盘，生成一个新的 segment，commit 文件同步更新。索引状态如图 2-3。 After a commit, a new segment is added to the index and the buffer is cleared 图 2-3
利用磁盘缓存实现的准实时检索

既然涉及到磁盘，那么一个不可避免的问题就来了：磁盘太慢了！对我们要求实时性很高的服务来说，这种处理还不够。所以，在第 3 步的处理中，还有一个中间状态：
内存 buffer 生成一个新的 segment，刷到文件系统缓存中，Lucene 即可检索这个新 segment。索引状态如图 2-4。 The buffer contents have been written to a segment, which is searchable, but is not yet commited 图 2-4
文件系统缓存真正同步到磁盘上，commit 文件更新。达到图 2-3 中的状态。
这一步刷到文件系统缓存的步骤，在 Elasticsearch 中，是默认设置为 1 秒间隔的，对于大多数应用来说，几乎就相当于是实时可搜索了。Elasticsearch 也提供了单独的 /_refresh 接口，用户如果对 1 秒间隔还不满意的，可以主动调用该接口来保证搜索可见。
不过对于 ELK Stack 的日志场景来说，恰恰相反，我们并不需要如此高的实时性，而是需要更快的写入性能。所以，一般来说，我们反而会通过 /_settings 接口或者定制 template 的方式，加大 refresh_interval 参数：
# curl -XPOST http://127.0.0.1:9200/logstash-2015.06.21/_settings -d'
{ "refresh_interval": "10s" }
'
如果是导入历史数据的场合，那甚至可以先完全关闭掉：
# curl -XPUT http://127.0.0.1:9200/logstash-2015.05.01 -d'
{
  "settings" : {
    "refresh_interval": "-1"
  }
}'
在导入完成以后，修改回来或者手动调用一次即可：
# curl -XPOST http://127.0.0.1:9200/logstash-2015.05.01/_refresh
translog 提供的磁盘同步控制

既然 refresh 只是写到文件系统缓存，那么第 4 步写到实际磁盘又是有什么来控制的？如果这期间发生主机错误、硬件故障等异常情况，数据会不会丢失？
这里，其实有另一个机制来控制。Elasticsearch 在把数据写入到内存 buffer 的同时，其实还另外记录了一个 translog 日志。也就是说，第 2 步并不是图 2-2 的状态，而是像图 2-5 这样：
New documents are added to the in-memory buffer and appended to the transaction log 图 2-5
在第 3 和第 4 步，refresh 发生的时候，translog 日志文件依然保持原样，如图 2-6：
The transaction log keeps accumulating documents 图 2-6
也就是说，如果在这期间发生异常，Elasticsearch 会从 commit 位置开始，恢复整个 translog 文件中的记录，保证数据一致性。
等到真正把 segment 刷到磁盘，且 commit 文件进行更新的时候， translog 文件才清空。这一步，叫做 flush。同样，Elasticsearch 也提供了 /_flush 接口。
对于 flush 操作，Elasticsearch 默认设置为：每 30 分钟主动进行一次 flush，或者当 translog 文件大小大于 512MB (老版本是 200MB)时，主动进行一次 flush。这两个行为，可以分别通过 index.translog.flush_threshold_period 和 index.translog.flush_threshold_size 参数修改。
如果对这两种控制方式都不满意，Elasticsearch 还可以通过 index.translog.flush_threshold_ops 参数，控制每收到多少条数据后 flush 一次。
translog 的一致性

索引数据的一致性通过 translog 保证。那么 translog 文件自己呢？
默认情况下，Elasticsearch 每 5 秒，或每次请求操作结束前，会强制刷新 translog 日志到磁盘上。
后者是 Elasticsearch 2.0 新加入的特性。为了保证不丢数据，每次 index、bulk、delete、update 完成的时候，一定触发刷新 translog 到磁盘上，才给请求返回 200 OK。这个改变在提高数据安全性的同时当然也降低了一点性能。
如果你不在意这点可能性，还是希望性能优先，可以在 index template 里设置如下参数：
{
    "index.translog.durability": "async"
}
Elasticsearch 分布式索引

大家可能注意到了，前面一段内容，一直写的是"Lucene 索引"。这个区别在于，Elasticsearch 为了完成分布式系统，对一些名词概念作了变动。索引成为了整个集群级别的命名，而在单个主机上的Lucene 索引，则被命名为分片(shard)。至于数据是怎么识别到自己应该在哪个分片，请阅读稍后有关 routing 的章节。





segment merge对写入性能的影响

通过上节内容，我们知道了数据怎么进入 ES 并且如何才能让数据更快的被检索使用。其中用一句话概括了 Lucene 的设计思路就是"开新文件"。从另一个方面看，开新文件也会给服务器带来负载压力。因为默认每 1 秒，都会有一个新文件产生，每个文件都需要有文件句柄，内存，CPU 使用等各种资源。一天有 86400 秒，设想一下，每次请求要扫描一遍 86400 个文件，这个响应性能绝对好不了！2

为了解决这个问题，ES 会不断在后台运行任务，主动将这些零散的 segment 做数据归并，尽量让索引内只保有少量的，每个都比较大的，segment 文件。这个过程是有独立的线程来进行的，并不影响新 segment 的产生。归并过程中，索引状态如图 2-7，尚未完成的较大的 segment 是被排除在检索可见范围之外的：
Two commited segments and one uncommited segment in the process of being merged into a bigger segment 图 2-7
当归并完成，较大的这个 segment 刷到磁盘后，commit 文件做出相应变更，删除之前几个小 segment，改成新的大 segment。等检索请求都从小 segment 转到大 segment 上以后，删除没用的小 segment。这时候，索引里 segment 数量就下降了，状态如图 2-8 所示：
Once merging has finished, the old segments are deleted 图 2-8
归并线程配置

segment 归并的过程，需要先读取 segment，归并计算，再写一遍 segment，最后还要保证刷到磁盘。可以说，这是一个非常消耗磁盘 IO 和 CPU 的任务。所以，ES 提供了对归并线程的限速机制，确保这个任务不会过分影响到其他任务。
默认情况下，归并线程的限速配置 indices.store.throttle.max_bytes_per_sec 是 20MB。对于写入量较大，磁盘转速较高，甚至使用 SSD 盘的服务器来说，这个限速是明显过低的。对于 ELK Stack 应用，建议可以适当调大到 100MB或者更高。
# curl -XPUT http://127.0.0.1:9200/_cluster/settings -d'
{
    "persistent" : {
        "indices.store.throttle.max_bytes_per_sec" : "100mb"
    }
}'
归并线程的数目，ES 也是有所控制的。默认数目的计算公式是： Math.min(3, Runtime.getRuntime().availableProcessors() / 2)。即服务器 CPU 核数的一半大于 3 时，启动 3 个归并线程；否则启动跟 CPU 核数的一半相等的线程数。相信一般做 ELK Stack 的服务器 CPU 合数都会在 6 个以上。所以一般来说就是 3 个归并线程。如果你确定自己磁盘性能跟不上，可以降低 index.merge.scheduler.max_thread_count 配置，免得 IO 情况更加恶化。
归并策略

归并线程是按照一定的运行策略来挑选 segment 进行归并的。主要有以下几条：
index.merge.policy.floor_segment 默认 2MB，小于这个大小的 segment，优先被归并。
index.merge.policy.max_merge_at_once 默认一次最多归并 10 个 segment
index.merge.policy.max_merge_at_once_explicit 默认 optimize 时一次最多归并 30 个 segment。
index.merge.policy.max_merged_segment 默认 5 GB，大于这个大小的 segment，不用参与归并。optimize 除外。
根据这段策略，其实我们也可以从另一个角度考虑如何减少 segment 归并的消耗以及提高响应的办法：加大 flush 间隔，尽量让每次新生成的 segment 本身大小就比较大。
optimize 接口

既然默认的最大 segment 大小是 5GB。那么一个比较庞大的数据索引，就必然会有为数不少的 segment 永远存在，这对文件句柄，内存等资源都是极大的浪费。但是由于归并任务太消耗资源，所以一般不太选择加大 index.merge.policy.max_merged_segment 配置，而是在负载较低的时间段，通过 optimize 接口，强制归并 segment。2

# curl -XPOST http://127.0.0.1:9200/logstash-2015-06.10/_optimize?max_num_segments=1
由于 optimize 线程对资源的消耗比普通的归并线程大得多，所以，绝对不建议对还在写入数据的热索引执行这个操作。这个问题对于 ELK Stack 来说非常好办，一般索引都是按天分割的。更合适的任务定义方式，请阅读本书稍后的 curator 章节。



routing和replica的读写过程

之前两节，完整介绍了在单个 Lucene 索引，即 ES 分片内的数据写入流程。现在彻底回到 ES 的分布式层面上来，当一个 ES 节点收到一条数据的写入请求时，它是如何确认这个数据应该存储在哪个节点的哪个分片上的？
路由计算

作为一个没有额外依赖的简单的分布式方案，ES 在这个问题上同样选择了一个非常简洁的处理方式，对任一条数据计算其对应分片的方式如下：
shard = hash(routing) % number_of_primary_shards
每个数据都有一个 routing 参数，默认情况下，就使用其 _id 值。将其 _id 值计算哈希后，对索引的主分片数取余，就是数据实际应该存储到的分片 ID。
由于取余这个计算，完全依赖于分母，所以导致 ES 索引有一个限制，索引的主分片数，不可以随意修改。因为一旦主分片数不一样，所以数据的存储位置计算结果都会发生改变，索引数据就完全不可读了。
副本一致性

作为分布式系统，数据副本可算是一个标配。ES 数据写入流程，自然也涉及到副本。在有副本配置的情况下，数据从发向 ES 节点，到接到 ES 节点响应返回，流向如下(附图 2-9)：
客户端请求发送给 Node 1 节点，注意图中 Node 1 是 Master 节点，实际完全可以不是。
Node 1 用数据的 _id 取余计算得到应该讲数据存储到 shard 0 上。通过 cluster state 信息发现 shard 0 的主分片已经分配到了 Node 3 上。Node 1 转发请求数据给 Node 3。
Node 3 完成请求数据的索引过程，存入主分片 0。然后并行转发数据给分配有 shard 0 的副本分片的 Node 1 和 Node 2。当收到任一节点汇报副本分片数据写入成功，Node 3 即返回给初始的接收节点 Node 1，宣布数据写入成功。Node 1 返回成功响应给客户端。
Creating, indexing or deleting a single document 图 2-9
这个过程中，有几个参数可以用来控制或变更其行为：
replication 通过在客户端发送请求的 URL 中加上 ?replication=async ，可以控制 Node 3 在完成本机主分片写入后，就返回给 Node 1 宣布写入成功。这个参数看似可以提高 ES 接收数据写入的性能，但事实上，由于 ES 的副本数据写入也是要经过完成索引过程的，一旦由于发送过多数据，主机负载偏高导致某块数据写入有异常，可能整个主机的 CPU 都会飙高，导致分配到这台主机上其他主分片的数据都无法高性能完成，最终反而拖累了整体的写入性能。 从 ES 1.6 版本开始，该参数已经被标记为废弃，2.0 版预计将正式删除该参数。
consistency 上面示例中，2 个副本分片只要有 1 个成功，就可以返回给客户端了。这点也是有配置项的。其默认值的计算来源如下：
int( (primary + number_of_replicas) / 2 ) + 1
根据需要，也可以将参数设置为 one，表示仅写完主分片就返回，等同于 async；还可以设置为 all，表示等所有副本分片都写完才能返回。
timeout 如果集群出现异常，有些分片当前不可用，ES 默认会等待 1 分钟看分片能否恢复。可以使用 ?timeout=30s 参数来缩短这个等待时间。
副本配置和分片配置不一样，是可以随时调整的。有些较大的索引，甚至可以在做 optimize 前，先把副本全部取消掉，等 optimize 完后，再重新开启副本，节约单个 segment 的重复归并消耗。
# curl -XPUT http://127.0.0.1:9200/logstash-mweibo-2015.05.02/_settings -d '{
    "index": { "number_of_replicas" : 0 }
}'




shard 的 allocate 控制

某个 shard 分配在哪个节点上，一般来说，是由 ES 自动决定的。以下几种情况会触发分配动作：
新索引生成
索引的删除
新增副本分片
节点增减引发的数据均衡
ES 提供了一系列参数详细控制这部分逻辑：
cluster.routing.allocation.enable 该参数用来控制允许分配哪种分片。默认是 all。可选项还包括 primaries 和 new_primaries。none 则彻底拒绝分片。该参数的作用，本书稍后集群升级章节会有说明。
cluster.routing.allocation.allow_rebalance 该参数用来控制什么时候允许数据均衡。默认是 indices_all_active，即要求所有分片都正常启动成功以后，才可以进行数据均衡操作，否则的话，在集群重启阶段，会浪费太多流量了。
cluster.routing.allocation.cluster_concurrent_rebalance 该参数用来控制集群内同时运行的数据均衡任务个数。默认是 2 个。如果有节点增减，且集群负载压力不高的时候，可以适当加大。
cluster.routing.allocation.node_initial_primaries_recoveries 该参数用来控制节点重启时，允许同时恢复几个主分片。默认是 4 个。如果节点是多磁盘，且 IO 压力不大，可以适当加大。
cluster.routing.allocation.node_concurrent_recoveries 该参数用来控制节点除了主分片重启恢复以外其他情况下，允许同时运行的数据恢复任务。默认是 2 个。所以，节点重启时，可以看到主分片迅速恢复完成，副本分片的恢复却很慢。除了副本分片本身数据要通过网络复制以外，并发线程本身也减少了一半。当然，这种设置也是有道理的——主分片一定是本地恢复，副本分片却需要走网络，带宽是有限的。从 ES 1.6 开始，冷索引的副本分片可以本地恢复，这个参数也就是可以适当加大了。
indices.recovery.concurrent_streams 该参数用来控制节点从网络复制恢复副本分片时的数据流个数。默认是 3 个。可以配合上一条配置一起加大。
indices.recovery.max_bytes_per_sec 该参数用来控制节点恢复时的速率。默认是 40MB。显然是比较小的，建议加大。
此外，ES 还有一些其他的分片分配控制策略。比如以 tag 和 rack_id 作为区分等。一般来说，ELK Stack 场景中使用不多。运维人员可能比较常见的策略有两种：
磁盘限额 为了保护节点数据安全，ES 会定时(cluster.info.update.interval，默认 30 秒)检查一下各节点的数据目录磁盘使用情况。在达到 cluster.routing.allocation.disk.watermark.low (默认 85%)的时候，新索引分片就不会再分配到这个节点上了。在达到 cluster.routing.allocation.disk.watermark.high (默认 90%)的时候，就会触发该节点现存分片的数据均衡，把数据挪到其他节点上去。这两个值不但可以写百分比，还可以写具体的字节数。有些公司可能出于成本考虑，对磁盘使用率有一定的要求，需要适当抬高这个配置：
# curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.disk.watermark.low" : "85%",
        "cluster.routing.allocation.disk.watermark.high" : "10gb",
        "cluster.info.update.interval" : "1m"
    }
}'
热索引分片不均 默认情况下，ES 集群的数据均衡策略是以各节点的分片总数(indices_all_active)作为基准的。这对于搜索服务来说无疑是均衡搜索压力提高性能的好办法。但是对于 ELK Stack 场景，一般压力集中在新索引的数据写入方面。正常运行的时候，也没有问题。但是当集群扩容时，新加入集群的节点，分片总数远远低于其他节点。这时候如果有新索引创建，ES 的默认策略会导致新索引的所有主分片几乎全分配在这台新节点上。整个集群的写入压力，压在一个节点上，结果很可能是这个节点直接被压死，集群出现异常。 所以，对于 ELK Stack 场景，强烈建议大家预先计算好索引的分片数后，配置好单节点分片的限额。比如，一个 5 节点的集群，索引主分片 10 个，副本 1 份。则平均下来每个节点应该有 4 个分片，那么就配置：
# curl -s -XPUT http://127.0.0.1:9200/logstash-2015.05.08/_settings -d '{
    "index": { "routing.allocation.total_shards_per_node" : "5" }
}'
注意，这里配置的是 5 而不是 4。因为我们需要预防有机器故障，分片发生迁移的情况。如果写的是 4，那么分片迁移会失败。
reroute 接口

上面说的各种配置，都是从策略层面，控制分片分配的选择。在必要的时候，还可以通过 ES 的 reroute 接口，手动完成对分片的分配选择的控制。
reroute 接口支持三种指令：allocate，move 和 cancel。常用的一般是 allocate 和 move：
allocate 指令
因为负载过高等原因，有时候个别分片可能长期处于 UNASSIGNED 状态，我们就可以手动分配分片到指定节点上。默认情况下只允许手动分配副本分片，所以如果是主分片故障，需要单独加一个 allow_primary 选项：1

# curl -XPOST 127.0.0.1:9200/_cluster/reroute -d '{
  "commands" : [ {
        "allocate" :
            {
              "index" : "logstash-2015.05.27", "shard" : 61, "node" : "10.19.0.77", "allow_primary" : true
            }
        }
  ]
}'
注意，如果是历史数据的话，请提前确认一下哪个节点上保留有这个分片的实际目录，且目录大小最大。然后手动分配到这个节点上。以此减少数据丢失。
move 指令
因为负载过高，磁盘利用率过高，服务器下线，更换磁盘等原因，可以会需要从节点上移走部分分片：
curl -XPOST 127.0.0.1:9200/_cluster/reroute -d '{
  "commands" : [ {
        "move" :
            {
              "index" : "logstash-2015.05.22", "shard" : 0, "from_node" : "10.19.0.81", "to_node" : "10.19.0.104"
            }
        }
  ]
}'
节点下线

集群中个别节点出现故障预警等情况，需要下线，也是 Elasticsearch 运维工作中常见的情况。如果已经稳定运行过一段时间的集群，每个节点上都会保存有数量不少的分片。这种时候通过 reroute 接口手动转移，就显得太过麻烦了。这个时候，有另一种方式：
curl -XPOST 127.0.0.1:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
   }
}'
Elasticsearch 集群就会自动把这个 IP 上的所有分片，都自动转移到其他节点上。等到转移完成，这个空节点就可以毫无影响的下线了。
和 _ip 类似的参数还有 _host, _name 等。此外，这类参数不单是 cluster 级别，也可以是 index 级别。下一小节就是 index 级别的用例。
冷热数据的读写分离

Elasticsearch 集群一个比较突出的问题是: 用户做一次大的查询的时候, 非常大量的读 IO 以及聚合计算导致机器 Load 升高, CPU 使用率上升, 会影响阻塞到新数据的写入, 这个过程甚至会持续几分钟。所以，可能需要仿照 MySQL 集群一样，做读写分离。1

实施方案

N 台机器做热数据的存储, 上面只放当天的数据。这 N 台热数据节点上面的 elasticsearc.yml 中配置 node.tag: hot
之前的数据放在另外的 M 台机器上。这 M 台冷数据节点中配置 node.tag: stale
模板中控制对新建索引添加 hot 标签：
{
 "order" : 0,
 "template" : "*",
 "settings" : {
   "index.routing.allocation.require.tag" : "hot"
 }
}
每天计划任务更新索引的配置, 将 tag 更改为 stale, 索引会自动迁移到 M 台冷数据节点
# curl -XPUT http://127.0.0.1:9200/indexname/_settings -d'
{
"index": {
   "routing": {
      "allocation": {
         "require": {
            "tag": "stale"
         }
      }
  }
}
}'
这样，写操作集中在 N 台热数据节点上，大范围的读操作集中在 M 台冷数据节点上。避免了堵塞影响。
该方案运用的，是 Elasticsearch 中的 allocation filter 功能，详细说明见：https://www.elastic.co/guide/en/elasticsearch/reference/master/shard-allocation-filtering.html






集群自动发现

ES 是一个 P2P 类型(使用 gossip 协议)的分布式系统，除了集群状态管理以外，其他所有的请求都可以发送到集群内任意一台节点上，这个节点可以自己找到需要转发给哪些节点，并且直接跟这些节点通信。
所以，从网络架构及服务配置上来说，构建集群所需要的配置极其简单。在无阻碍的网络下，所有配置了相同 cluster.name 的节点都自动归属到一个集群中。
multicast 方式

只配置 cluster.name 的集群，其实就是采用了默认的自动发现协议，即组播(multicast)方式。节点会在本机所有网卡接口上，使用组播地址 224.2.2.4 ，以 54328 端口建立组播组发送 clustername 信息。
但是，并不是所有的路由交换设备都支持并且开启了组播信息传输！甚至可以说，默认情况下，都是不开启组播信息传输的。
所以在没有网络工程师帮助的情况下，ES 以默认组播方式，只有在同一个交换机下的节点，能自动发现，跨交换机的节点，是无法收到组播信息的。
此外，由于节点是以所有网卡接口发送组播信息，而操作系统内核层面对组播信息来源的验证中，却对网卡接口地址有一步校验，有可能发生内核层面的信息丢弃，导致多网卡的节点也无法正常使用组播方式。
Elasticsearch 2.0 开始，为安全考虑，默认不分发 multicast 功能。依然希望使用 multicast 自动发现的用户，需要单独安装:
bin/plugin install discovery-multicast
unicast 方式

除了组播方式，ES 还支持单播(unicast)方式。配置里提供几台节点的地址，ES 将其视作 gossip router 角色，借以完成集群的发现。由于这只是 ES 内一个很小的功能，所以 gossip router 角色并不需要单独配置，每个 ES 节点都可以担任。所以，采用单播方式的集群，各节点都配置相同的几个节点列表作为 router 即可。
此外，考虑到节点有时候因为高负载，慢 GC 等原因可能会有偶尔没及时响应 ping 包的可能，一般建议稍微加大 Fault Detection 的超时时间。
discovery.zen.minimum_master_nodes: 3
discovery.zen.ping.timeout: 100s
discovery.zen.fd.ping_timeout: 100s
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["10.19.0.97","10.19.0.98","10.19.0.99","10.19.0.100"]






增删改查

增删改查是数据库的基础操作方法。ES 虽然不是数据库，但是很多场合下，都被人们当做一个文档型 NoSQL 数据库在使用，原因自然是因为在接口和分布式架构层面的相似性。虽然在 ELK Stack 场景下，数据的写入和查询，分别由 Logstash 和 Kibana 代劳，作为测试、调研和排错时的基本功，还是需要了解一下 ES 的增删改查用法的。
数据写入

ES 的一大特点，就是全 RESTful 接口处理 JSON 请求。所以，数据写入非常简单：
# curl -XPOST http://127.0.0.1:9200/logstash-2015.06.21/testlog -d '{
    "date" : "1434966686000",
    "user" : "chenlin7",
    "mesg" : "first message into Elasticsearch"
}'
命令返回响应结果为：
{"_index":"logstash-2015.06.21","_type":"testlog","_id":"AU4ew3h2nBE6n0qcyVJK","_version":1,"created":true}
数据获取

可以看到，在数据写入的时候，会返回该数据的 _id。这就是后续用来获取数据的关键：
# curl -XGET http://127.0.0.1:9200/logstash-2015.06.21/testlog/AU4ew3h2nBE6n0qcyVJK
命令返回响应结果为：
{"_index":"logstash-2015.06.21","_type":"testlog","_id":"AU4ew3h2nBE6n0qcyVJK","_version":1,"found":true,"_source":{
    "date" : "1434966686000",
    "user" : "chenlin7",
    "mesg" : "first message into Elasticsearch"
}}
这个 _source 里的内容，正是之前写入的数据。
如果觉得这个返回看起来有点太过麻烦，可以使用 curl -XGET http://127.0.0.1:9200/logstash-2015.06.21/testlog/AU4ew3h2nBE6n0qcyVJK/_source 来指明只获取源数据部分。
更进一步的，如果你只想看数据中的一部分字段内容，可以使用 curl -XGET http://127.0.0.1:9200/logstash-2015.06.21/testlog/AU4ew3h2nBE6n0qcyVJK?fields=user,mesg 来指明获取字段，结果如下：
{"_index":"logstash-2015.06.21","_type":"testlog","_id":"AU4ew3h2nBE6n0qcyVJK","_version":1,"found":true,"fields":{"user":["chenlin7"],"mesg":["first message into Elasticsearch"]}}
数据删除

要删除数据，修改发送的 HTTP 请求方法为 DELETE 即可：
# curl -XDELETE http://127.0.0.1:9200/logstash-2015.06.21/testlog/AU4ew3h2nBE6n0qcyVJK
删除不单针对单条数据，还可以删除整个整个索引。甚至可以用通配符。
# curl -XDELETE http://127.0.0.1:9200/logstash-2015.06.0*
在 Elasticsearch 2.x 之前，可以通过查询语句删除，也可以删除某个 _type 内的数据。现在都已经不再内置支持，改为 Delete by Query 插件。因为这种方式本身对性能影响较大！
数据更新

已经写过的数据，同样还是可以修改的。有两种办法，一种是全量提交，即指明 _id 再发送一次写入请求。
# curl -XPOST http://127.0.0.1:9200/logstash-2015.06.21/testlog/AU4ew3h2nBE6n0qcyVJK -d '{
    "date" : "1434966686000",
    "user" : "chenlin7",
    "mesg" " "first message into Elasticsearch but version 2"
}'
另一种是局部更新，使用 /_update 接口：
# curl -XPOST 'http://127.0.0.1:9200/logstash-2015.06.21/testlog/AU4ew3h2nBE6n0qcyVJK/_update' -d '{
    "doc" : {
        "user" : "someone"
    }
}'
或者

# curl -XPOST 'http://127.0.0.1:9200/logstash-2015.06.21/testlog/AU4ew3h2nBE6n0qcyVJK/_update' -d '{
    "script" : "ctx._source.user = \"someone\""
}'





搜索请求

上节介绍的，都是针对单条数据的操作。在 ES 环境中，更多的是搜索和聚合请求。在之前章节中，我们也介绍过数据获取和数据搜索的一点区别：刚写入的数据，可以通过 translog 立刻获取；但是却要等到 refresh 成为一个 segment 后，才能被搜索到。本节就介绍一下 ES 的搜索语法。
全文搜索

ES 对搜索请求，有简易语法和完整语法两种方式。简易语法作为以后在 Kibana 上最常用的方式，一定是需要学会的。而在命令行里，我们可以通过最简单的方式来做到。还是上节输入的数据：
# curl -XGET http://127.0.0.1:9200/logstash-2015.06.21/testlog/_search?q=first
可以看到返回结果：
{"took":240,"timed_out":false,"_shards":{"total":27,"successful":27,"failed":0},"hits":{"total":1,"max_score":0.11506981,"hits":[{"_index":"logstash-2015.06.21","_type":"testlog","_id":"AU4ew3h2nBE6n0qcyVJK","_score":0.11506981,"_source":{
    "date" : "1434966686000",
    "user" : "chenlin7",
    "mesg" : "first message into Elasticsearch"
}}]}}
还可以用下面语句搜索，结果是一样的。
# curl -XGET http://127.0.0.1:9200/logstash-2015.06.21/testlog/_search?q=user:"chenlin7"
querystring 语法

上例中，?q=后面写的，就是 querystring 语法。鉴于这部分内容会在 Kibana 上经常使用，这里详细解析一下语法：
全文检索：直接写搜索的单词，如上例中的 first；
单字段的全文检索：在搜索单词之前加上字段名和冒号，比如如果知道单词 first 肯定出现在 mesg 字段，可以写作 mesg:first；
单字段的精确检索：在搜索单词前后加双引号，比如 user:"chenlin7"；
多个检索条件的组合：可以使用 NOT, AND 和 OR 来组合检索，注意必须是大写。比如 user:("chenlin7" OR "chenlin") AND NOT mesg:first；
字段是否存在：_exists_:user 表示要求 user 字段存在，_missing_:user 表示要求 user 字段不存在；
通配符：用 ? 表示单字母，* 表示任意个字母。比如 fir?t mess*；
正则：需要比通配符更复杂一点的表达式，可以使用正则。比如 mesg:/mes{2}ages?/。注意 ES 中正则性能很差，而且支持的功能也不是特别强大，尽量不要使用。ES 支持的正则语法见：https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-regexp-query.html#regexp-syntax；
近似搜索：用 ~ 表示搜索单词可能有一两个字母写的不对，请 ES 按照相似度返回结果。比如 frist~；
范围搜索：对数值和时间，ES 都可以使用范围搜索，比如：rtt:>300，date:["now-6h" TO "now"} 等。其中，[] 表示端点数值包含在范围内，{} 表示端点数值不包含在范围内；
完整语法

ES 支持各种类型的检索请求，除了可以用 querystring 语法表达的以外，还有很多其他类型，具体列表和示例可参见：https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-queries.html。
作为最简单和常用的示例，这里展示一下 term query 的写法，相当于 querystring 语法中的 user:"chenlin7"：
# curl -XGET http://127.0.0.1:9200/_search -d '
{
  "query": {
    "term": {
      "user": "chenlin7" 
    }
  }
}'
聚合请求

在检索范围确定之后，ES 还支持对结果集做聚合查询，返回更直接的聚合统计结果。在 ES 1.0 版本之前，这个接口叫 Facet，1.0 版本之后，这个接口改为 Aggregation。
Kibana 分别在 v3 中使用 Facet，v4 中使用 Aggregation。不过总的来说，Aggregation 是 Facet 接口的强化升级版本，我们直接了解 Aggregation 即可。本书后续章节也会介绍如何在 Kibana 的 v3 版本中使用 aggregation 接口做二次开发。
堆叠聚合示例

在 Elasticsearch 1.x 系列中，aggregation 分为 bucket 和 metric 两种，分别用作词元划分和数值计算。而其中的 bucket aggregation，还支持在自身结果集的基础上，叠加新的 aggregation。这就是 aggregation 比 facet 最领先的地方。比如实现一个时序百分比统计，在 facet 接口就无法直接完成，而在 aggregation 接口就很简单了：1

# curl -XPOST 'http://127.0.0.1:9200/logstash-2015.06.22/_search?size=0&pretty' -d'{
    "aggs" : {
        "percentile_over_time" : {
            "date_histogram" : {
                "field"    : "@timestamp",
                "interval" : "1h"
            },
            "aggs" : {
                "percentile_one_time" : {
                    "percentiles" : {
                        "field" : "requesttime"
                    }
                }
            }
        }
    }
}'
得到结果如下：
{
  "took" : 151595,
  "timed_out" : false,
  "_shards" : {
    "total" : 81,
    "successful" : 81,
    "failed" : 0
  },
  "hits" : {
    "total" : 3307142043,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "percentile_over_time" : {
      "buckets" : [ {
        "key_as_string" : "22/Jun/2015:22:00:00 +0000",
        "key" : 1435010400000,
        "doc_count" : 459273981,
        "percentile_one_time" : {
          "values" : {
            "1.0" : 0.004,
            "5.0" : 0.006,
            "25.0" : 0.023,
            "50.0" : 0.035,
            "75.0" : 0.08774675719725569,
            "95.0" : 0.25732934416125663,
            "99.0" : 0.7508899754871812
          }
        }
      }, {
        "key_as_string" : "23/Jun/2015:00:00:00 +0000",
        "key" : 1435017600000,
        "doc_count" : 768620219,
        "percentile_one_time" : {
          "values" : {
            "1.0" : 0.004,
            "5.0" : 0.007000000000000001,
            "25.0" : 0.025,
            "50.0" : 0.03987809503972864,
            "75.0" : 0.10297843567746187,
            "95.0" : 0.30047269327062875,
            "99.0" : 1.015495933753329
          }
        }
      }, {
        "key_as_string" : "23/Jun/2015:02:00:00 +0000",
        "key" : 1435024800000,
        "doc_count" : 849467060,
        "percentile_one_time" : {
          "values" : {
            "1.0" : 0.004,
            "5.0" : 0.008,
            "25.0" : 0.027000000000000003,
            "50.0" : 0.0439999899006102,
            "75.0" : 0.1160416197625958,
            "95.0" : 0.3383140614483838,
            "99.0" : 1.0275839684542212
          }
        }
      } ]
    }
  }
}
管道聚合示例

在 Elasticsearch 2.x 中，新增了 pipeline aggregation 类型。可以在已有 aggregation 返回的数组数据之后，再对这组数值做一次运算。最常见的，就是对时序数据求移动平均值。比如对响应时间做周期为 7，移动窗口为 30，alpha, beta, gamma 参数均为 0.5 的 holt-winters 季节性预测 2 个未来值的请求如下：
{
    "aggs" : {
        "my_date_histo" : {
            "date_histogram" : {
                 "field" : "@timestamp",
                 "interval" : "1h"
            },
            "aggs" : {
                "avgtime" : {
                    "avg" : { "field" : "requesttime" }
                },
                "the_movavg" : {
                    "moving_avg" : {
                        "buckets_path" : "avgtime",
                        "window" : 30,
                        "model" : "holt_winters",
                        "predict" : 2,
                        "settings" : {
                            "type" : "mult",
                            "alpha" : 0.5,
                            "beta" : 0.5,
                            "gamma" : 0.5,
                            "period" : 7,
                            "pad" : true
                        }
                    }
                }
            }
        }
    }
}
响应如下：
{
  "took" : 12,
  "timed_out" : false,
  "_shards" : {
    "total" : 10,
    "successful" : 10,
    "failed" : 0
  },
  "hits" : {
    "total" : 111331,
    "max_score" : 0.0,
    "hits" : [  ]
  },
  "aggregations" : {
    "my_date_histo" : {
      "buckets" : [ {
        "key_as_string" : "2015-12-24T02:00:00.000Z",
        "key" : 1450922400000,
        "doc_count" : 1462,
        "avgtime" : {
          "value" : 508.25649794801643
        }
      }, {
        ...
      }, {
        "key_as_string" : "2015-12-24T17:00:00.000Z",
        "key" : 1450976400000,
        "doc_count" : 1664,
        "avgtime" : {
          "value" : 504.7067307692308
        },
        "the_movavg" : {
          "value" : 500.9766851760192
        }
      }, {
        ...
      }, {
        "key_as_string" : "2015-12-25T09:00:00.000Z",
        "key" : 1451034000000,
        "doc_count" : 0,
        "the_movavg" : {
          "value" : 493.9519632950849,
          "value_as_string" : "1970-01-01T00:00:00.493Z"
        }
    } ]
  }
}
可以看到，在第一个移动窗口还没满足之前，是没有移动平均值的；而在实际数据已经结束以后，虽然没有平均值了，但是预测的移动平均值却还有数。
buckets_path 语法

由于 aggregation 是有堆叠层级关系的，所以 pipeline aggregation 在引用 metric aggregation 时也就会涉及到层级的问题。在上例中，the_movavg 和 avgtime 是同一层级，所以 buckets_path 直接写 avgtime 即可。但是如果我们把 the_movavg 上提一层，跟 my_date_histo 同级，这个 buckets_path 怎么写才行呢？
"buckets_path" : "my_date_histo>avgtime"
如果用的是返回的数值有多个值的聚合，比如 percentiles 或者 extended_stats，则是：
"buckets_path" : "percentile_over_time>percentile_one_time.95"
ES 目前能支持的聚合请求列表，参见：https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html。
See Also

Holt Winters 预测算法，见：https://en.wikipedia.org/wiki/Holt-Winters。其在运维领域最著名的运用是 RRDtool 中的 HWPREDICT。
search 请求参数

from
从索引的第几条数据开始返回，默认是 0；
size
返回多少条数据，默认是 10。
注意：Elasticsearch 集群实际是需要给 coordinate node 返回 shards number * (from + size) 条数据，然后在单机上进行排序，最后给客户端返回这个 size 大小的数据的。所以请谨慎使用 from 和 size 参数。
此外，Elasticsearch 2.x 还新增了一个索引级别的动态控制配置项：index.max_result_window，默认为 10000。即 from + size 大于 10000 的话，Elasticsearch 直接拒绝掉这次请求不进行具体搜索，以保护节点。
另外，Elasticsearch 2.x 还提供了一个小优化：当设置 "size":0 时，自动改变 search_type 为 count。跳过搜索过程的 fetch 阶段。
timeout
coordinate node 等待超时时间。到达该阈值后，coordinate node 直接把当前收到的数据返回给客户端，不再继续等待 data node 后续的返回了。
注意：这个参数只是为了配合客户端程序，并不能取消掉 data node 上搜索任务还在继续运行和占用资源。
terminate_after
各 data node 上，扫描单个分片时，找到多少条记录后，就认为足够了。这个参数可以切实保护 data node 上搜索任务不会长期运行和占用资源。但是也就意味着搜索范围没有覆盖全部索引，是一个抽样数据。准确率是不好判断的。
request_cache
各 data node 上，在分片级别，对请求的响应(仅限于 hits.total 数值、aggregation 和 suggestion 的结果集)做的缓存。注意：这个缓存的键值要求很严格，请求的 JSON 必须一字不易，缓存才能命中。
另外，request_cache 参数不能写在请求 JSON 里，只能以 URL 参数的形式存在。示例如下：
curl -XPOST http://localhost:9200/_search?request_cache=true -d '
{
    "size" : 0,
    "timeout" : "120s",
    "terminate_after" : 1000000,
    "query" : { "match_all" : {} },
    "aggs" : { "terms" : { "terms" : { "field" : "keyname" } } }
}
'






script

Elasticsearch 中，可以使用自定义脚本扩展功能。包括评分、过滤函数和聚合字段等方面。作为 ELK Stack 场景，我们只介绍在聚合字段方面使用 script 的方式。
动态提交

最简单易用的方式，就是在正常的请求体中，把 field 换成 script 提交。比如一个标准的 terms agg 改成 script 方式，写法如下：
# curl 127.0.0.1:9200/logstash-2015.06.29/_search -d '{
    "aggs" : {
        "clientip_top10" : {
            "terms" : {
                "script" : "doc['clientip'].value"
            }
        }
    }
}'
在 script 中，有两种方式引用数据：doc['clientip'].value 和 _source.clientip。其区别在于：doc[].value 读取 fielddata 内的数据，_source.obj.attr 读取 _source 的 JSON 内容。这也意味着，前者必须读取的是最终的词元字段数据，而后者可以返回任意的数据结构。
注意：因为读取的是 fielddata，所以如果有分词的话，doc[].value 读取到的是分词后的数据。所以请按需使用 doc['clientip.raw'].value 写法。
固定文件

ES 在 1.4.0 之前，默认脚本引擎是使用 mvel 语言，随后改成 groovy，但是从 1.4.3 开始，因为安全漏洞，关掉了动态提交功能。只能使用固定文件方式运行。
为了和动态提交的语法有区别，调用固定文件的写法如下：
# curl 127.0.0.1:9200/logstash-2015.06.29/_search -d '{
    "aggs" : {
        "clientip_subnet_top10" : {
            "terms" : {
                "script_file" : "getvalue",
                "lang" : "groovy",
                "params" : {
                    "fieldname": "clientip.raw",
                    "pattern": "^((?:\d{1,3}\.?){3})\.\d{1,3}$"
                }
            }
        }
    }
}'
上例要求在 ES 集群的所有数据节点上，都保存有一个 /etc/elasticsearch/scripts/getvalue.groovy 文件，并且该脚本文件可以接收 fieldname 和 pattern 两个变量。试举例如下：
#!/usr/bin/env groovy
matcher = ( doc[fieldname].value =~ /${pattern}/ )
if (matcher.matches()) {
    matcher[0][1]
}
注意：ES 进程默认每分钟扫描一次 /etc/elasticsearch/scripts/ 目录，并尝试加载该目录下所有文件作为 script。所以，不要在该目录内做文件编辑等工作，不要分发 .svn 等目录到生成环境，这些临时或者隐藏文件都会被 ES 进程加载然后报错。
其他语言

ES 支持通过插件方式，扩展脚本语言的支持，目前默认自带的语言包括：
lucene expression
groovy
mustache
而 github 上目前已有以下语言插件支持，基本覆盖了所有 JVM 上的可用语言：
https://github.com/elastic/elasticsearch-lang-mvel
https://github.com/elastic/elasticsearch-lang-javascript
https://github.com/elastic/elasticsearch-lang-python
https://github.com/hiredman/elasticsearch-lang-clojure
https://github.com/felipehummel/elasticsearch-lang-scala
https://github.com/fcheung/elasticsearch-jruby




reindex

Elasticsearch 本身不提供对索引的 rename，mapping 的 alter 等操作。所以，如果有需要对全索引数据进行导出，或者修改某个已有字段的 mapping 设置等情况下，我们只能通过 scroll API 导出全部数据，然后重新做一次索引写入。这个过程，叫做 reindex。
既然没有直接的方式，那么自然只能使用其他工具了。这里介绍两个常用的方法，自己写程序和用 logstash。
Perl 客户端

Elastic 官方提供各种语言的客户端库，其中，Perl 库提供了对 reindex 比较方便的写法和示例。通过 cpanm Search::Elasticsearch 命令安装库完毕后，使用以下程序即可：
use Search::Elasticsearch;

my $es   = Search::Elasticsearch->new(
    nodes => ['192.168.0.2:9200']
);
my $bulk = $es->bulk_helper(
    index   => 'new_index',
);

$bulk->reindex(
    source  => {
        index       => 'old_index',
        size        => 500,         # default
        search_type => 'scan'       # default
    }
);
Logstash 做 reindex

在最新版的 Logstash 中，对 logstash-input-elasticsearch 插件做了一定的修改，使得通过 logstash 完成 reindex 成为可能。
reindex 操作的 logstash 配置如下：
input {
  elasticsearch {
    hosts => [ "192.168.0.2" ]
    index => "old_index"
    size => 500
    scroll => "5m"
    docinfo => true
  }
}
output {
  elasticsearch {
    hosts => [ "192.168.0.3" ]
    index => "%{[@metadata][_index]}"
    document_type => "%{[@metadata][_type]}"
    document_id => "%{[@metadata][_id]}"
  }
}
如果你做 reindex 的源索引并不是 logstash 记录的内容，也就是没有 @timestamp, @version 这两个 logstash 字段，那么可以在上面配置中添加一段 filter 配置，确保前后索引字段完全一致：
filter {
  mutate {
    remove_field => [ "@timestamp", "@version" ]
  }
}





批量提交

在 CRUD 章节，我们已经知道 ES 的数据写入是如何操作的了。喜欢自己动手的读者可能已经迫不及待的自己写了程序开始往 ES 里写数据做测试。这时候大家会发现：程序的运行速度非常一般，即使 ES 服务运行在本机，一秒钟大概也就能写入几百条数据。
这种速度显然不是 ES 的极限。事实上，每条数据经过一次完整的 HTTP POST 请求和 ES indexing 是一种极大的性能浪费，为此，ES 设计了批量提交方式。在数据读取方面，叫 mget 接口，在数据变更方面，叫 bulk 接口。mget 一般常用于搜索时 ES 节点之间批量获取中间结果集，对于 ELK Stack 用户，更常见到的是 bulk 接口。
bulk 接口采用一种比较简朴的数据积累格式，示例如下：
# curl -XPOST http://127.0.0.1:9200/_bulk -d'
{ "create" : { "_index" : "test", "_type" : "type1"  } }
{ "field1" : "value1" }
{ "delete" : { "_index" : "test", "_type" : "type1" } }
{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } }
{ "field1" : "value2" }
{ "update" : {"_id" : "1", "_type" : "type1", "_index" : "test"} }
{ "doc" : {"field2" : "value2"} }
'
格式是，每条 JSON 数据的上面，加一行描述性的元 JSON，指明下一行数据的操作类型，归属索引信息等。
采用这种格式，而不是一般的 JSON 数组格式，是因为接收到 bulk 请求的 ES 节点，就可以不需要做完整的 JSON 数组解析处理，直接按行处理简短的元 JSON，就可以确定下一行数据 JSON 转发给哪个数据节点了。这样，一个固定内存大小的 network buffer 空间，就可以反复使用，又节省了大量 JVM 的 GC。
事实上，产品级的 logstash、rsyslog、spark 都是默认采用 bulk 接口进行数据写入的。对于打算自己写程序的读者，建议采用 Perl 的 Search::Elasticsearch::Bulk 或者 Python 的 elasticsearch.helpers.* 库。
bulk size

在配置 bulk 数据的时候，一般需要注意的就是请求体大小(bulk size)。
这里有一点细节上的矛盾，我们知道，HTTP 请求，是可以通过 HTTP 状态码 100 Continue 来持续发送数据的。但对于 ES 节点接收 HTTP 请求体的 Content-Length 来说，是按照整个大小来计算的。所以，首先，要确保 bulk 数据不要超过 http.max_content_length 设置。
那么，是不是尽量让 bulk size 接近这个数值呢？当然不是。
依然是请求体的问题，因为请求体需要全部加载到内存，而 JVM Heap 一共就那么多(按 31GB 算)，过大的请求体，会挤占其他线程池的空间，反而导致写入性能的下降。
再考虑网卡流量，磁盘转速的问题，所以一般来说，建议 bulk 请求体的大小，在 15MB 左右，通过实际测试继续向上探索最合适的设置。
注意：这里说的 15MB 是请求体的字节数，而不是程序里里设置的 bulk size。bulk size 一般指数据的条目数。不要忘了，bulk 请求体中，每条数据还会额外带上一行元 JSON。
以 logstash 默认的 bulk_size => 5000 为例，假设单条数据平均大小 200B ，一次 bulk 请求体的大小就是 1.5MB。那么我们可以尝试 bulk_size => 50000；而如何单条数据平均大小是 20KB，一次 bulk 大小就是 100MB，显然超标了，需要尝试下调至 bulk_size => 500。
UDP

ES 其实还提供了一个连 HTTP header 解析步骤都能省略的 bulk 方法，叫 UDP bulk，即开启 UDP 9700 端口，直接 nc 发送 bulk 数据内容写入。
由于 UDP 的不可靠性，ES 计划从 2.0 版本开始废弃该功能，确实需要高性能写入又不担心数据缺失问题的读者，可以参考 ES 官方文档使用该功能。




gateway

gateway 是 ES 设计用来长期存储索引数据的接口。一般来说，大家都是用本地磁盘来存储索引数据，即 gateway.type 为 local。
数据恢复中，有很多策略调整我们已经在之前分片控制小节讲过。除开分片级别的控制以外，gateway 级别也还有一些可优化的地方：
gateway.recover_after_nodes 该参数控制集群在达到多少个节点的规模后，才开始数据恢复任务。这样可以避免集群自动发现的初期，分片不全的问题。
gateway.recover_after_time 该参数控制集群在达到上条配置设置的节点规模后，再等待多久才开始数据恢复任务。
gateway.expected_nodes 该参数设置集群的预期节点总数。在达到这个总数后，即认为集群节点已经完全加载，即可开始数据恢复，不用再等待上条设置的时间。
注意：gateway 中说的节点，仅包括主节点和数据节点，纯粹的 client 节点是不算在内的。如果你有更明确的选择，也可以按需求写：
gateway.recover_after_data_nodes
gateway.recover_after_master_nodes
gateway.expected_data_nodes
gateway.expected_master_nodes
共享存储上的影子副本

虽然 ES 对 gateway 使用 NFS，iscsi 等共享存储的方式极力反对，但是对于较大量级的索引的副本数据，ES 从 1.5 版本开始，还是提供了一种节约成本又不特别影响性能的方式：影子副本(shadow replica)。
首先，需要在集群各节点的 elasticsearch.yml 中开启选项：
node.enable_custom_paths: true
同时，确保各节点使用相同的路径挂载了共享存储，且目录权限为 Elasticsearch 进程用户可读可写。
然后，创建索引：
# curl -XPUT 'http://127.0.0.1:9200/my_index' -d '
{
    "index" : {
        "number_of_shards" : 1,
        "number_of_replicas" : 4,
        "data_path": "/var/data/my_index",
        "shadow_replicas": true
    }
}'
针对 shadow replicas ，ES 节点不会做实际的索引操作，而是单纯的每次 flush 时，把 segment 内容 fsync 到共享存储磁盘上。然后 refresh 让其他节点能够搜索该 segment 内容。所以，shadow replicas 里是没有 translog 的，对于还没有 refresh 的数据，如果 GET 获取请求传到 shadow replicas 上，是查询不到的，请求会自动变成 ?preference=_primary 模式，只从主分片上获取数据。同理，在 cluster state 还没定期更新过来之前，节点上的索引映射可能也还保持着自己主分片数据的样式，不会因为 shadow replica 里数据样式的变动发生变动，搜索请求也有可能失败。
综上，shadow replicas 只是一个在某些特定环境下有用的方式。在资源允许的情况下，还是应该使用 local gateway。而另外采用 snapshot 接口来完成数据长期备份到 HDFS 或其他共享存储的需要。




集群状态维护

我们都知道，ES 中的 master 跟一般 MySQL、Hadoop 的 master 是不一样的。它即不是写入流量的唯一入口，也不是所有数据的元信息的存放地点。所以，一般来说，ES 的 master 节点负载很轻，集群性能是可以近似认为随着 data 节点的扩展线性提升的。
但是，上面这句话并不是完全正确的。
ES 中有一件事情是只有 master 节点能管理的，这就是集群状态(cluster state)。
集群状态中包括以下信息：
集群层面的设置
集群内有哪些节点
各索引的设置，映射，分析器和别名等
索引内各分片所在的节点位置
这些信息在集群的任意节点上都存放着，你也可以通过 /_cluster/state 接口直接读取到其内容。注意这最后一项信息，之前我们已经讲过 ES 怎么通过简单地取余知道一条数据放在哪个分片里，加上现在集群状态里又记载了分片在哪个节点上，那么，整个集群里，任意节点都可以知道一条数据在哪个节点上存储了。所以，数据读写才可以发送给集群里任意节点。
至于修改，则只能由 master 节点完成！显然，集群状态里大部分内容是极少变动的，唯独有一样除外——索引的映射。因为 ES 的 schema-less 特性，我们可以任意写入 JSON 数据，所以索引中随时可能增加新的字段。这个时候，负责容纳这条数据的主分片所在的节点，会暂停写入操作，将字段的映射结果传递给 master 节点；master 节点合并这段修改到集群状态里，发送新版本的集群状态到集群的所有节点上。然后写入操作才会继续。一般来说，这个操作是在一二十毫秒内就可以完成，影响也不大。1

但是也有一些情况会是例外。
批量新索引创建

在较大规模的 ELK Stack 应用场景中，这是比较常见的一个情况。因为 ELK Stack 建议采用日期时间作为索引的划分方式，所以定时(一般是每天)，会统一产生一批新的索引。而前面已经讲过，ES 的集群状态每次更新都是阻塞式的发布到全部节点上以后，节点才能继续后续处理。
这就意味着，如果在集群负载较高的时候，批量新建新索引，可能会有一个显著的阻塞时间，无法写入任何数据。要等到全部节点同步完成集群状态以后，数据写入才能恢复。
不巧的是，中国使用的是北京时间，UTC +0800。也就是说，默认的 ELK Stack 新建索引时间是在早上 8 点。这个时间点一般日志写入量已经上涨到一定水平了(当然，晚上 0 点的量其实也不低)。
对此，可以通过定时任务，每天在最低谷的早上三四点，提前通过 POST mapping 的方式，创建好之后几天的索引。就可以避免这个问题了。
过多字段持续更新

这是另一种常见的滥用。在使用 ELK Stack 处理访问日志时，为了查询更方便，可能会采用 logstash-filter-kv 插件，将访问日志中的每个 URL 参数，都切分成单独的字段。比如一个 "/index.do?uid=1234567890&action=payload" 的 URL 会被转换成如下 JSON：1

  "urlpath" : "/index.do",
  "urlargs" : {
    "uid" : "1234567890",
    "action" : "payload",
    ...
  }
但是，因为集群状态是存在所有节点的内存里的，一旦 URL 参数过多，ES 节点的内存就被大量用于存储字段映射内容。这是一个极大的浪费。如果碰上 URL 参数的键内容本身一直在变动，直接撑爆 ES 内存都是有可能的！
以上是真实发生的事件，开发人员莫名的选择将一个 UUID 结果作为 key 放在 URL 参数里。直接导致 ES 集群 master 节点全部 OOM。
如果你在 ES 日志中一直看到有新的 updating mapping [logstash-2015.06.01] 字样出现的话，请郑重考虑一下自己是不是用的上如此细分的字段列表吧。
好，三秒钟过去，如果你确定一定以及肯定还要这么做，下面是一个变通的解决办法。
nested object

用 nested object 来存放 URL 参数的方法稍微复杂，但还可以接受。单从 JSON 数据层面看，新方式的数据结构如下：
  "urlargs": [
    { "key": "uid", "value": "1234567890" },
    { "key": "action", "value": "payload" },
    ...
  ]
没错，看起来就是一个数组。但是 JSON 数组在 ES 里是有两种处理方式的。
如果直接写入数组，ES 在实际索引过程中，会把所有内容都平铺开，变成 Arrays of Inner Objects。整条数据实际类似这样的结构：
{
  "urlpath" : ["/index.do"],
  "urlargs.key" : ["uid", "action", ...],
  "urlargs.value" : ["1234567890", "payload", ...]
这种方式最大的问题是，当你采用 urlargs.key:"uid" AND urlargs.value:"0987654321" 语句意图搜索一个 uid=0987654321 的请求时，实际是整个 URL 参数中任意一处 value 为 0987654321 的，都会命中。
要想达到正确搜索的目的，需要在写入数据之前，指定 urlargs 字段的映射类型为 nested object。命令如下：
curl -XPOST http://127.0.0.1:9200/logstash-2015.06.01/_mapping -d '{
  "accesslog" : {
    "properties" : {
      "urlargs" : {
        "type" : "nested",
        "properties" : {
            "key" : { "type" : "string", "index" : "not_analyzed", "doc_values" : true },
            "value" : { "type" : "string", "index" : "not_analyzed", "doc_values" : true }
        }
      }
    }
  } 
}'
这样，数据实际是类似这样的结构：
{
  "urlpath" : ["/index.do"],
},
{
  "urlargs.key" : ["uid"],
  "urlargs.value" : ["1234567890"],
},
{
  "urlargs.key" : ["action"],
  "urlargs.value" : ["payload"],
}
当然，nested object 节省字段映射的优势对应的是它在使用的复杂。Query 和 Aggs 都必须使用专门的 nested query 和 nested aggs 才能正确读取到它。
nested query 语法如下：
curl -XPOST http://127.0.0.1:9200/logstash-2015.06.01/accesslog/_search -d '
{
  "query": {
    "bool": {
      "must": [
        { "match": { "urlpath" : "/index.do" }}, 
        {
          "nested": {
            "path": "urlargs", 
            "query": {
              "bool": {
                "must": [ 
                  { "match": { "urlargs.key": "uid" }},
                  { "match": { "urlargs.value": "1234567890" }}
                ]
        }}}}
      ]
}}}'
nested aggs 语法如下：
curl -XPOST http://127.0.0.1:9200/logstash-2015.06.01/accesslog/_search -d '
{
  "aggs": {
    "topnuid": {
      "nested": {
        "path": "urlargs"
      },
      "aggs": {
        "uid": {
          "filter": {
            "term": {
              "urlargs.key": "uid",
            }
          },
          "aggs": {
            "topn": {
              "terms": { 
                "field": "urlargs.value"
              }
            }
          }
        }
      }
    }
  }
}'





缓存

ES 内针对不同阶段，设计有不同的缓存。以此提升数据检索时的响应性能。主要包括节点层面的 filter cache 和索引层面的 query cache。下面分别讲述。
filter cache

ES 的 query DSL 分为 query 和 filter 两种，很多检索语法，是同时存在 query 和 filter 里的。比如最常用的 term、prefix、range 等。那么，怎么选择是使用 query 还是 filter 呢？
首先，要明白 query 跟 filter 的区别：1

query 是要相关性评分的，filter 不要；
query 结果无法缓存，filter 可以。
所以，选择也就出来了：
全文搜索、评分排序，使用 query；
是非过滤，精确匹配，使用 filter。
上面做对比时，说 filter 能用缓存 query 不能，其实是不精确的。默认情况下，并不是所有的 filter 都能用缓存。常用的比如 term、terms、prefix、range、bool 等 filter，其过滤结果明确，也容易设置缓存，ES 就对这几个默认开启了 filter cache 功能。而更复杂的一些比如 geo、script 等 filter，从 fielddata 数据到过滤结果还需要进行一系列计算的，ES 默认是不开启 filter cache 的。而像 and、not、or 这几个关系型 filter，也是不开启的。
如果想要强制开启这些默认没有的 filter cache，需要在请求的 JSON 中带上 "cache": true 参数。
检索的时候想尽量使用 filter cache，但是 ES 接口有要求必须传递 query 参数，这时候，有一个特殊的 query，叫 filtered query。其作用是在 query 中使用 filter，这样，我们就可以实现如下这样的请求：
# curl -XGET http://127.0.0.1:9200/_search -d '
{
  "query": {
    "filtered": {
      "filter": {
        "range": { "@timestamp": { "gte": "now - 1d / d" }}
      }
    }
  }
}'
事实上，Kibana3 中，就大量使用了这种 filtered query 语法。
需要注意的是，filter cache 是节点层面的缓存设置，每个节点上所有数据在响应请求时，是共用一个缓存空间的。当空间用满，按照 LRU 策略淘汰掉最冷的数据。
可以用 indices.cache.filter.size 配置来设置这个缓存空间的大小，默认是 JVM 堆的 10%，也可以设置一个绝对值。
shard query cache

ES 还有另一个索引层面的缓存，叫 shard query cache。之前章节中说过，ES 集群的任意节点都可以接受请求，它会自动转发给数据所在的各个节点，等待各节点把各自的结果返回后，完成数据的汇聚处理再返回给客户端。
这里可以把这个过程再细化一下。ES 对请求的处理过程，是有不同类型的，默认的叫 query_then_fetch。在这种情况下，各数据节点处理检索请求后，返回的，是只包含文档 id 和相关性分值的结果，这一段处理，叫做 query 阶段；汇聚到这份结果后，按照分值排序，得到一个全集群最终需要的文档 id，再向对应节点发送一次文档获取请求，拿到文档内容，这一段处理，叫做 fetch 阶段。两段都结束后才返回响应。在稍后的 ES 日志记录章节，我们可以看到 ES 对这两个阶段，甚至都有分别的慢查询记录。
此外，还有 DFS_query_then_fetch 类型，提高小数据量时的精确度；query_and_fetch 类型，在有明确 routing 时可以省略一个数据来回；count 类型，再不关心文档内容只需要计数时省略 fetch 阶段，这是 ELK Stack 聚合统计场景最常用的类型；scan 类型，批量获取数据省略 query 阶段，在 reindex 时就是使用这种类型。
回到 query cache，这里这个 query，就是处理过程中 query 阶段的意思。各个节点上的数据分片，会在处理完 query 阶段时，将得到的本分片有关该请求的计数值，缓存起来。
根据上面的请求类型介绍，显然，只有当 ?search_type=count 的时候，这个 query cache 才能起到作用。
不过，query cache 默认并不开启。因为 query cache 要起作用，还有几个先决条件：
分片数据不再变动，也就是对当天的索引是无效的(如果 refresh_interval 很大，那么在这个间隔内倒也算有效)；
使用了 "now" 语法的请求无法被缓存，因为这个是要即时计算的；
缓存的键是请求的整个 JSON 字符串，整个字符串发生任何字节变动，缓存都无效。
以 ELK Stack 场景来说，Kibana 里几乎所有的请求，都是有 @timestamp 作为过滤条件的，而且大多数是以最近 N 小时/分钟这样的选项，也就是说，页面每次刷新，发出的请求 JSON 里的时间过滤部分都是在变动的。query cache 在处理 Kibana 发出的请求时，完全无用。
所以，虽然官网宣称 query cache 对日志场景非常有用，但是对使用 Kibana 的一般用户来说，完全没法体会到这个优势。
如果是自己写程序做历史统计分析和展示的，想办法固化时间过滤条件或者干脆去掉这个条件，那么用上这个特性还是不错的。要对某个索引开启 query cache 的话，利用 index settings 接口动态调整即可：
# curl -XPUT http://127.0.0.1:9200/logstash-2015.06.23/_settings -d'
{
    "index.cache.query.enable": true 
}'
此外，还可以针对具体某个请求单独开启：
# curl 'http://127.0.0.1:9200/logstash-2015.06.23/_search?search_type=count&query_cache=true' -d'
{
  "aggs": {
    "popular_colors": {
      "terms": {
        "field": "colors"
      }
    }
  }
}'
和 filter cache 一样，query cache 的大小也是以节点级别控制的，配置项名为 indices.cache.query.size，其默认值为 1%。




字段数据

Elasticsearch 2.x 已经默认所有 not_analyzed 字段自动启用 doc_values ！本节不再重要，仅作为知识共享保留。
字段数据(fielddata)，在 Lucene 中又叫 uninverted index。我们都知道，搜索引擎会使用倒排索引(inverted index)来映射单词到文档的 ID 号。而同时，为了提供对文档内容的聚合，Lucene 还可以在运行时将每个字段的单词以字典序排成另一个 uninverted index，可以大大加速计算性能。
作为一个加速性能的方式，fielddata 当然是被全部加载在内存的时候最为有效。这也是 ES 默认的运行设置。但是，内存是有限的，所以 ES 同时也需要提供对 fielddata 内存的限额方式：
indices.fielddata.cache.size 节点用于 fielddata 的最大内存，如果 fielddata 达到该阈值，就会把旧数据交换出去。该参数可以设置百分比或者绝对值。默认设置是不限制，所以强烈建议设置该值，比如 10%。
indices.fielddata.cache.expire 进入 fielddata 内存中的数据多久自动过期。注意，因为 ES 的 fielddata 本身是一种数据结构，而不是简单的缓存，所以过期删除 fielddata 是一个非常消耗资源的操作。ES 官方在文档中特意说明，这个参数绝对绝对不要设置！
Circuit Breaker

Elasticsearch 在 total，fielddata，request 三个层面上都设计有 circuit breaker 以保护进程不至于发生 OOM 事件。在 fielddata 层面，其设置为：
indices.breaker.fielddata.limit 默认是 JVM 堆内存大小的 60%。注意，为了让设置正常发挥作用，如果之前设置过 indices.fielddata.cache.size 的，一定要确保 indices.breaker.fielddata.limit 的值大于 indices.fielddata.cache.size 的值。否则的话，fielddata 大小一到 limit 阈值就报错，就永远道不了 size 阈值，无法触发对旧数据的交换任务了。
doc values

但是相比较集群庞大的数据量，内存本身是远远不够的。为了解决这个问题，ES 引入了另一个特性，可以对精确索引的字段，指定 fielddata 的存储方式。这个配置项叫：doc_values。
所谓 doc_values，其实就是在 ES 将数据写入索引的时候，提前生成好 fielddata 内容，并记录到磁盘上。因为 fielddata 数据是顺序读写的，所以即使在磁盘上，通过文件系统层的缓存，也可以获得相当不错的性能。
注意：因为 doc_values 是在数据写入时即生成内容，所以，它只能应用在精准索引的字段上，因为索引进程没法知道后续会有什么分词器生成的结果。所以，字段设置应该是这样：
    "myfieldname": {
        "type":       "string",
        "index":      "not_analyzed",
        "doc_values": true
    }




curator

如果经过之前章节的一系列优化之后，数据确实超过了集群能承载的能力，除了拆分集群以外，最后就只剩下一个办法了：清除废旧索引。
为了更加方便的做清除数据，合并 segment，备份恢复等管理任务，Elasticsearch 在提供相关 API 的同时，另外准备了一个命令行工具，叫 curator 。curator 是 Python 程序，可以直接通过 pypi 库安装：
pip install elasticsearch-curator
注意，是 elasticsearch-curator 不是 curator。PyPi 原先就有另一个项目叫这个名字
参数介绍

和 ELK Stack 里其他组件一样，curator 也是被 Elastic.co 收购的原开源社区周边。收编之后同样进行了一次重构，命令行参数从单字母风格改成了长单词风格。新版本的 curator 命令可用参数如下：
Usage: curator [OPTIONS] COMMAND [ARGS]...
Options 包括:
--host TEXT Elasticsearch host. --url_prefix TEXT Elasticsearch http url prefix. --port INTEGER Elasticsearch port. --use_ssl Connect to Elasticsearch through SSL. --http_auth TEXT Use Basic Authentication ex: user:pass --timeout INTEGER Connection timeout in seconds. --master-only Only operate on elected master node. --dry-run Do not perform any changes. --debug Debug mode --loglevel TEXT Log level --logfile TEXT log file --logformat TEXT Log output format [default|logstash]. --version Show the version and exit. --help Show this message and exit.
Commands 包括: alias Index Aliasing allocation Index Allocation bloom Disable bloom filter cache close Close indices delete Delete indices or snapshots open Open indices optimize Optimize Indices replicas Replica Count Per-shard show Show indices or snapshots snapshot Take snapshots of indices (Backup)
针对具体的 Command，还可以继续使用 --help 查看该子命令的帮助。比如查看 close 子命令的帮助，输入 curator close --help，结果如下：
Usage: curator close [OPTIONS] COMMAND [ARGS]...

  Close indices

Options:
  --help  Show this message and exit.

Commands:
  indices  Index selection.
常用示例

在使用 1.4.0 以上版本的 Elasticsearch 前提下，curator 曾经主要的一个子命令 bloom 已经不再需要使用。所以，目前最常用的三个子命令，分别是 close, delete 和 optimize，示例如下：
curator --timeout 36000 --host 10.0.0.100 delete indices --older-than 5 --time-unit days --timestring '%Y.%m.%d' --prefix logstash-mweibo-nginx-
curator --timeout 36000 --host 10.0.0.100 delete indices --older-than 10 --time-unit days --timestring '%Y.%m.%d' --prefix logstash-mweibo-client- --exclude 'logstash-mweibo-client-2015.05.11'
curator --timeout 36000 --host 10.0.0.100 delete indices --older-than 30 --time-unit days --timestring '%Y.%m.%d' --regex '^logstash-mweibo-\d+'
curator --timeout 36000 --host 10.0.0.100 close indices --older-than 7 --time-unit days --timestring '%Y.%m.%d' --prefix logstash-
curator --timeout 36000 --host 10.0.0.100 optimize --max_num_segments 1 indices --older-than 1 --newer-than 7 --time-unit days --timestring '%Y.%m.%d' --prefix logstash-
这一顿任务，结果是：
logstash-mweibo-nginx-yyyy.mm.dd 索引保存最近 5 天，logstash-mweibo-client-yyyy.mm.dd 保存最近 10 天，logstash-mweibo-yyyy.mm.dd 索引保存最近 30 天；且所有七天前的 logstash-* 索引都暂时关闭不用；最后对所有非当日日志做 segment 合并优化。





扩展和测试方案

在体验完 Elasticsearch 便捷的操作后，下一步一定会碰到的问题是：数据写入变慢了，机器变卡了，是需要做优化呢？还是需要扩容设备了？如果做扩容，索引的分片和副本设置多少才合适？如果做优化，某个参数能造成什么样的影响？
而 ES 集群性能，受服务器硬件、数据结构和长度、请求接口复杂度等各种环节影响颇大。这些问题，都需要有一个标准的测试流程给出答案。
由于 ES 是近乎线性扩展的分布式系统，所以对上述需求我们都可以总结成同一个测试模式：
使用和线上集群相同硬件配置的服务器搭建一个单节点集群。
使用和线上集群相同的映射创建一个 0 副本，1 分片的测试索引。
使用和线上集群相同的数据写入进行压测。
观察写入性能，或者运行查询请求观察搜索聚合性能。
持续压测数小时，使用监控系统记录 eps、requesttime、fielddata cache、GC count 等关键数据。
测试完成后，根据监控系统数据，确定单分片的性能拐点，或者适合自己预期值的临界点。这个数据，就是一个基准数据。之后的扩容计划，都可以以这个基准单位进行。
需要注意的是，测试是以分片为单位的，在实际使用中，因为主分片和副本分片都是在各自节点做 indexing 和 merge 操作，需要消耗同样的写入性能。所以，实际集群的容量预估中，要考虑副本数的影响。也就是说，假如你在基准测试中得到单机写入性能在 10000 eps，那么开启一个副本后所能达到的 eps 就只有 5000 了。还想写入 10000 eps 的话，就需要加一倍机器。
另外，测试中我们使用的配置都尽量贴合当前现状。事实上，很多配置可能其实并不合理。在确定基准线并开始扩容之前，还是要认真调节配置，审核请求使用的接口是否最优，然后反复测试。然后取一个最终的基准值。
审核请求，更是一个长期的过程，就像 DBA 永远需要关注慢查询一样。ES 的慢查询请求处理，请阅读稍后性能日志一节。+




多集群连接

当你的 ES 集群发展到一定规模，单集群不足以应对庞大的在线索引量级，或者由于业务隔离需求，都有可能划分成多个集群。这时候，另一个问题就出来了：可能其中有一部分数据，被分割在两个集群里，但是还是需要一起使用的。如果是自己写程序，当然可以初始化两个对象，分别连接两个集群，得到结果集后再自行合并。但是如果用 ELK Stack 的，Kibana 可不支持同时连接两个集群地址，这时候，就要用到 ES 中一个特殊的角色：tribe 节点。
tribe 节点只需要提供集群自动发现方面的配置，连接上多个集群后，对外提供只读功能。elasticsearch.yml 配置示例如下：
tribe:
    1002:
        cluster.name: es1002
        discovery.zen.ping.timeout: 100s
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.19.0.22","10.19.0.24",10.19.0.21"]
    1003:
        cluster.name: es1003
        discovery.zen.ping.timeout: 100s
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.19.0.97","10.19.0.98","10.19.0.99","10.19.0.100"]
    blocks:
        write:    true
        metadata: true
    on_conflict: prefer_1003
注意这里的 on_conflict 设置，当多个集群内，索引名称有冲突的时候，tribe 节点默认会把请求轮询转发到各个集群上，这显然是不可以的。所以可以设置一个优先级，在索引名冲突的时候，偏向于转发给某一个集群。
以 tribe 配置启动的 Elasticsearch 服务，其日志输入如下：
[2015-06-18 18:05:51,983][INFO ][node                     ] [Manslaughter] version[1.5.1], pid[12846], build[5e38401/2015-04-09T13:41:35Z]
[2015-06-18 18:05:51,984][INFO ][node                     ] [Manslaughter] initializing ...
[2015-06-18 18:05:51,990][INFO ][plugins                  ] [Manslaughter] loaded [], sites []
[2015-06-18 18:05:54,891][INFO ][node                     ] [Manslaughter/1003] version[1.5.1], pid[12846], build[5e38401/2015-04-09T13:41:35Z]
[2015-06-18 18:05:54,891][INFO ][node                     ] [Manslaughter/1003] initializing ...
[2015-06-18 18:05:54,891][INFO ][plugins                  ] [Manslaughter/1003] loaded [], sites []
[2015-06-18 18:05:55,654][INFO ][node                     ] [Manslaughter/1003] initialized
[2015-06-18 18:05:55,655][INFO ][node                     ] [Manslaughter/1002] version[1.5.1], pid[12846], build[5e38401/2015-04-09T13:41:35Z]
[2015-06-18 18:05:55,655][INFO ][node                     ] [Manslaughter/1002] initializing ...
[2015-06-18 18:05:55,656][INFO ][plugins                  ] [Manslaughter/1002] loaded [], sites []
[2015-06-18 18:05:56,275][INFO ][node                     ] [Manslaughter/1002] initialized
[2015-06-18 18:05:56,285][INFO ][node                     ] [Manslaughter] initialized
[2015-06-18 18:05:56,286][INFO ][node                     ] [Manslaughter] starting ...
[2015-06-18 18:05:56,486][INFO ][transport                ] [Manslaughter] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/10.19.0.100:9301]}
[2015-06-18 18:05:56,499][INFO ][discovery                ] [Manslaughter] elasticsearch/Oewo-L2fR3y2xsgpsoI4Og
[2015-06-18 18:05:56,499][WARN ][discovery                ] [Manslaughter] waited for 0s and no initial state was set by the discovery
[2015-06-18 18:05:56,529][INFO ][http                     ] [Manslaughter] bound_address {inet[/0:0:0:0:0:0:0:0:9201]}, publish_address {inet[/10.19.0.100:9201]}
[2015-06-18 18:05:56,530][INFO ][node                     ] [Manslaughter/1003] starting ...
[2015-06-18 18:05:56,603][INFO ][transport                ] [Manslaughter/1003] bound_address {inet[/0:0:0:0:0:0:0:0:9302]}, publish_address {inet[/10.19.0.100:9302]}
[2015-06-18 18:05:56,609][INFO ][discovery                ] [Manslaughter/1003] es1003/m1-cDaFTSoqqyC2iiQhECA
[2015-06-18 18:06:26,610][WARN ][discovery                ] [Manslaughter/1003] waited for 30s and no initial state was set by the discovery
[2015-06-18 18:06:26,610][INFO ][node                     ] [Manslaughter/1003] started
[2015-06-18 18:06:26,611][INFO ][node                     ] [Manslaughter/1002] starting ...
[2015-06-18 18:06:26,674][INFO ][transport                ] [Manslaughter/1002] bound_address {inet[/0:0:0:0:0:0:0:0:9303]}, publish_address {inet[/10.19.0.100:9303]}
[2015-06-18 18:06:26,676][INFO ][discovery                ] [Manslaughter/1002] es1002/4FPiRPh7TFyBk-BaPc_TLg
[2015-06-18 18:06:56,676][WARN ][discovery                ] [Manslaughter/1002] waited for 30s and no initial state was set by the discovery
[2015-06-18 18:06:56,677][INFO ][node                     ] [Manslaughter/1002] started
[2015-06-18 18:06:56,677][INFO ][node                     ] [Manslaughter] started
[2015-06-18 18:07:37,266][INFO ][cluster.service          ] [Manslaughter/1003] detected_master [10.19.0.97][jnA-rt2fS_22Mz9nYl5Ueg][localhost.localdomain][inet[/10.19.0.97:9300]]{max_local_storage_nodes=1, data=false, master=true}, added {[10.19.0.73][_S8ylz1OTv6Nyp1YoMRNGQ][esnode073.mweibo.bx.sinanode.com][inet[/10.19.0.73:9300]]{max_local_storage_nodes=1, master=false},}, reason: zen-disco-receive(from master [[10.19.0.97][jnA-rt2fS_22Mz9nYl5Ueg][localhost.localdomain][inet[/10.19.0.97:9300]]{max_local_storage_nodes=1, data=false, master=true}])
[2015-06-18 18:07:37,382][INFO ][tribe                    ] [Manslaughter] [1003] adding node [[10.19.0.73][_S8ylz1OTv6Nyp1YoMRNGQ][esnode073.mweibo.bx.sinanode.com][inet[/10.19.0.73:9300]]{max_local_storage_nodes=1, tribe.name=1003, master=false}]
[2015-06-18 18:07:37,391][INFO ][tribe                    ] [Manslaughter] [1003] adding node [[Manslaughter/1003][m1-cDaFTSoqqyC2iiQhECA][localhost.localdomain][inet[/10.19.0.100:9302]]{data=false, tribe.name=1003, client=true}]
[2015-06-18 18:07:37,393][INFO ][tribe                    ] [Manslaughter] [1003] adding node [[10.19.0.97][_mIrWKzZTYifp1xshngBew][esnode054.mweibo.bx.sinanode.com][inet[/10.19.0.54:9300]]{max_local_storage_nodes=1, tribe.name=1003, master=false}]
[2015-06-18 18:07:37,393][INFO ][tribe                    ] [Manslaughter] [1003] adding index [logstash-mweibo-vip-2015.06.15]
[2015-06-18 18:07:37,394][INFO ][tribe                    ] [Manslaughter] [1003] adding index [logstash-php-2015.06.08]
[2015-06-18 18:07:37,394][INFO ][tribe                    ] [Manslaughter] [1003] adding index [logstash-mweibo-vip-2015.06.16]
[2015-06-18 18:07:37,395][INFO ][tribe                    ] [Manslaughter] [1003] adding index [.kibana]
[2015-06-18 18:07:37,398][INFO ][tribe                    ] [Manslaughter] [1003] adding index [logstash-php-2015.06.14]
[2015-06-18 18:07:37,403][INFO ][tribe                    ] [Manslaughter] [1003] adding index [logstash-mweibo-vip-2015.06.10]
[2015-06-18 18:07:37,403][INFO ][tribe                    ] [Manslaughter] [1003] adding index [kibana-int]
[2015-06-18 18:07:37,404][INFO ][tribe                    ] [Manslaughter] [1003] adding index [logstash-mweibo-2015.06.13]
[2015-06-18 18:07:37,411][INFO ][cluster.service          ] [Manslaughter] added {[10.19.0.73][_S8ylz1OTv6Nyp1YoMRNGQ][esnode073.mweibo.bx.sinanode.com][inet[/10.19.0.73:9300]]{max_local_storage_nodes=1, tribe.name=1003, master=false},[10.19.0.97][jnA-rt2fS_22Mz9nYl5Ueg][localhost.localdomain][inet[/10.19.0.97:9300]]{max_local_storage_nodes=1, tribe.name=1003, data=false, master=true},}, reason: cluster event from 1003, zen-disco-receive(from master [[10.19.0.97][jnA-rt2fS_22Mz9nYl5Ueg][localhost.localdomain][inet[/10.19.0.97:9300]]{max_local_storage_nodes=1, data=false, master=true}])
[2015-06-18 18:08:07,316][INFO ][cluster.service          ] [Manslaughter/1002] detected_master [10.19.0.22][6qyQh9EURUyO7RBC_dXDow][localhost.localdomain][inet[/10.19.0.22:9300]]{max_local_storage_nodes=1, master=true}, added {[10.19.0.93][qAklY08iSsSfIf2vvu6Iyw][localhost.localdomain][inet[/10.19.0.93:9300]]{max_local_storage_nodes=1, master=false}])
[2015-06-18 18:08:07,350][INFO ][indices.breaker          ] [Manslaughter/1002] Updating settings parent: [PARENT,type=PARENT,limit=259489792/247.4mb,overhead=1.0], fielddata: [FIELDDATA,type=MEMORY,limit=155693875/148.4mb,overhead=1.03], request: [REQUEST,type=MEMORY,limit=103795916/98.9mb,overhead=1.0]
[2015-06-18 18:08:07,353][INFO ][tribe                    ] [Manslaughter] [1002] adding node [[10.19.0.93][qAklY08iSsSfIf2vvu6Iyw][localhost.localdomain][inet[/10.19.0.93:9300]]{max_local_storage_nodes=1, tribe.name=1002, master=false}]
[2015-06-18 18:08:07,357][INFO ][tribe                    ] [Manslaughter] [1002] adding node [[Manslaughter/1002][4FPiRPh7TFyBk-BaPc_TLg][localhost.localdomain][inet[/10.19.0.100:9303]]{data=false, tribe.name=1002, client=true}]
[2015-06-18 18:08:07,358][INFO ][tribe                    ] [Manslaughter] [1002] adding node [[10.19.0.22][tkrBsbnLTry0zzZEdbQR0A][localhost.localdomain][inet[/10.19.0.27:9300]]{max_local_storage_nodes=1, tribe.name=1002, master=false}]
[2015-06-18 18:08:07,358][INFO ][tribe                    ] [Manslaughter] [1002] adding index [test.yingju1-mweibo_client_downstream_success-2015.06.07]
[2015-06-18 18:08:07,363][INFO ][tribe                    ] [Manslaughter] [1002] adding index [logstash-mweibo_client_downstream_error-2015.06.02]
[2015-06-18 18:08:07,366][INFO ][tribe                    ] [Manslaughter] [1002] adding index [.kibana_5601]
[2015-06-18 18:08:07,377][INFO ][cluster.service          ] [Manslaughter] added {[10.19.0.22][6qyQh9EURUyO7RBC_dXDow][localhost.localdomain][inet[/10.19.0.22:9300]]{max_local_storage_nodes=1, tribe.name=1002, master=false},[10.19.0.93][l7nkk-H7S6GvMzWwGe0_CA][localhost.localdomain][inet[/10.19.0.93:9300]]{max_local_storage_nodes=1, tribe.name=1002, master=false},}, reason: cluster event from 1002, zen-disco-receive(from master [[10.19.0.22][6qyQh9EURUyO7RBC_dXDow][localhost.localdomain][inet[/10.19.0.22:9300]]{max_local_storage_nodes=1, master=true}])
[2015-06-18 18:08:13,208][DEBUG][discovery.zen.publish    ] [Manslaughter/1003] received cluster state version 782404
[2015-06-18 18:08:21,803][DEBUG][discovery.zen.publish    ] [Manslaughter/1003] received cluster state version 782405
[2015-06-18 18:08:33,229][DEBUG][discovery.zen.publish    ] [Manslaughter/1003] received cluster state version 782406
日志中可以明显看到，节点是如何分别连接上两个集群的。
最后，我们可以使用标准的 RESTful 接口来验证一下：
# curl 10.19.0.100:9201/_cat/indices?v
health status index                                                    pri rep docs.count docs.deleted store.size pri.store.size
green  open   test.yingju1-mweibo_client_downstream_success-2015.06.07  20   1   40692459            0    154.1gb           77gb
green  open   weibo-client-video-2015.06.19                              5   1          0            0       970b           575b
green  open   dpool-pc-weibo-2015.06.19                                 20   1          0            0      3.7kb          2.2kb
green  open   logstash-video-2015.06.16                                 27   0  149015413            0     13.4gb         13.4gb
不同集群的索引，都可以通过 tribe node 访问到了。+





alias的几点应用

本节作者：childe
索引更改名字时, 无缝过渡

情景1

用Logstash采集当前的所有nginx日志, 放入ES, 索引名叫nginx-YYYY.MM.DD.
后来又增加了apache日志, 希望能放在同一个索引里面,统一叫web-YYYY.MM.DD.
我们只要把Logstash配置更改一下,然后重启, 数据就会写入新的索引名字下. 但是同一天的索引就会被分成了2个, kibana上面就不好配置了.
如此实现

今天是2015.07.28. 我们为nginx-2015.07.28建一个alias叫做web-2015.07.28, 之前的所有nginx日志也如此照做.
kibana中把dashboard配置的索引名改成web-YYYY.MM.DD
将logstash里面的elasticsearch的配置改成web-YYYY.MM.DD, 重启.
无缝切换实现.







映射与模板的定制

Elasticsearch 是一个 schema-less 的系统，但 schema-less 并不代表 no schema，而是 ES 会尽量根据 JSON 源数据的基础类型猜测你想要的字段类型映射。如果你对这种动态生成的映射关系不满意，或者想要使用一些更高级的映射设置，那么就需要使用自定义映射。
创建和更新映射

正如上面所说，ES 可以随时根据数据中的新字段来创建新的映射关系。所以，我们也可以自己在还没有正式数据写入之前，先创建一个基础的映射。等后续数据有其他字段时，ES 也一样会自动处理。
映射的的创建方式如下：
# curl -XPUT http://127.0.0.1:9200/logstash-2015.06.20/_mapping -d '
{
  "mappings": {
    "syslog" : {
      "properties" : {
        "@timestamp" : {
          "type" : "date"
        },
        "message" : {
          "type" : "string"
        },
        "pid" : {
          "type" : "long"
        }
      }
    }
  }
}'
注意：对于已存在的映射，ES 的自动处理仅限于新字段出现。已经生成的字段映射，是不可变更的。如果确实需要，请参阅之前的 reindex 接口小节，采用重新导入数据的方式完成。
而如果是新增一个字段映射的更新，那还是可以通过 /_mapping 接口直接完成的：
# curl -XPUT http://127.0.0.1:9200/logstash-2015.06.21/_mapping/syslog -d '
{
  "properties" : {
    "syslogtag" : {
      "type" :    "string",
      "index":    "not_analyzed"
    }
  }
}'
没错，这里只需要单独写这个新字段的内容就够了。ES 会自动合并进去。
删除映射

删除数据并不代表会删除数据的映射。比如：
# curl -XDELETE http://127.0.0.1:9200/logstash-2015.06.21/syslog
删除了索引下 syslog 的全部数据，但是 syslog 的映射还在。删除映射(同时也就删掉了数据)的命令是：
# curl -XDELETE http://127.0.0.1:9200/logstash-2015.06.21/_mapping/syslog
当然，如果删除整个索引，那映射也是同时被清除的。
核心类型

mapping 中主要就是针对字段设置类型以及类型相关参数。那么，我们首先来了解一下 Elasticsearch 支持的核心类型：
JSON 基础类型
字符串: string
数字: byte, short, integer, long, float, double
时间: date
布尔值: true, false
数组: array
对象: object
ES 独有类型
多重: multi
经纬度: geo_point
网络地址: ip
堆叠对象: nested object
二进制: binary
附件: attachment
前面提到，ES 是根据收到的 JSON 数据里的类型来猜测的。所以，一个内容为 "123" 的数据，猜测出来的类型应该是 string 而不是 long。除非这个字段已经有了确定为 long 的映射关系，那么 ES 会尝试做一次转换。如果转换失败，这条数据写入就会报错。
注意：

ES 的映射虽然有 index 和 type 两层关系，但是实际索引时是以 index 为基础的。如果同一个 index 下不同 type 的字段出现 mapping 不一致的情况，虽然数据依然可以成功写入并生成各自的 mapping，但实际 fielddata 中的索引结果却依然是以 index 内第一个 mapping 类型来生成的。这种情况下可能会有比较奇怪的事情发生。比如：
看似 double 的数据实际存储成 long，导致数值比较的搜索结果异常。
同样的字段，一部分采用标准分词器，一部分采用中文分词器，导致索引查询异常。
从 Kibana4 开始，会在 Object Setting 页对该情况做出冲突预警；并从 ES 2.0 版本开始正式拒绝这种冲突数据写入。
查看已有数据的映射

学习索引映射最直接的方式，就是查看已有数据索引的映射。我们用 logstash 写入 ES 的数据，都会根据 logstash 自带的 template，生成一个很有学习意义的映射：
# curl -XGET http://127.0.0.1:9200/logstash-2015.06.16/_mapping/tweet
{
   "gb": {
      "mappings": {
         "tweet": {
            "properties": {
               "date": {
                  "type": "date",
                  "format": "dateOptionalTime"
               },
               "name": {
                  "type": "string"
               },
               "tweet": {
                  "type": "string"
               },
               "user_id": {
                  "type": "long"
               }
            }
         }
      }
   }
}
自定义字段映射

大家可以通过上面一个现存的映射发现其实所有的字段都有好几个属性，这些都是我们可以自己定义修改的。除了已经看到的这些基本内容外，ES 还支持其他一些可能会比较常用的映射属性：
全文索引还是精确索引
自定义分词器
自定义日期格式
精确索引

字段都有几个基本的映射选项，类型(type)和索引方式(index)。以字符串类型为例，index 有三个可设置项：
analyzed 默认选项，以标准的全文索引方式，分析字符串，完成索引。
not_analyzed 精确索引，不对字符串做分析，直接索引字段内数据的精确内容。
no 不索引该字段。
对于日志应用来说，很多字段都是不需要在 Elasticsearch 里做解析这步的，所以，我们可以设置：
    "myfieldname": {
        "type":     "string",
        "index":    "not_analyzed"
    }
时间格式

稍微见过 ELK Stack 示例的人，都对其中 @timestamp 字段的特殊格式有深刻的印象。这个时间格式在 Nginx 中叫 $time_iso8601，在 Rsyslog 中叫 date-rfc3339，在 ES 中叫 dateOptionalTime。但事实上，ES 完全可以接收其他时间格式作为时间字段的内容。对于 ES 来说，时间字段内容实际都是转换成 long 类型作为内部存储的。所以，接收段的时间格式，可以任意配置：
"@timestamp" : {
    "type" : "date"
    "index" : "not_analyzed",
    "doc_values" : true,
    "format" : "dd/MMM/YYYY:HH:mm:ss Z",
}
而 ES 默认的时间字段格式，除了 dateOptionalTime 以外，还有一种，就是 epoch_millis，毫秒级的 UNIX 时间戳。因为这个数值 ES 可以直接毫不修改的存成内部实际的 long 数值。此外，从 ES 2.0 开始，新增了对秒级 UNIX 时间戳的支持，其 format 定义为：epoch_second。
注意：在 ES 2.x 中，同名 date 字段的 format 也必须保持一致。
多重索引

多重索引是 logstash 用户最习惯的一个映射，因为这是 logstash 默认设置开启的配置：
"title": {
    "type": "string",
    "fields": {
        "raw": { "type": "string", "index": "not_analyzed" }
    }
}
其作用是，在 title 字段数据写入的时候，ES 会自动生成两个字段，分别是 title 和 title.raw。这样，在可能同时需要分词与不分词结果的环境下，就可以很灵活的使用不同的索引字段了。比如，查看标题中最常用的单词，应该使用 title 字段；查看阅读数最多的文章标题，应该使用 title.raw 字段。
注意：raw 这个名字你可以自己随意取。比如说，如果你绝大多数时候用的是精确索引，那么你完全可以为了方便反过来定义：
"title": {
    "type": "string",
    "index": "not_analyzed",
    "fields": {
        "alz": { "type": "string" }
    }
}
特殊字段

上面介绍的，都是对普通数据字段的一些常用设置。而实际上，ES 默认还有一些特殊字段，在默默的发挥着作用。这些字段，统一以 _ 下划线开头。在之前 CRUD 章节中，我们就已经看到一些，比如 _index，_type，_id。默认不开启的还有 _ttl，_timestamp，_size，_parent 等。这里需要单独介绍两个，对我们索引和检索的效果和性能，都有较大影响的：
_all

_all 里存储了各字段的数据内容。其作用是，在检索的时候，如果无法或者未指明具体搜索哪个字段的数据，那么 ES 默认就会是从 _all 里去查找。
对于日志场景，如果你的日志划分出来的字段比较少且数目固定。那么，完全可以关闭掉 _all 功能，节省这部分 IO 和 CPU。
"_all" : {
    "enabled" : false
}
_source

_source 里存储了该条记录的 JSON 源数据内容。这部分内容只是按照 ES 接收到的内容原样存储下来，并不经过索引过程。对于 ES 的请求过程来说，它不参与 Query 阶段，而只用于 Fetch 阶段。我们在 GET 或者 /_search 时看到的数据内容，都是从 _source 里获取到的。
所以，虽然 _source 也重复了一遍索引中的数据，一般我们并不建议关闭这个功能。因为一旦关闭，你搜索的结果除了一个 _id，啥都看不到。对于日志场景，意义不是很大。
当然，也有少数场景是可以关闭 _source 的：
把 ES 作为时间序列数据库使用，只要聚合统计结果，不要源数据内容。
把 ES 作为纯检索工具使用，_id 对应的内容在 HDFS 上另外存储，搜索后使用所得 _id 去 HDFS 上读取内容。
动态模板映射

不想使用默认识别的结果，单独设置一个字段的映射的方法，上面已经介绍完毕。那么，如果你有一类相似的数据字段，想要统一设置其映射，就可以用到下一项功能：动态模板映射(dynamic_templates)。
    "_default_" : {
      "dynamic_templates" : [ {
        "message_field" : {
          "mapping" : {
            "index" : "analyzed",
            "omit_norms" : true,
            "store" : false,
            "type" : "string"
          },
          "match" : "*msg",
          "match_mapping_type" : "string"
        }
      }, {
        "string_fields" : {
          "mapping" : {
            "index" : "not_analyzed",
            "ignore_above" : 256,
            "store" : false,
            "doc_values" : true,
            "type" : "string"
          },
          "match" : "*",
          "match_mapping_type" : "string"
        }
      } ],
      "properties" : {
      }
    }
这样，只要字符串类型字段名以 msg 结尾的，都会经过全文索引，其他字符串字段则进行精确索引。同理，还可以继续书写其他类型(long, float, date 等)的 match_mapping_type 和 match。
索引模板

对每个希望自定义映射的索引，都要定时提前通过发送 PUT 请求的方式创建索引的话，未免太过麻烦。ES 对此设计了索引模板功能。我们可以针对同一类索引，定制相同的模板。
模板中的内容包括两大类，setting(设置)和 mapping(映射)。setting 部分，多为在 elasticsearch.yml 中可以设置全局配置的部分，而 mapping 部分，则是这节之前介绍的内容。
如下为定义所有以 te 开头的索引的模板：
# curl -XPUT http://localhost:9200/_template/template_1 -d '
{
    "template" : "te*",
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "_source" : { "enabled" : false }
        }
    }
}'
同时，索引模板是有序合并的。如果我们在同一类索引里，又想单独修改某一小类索引的一两处单独设置，可以再累加一层模板：
# curl -XPUT http://localhost:9200/_template/template_2 -d '
{
    "order" : 1,
    "template" : "tete*",
    "settings" : {
        "number_of_shards" : 2
    },
    "mappings" : {
        "type1" : {
            "_all" : { "enabled" : false }
        }
    }
}'
默认的 order 是 0，那么新创建的 order 为 1 的 template_2 在合并时优先级大于 template_1。最终，对 tete*/type1 的索引模板效果相当于：
{
    "settings" : {
        "number_of_shards" : 2
    },
    "mappings" : {
        "type1" : {
            "_source" : { "enabled" : false },
            "_all" : { "enabled" : false }
        }
    }
}







Puppet 自动部署 Elasticsearch

Elasticsearch 作为一个 Java 应用，本身的部署已经非常简单了。不过作为生产环境，还是有必要采用一些更标准化的方式进行集群的管理。Elasticsearch 官方提供并推荐使用 Puppet 方式部署和管理。其 Puppet 模块源码地址见：
https://github.com/elastic/puppet-elasticsearch
安装方法

和其他标准 Puppet Module 一样，puppet-elasticsearch 也可以通过 Puppet Forge 直接安装：
# puppet module install elasticsearch-elasticsearch
配置示例

安装好 Puppet 模块后，就可以使用了。模块提供几种 Puppet 资源，主要用法如下：
class { 'elasticsearch':
  version => '1.5.2',
  config => { 'cluster.name' => 'es1003' },
  java_install => true,
}
elasticsearch::instance { $fqdn:
  config => { 'node.name' => $fqdn },
  init_defaults => { 'ES_USER' => 'elasticsearch', 'ES_HEAP_SIZE' => $memorysize > 64 ? '31g' : $memorysize / 2 },
  datadir => [ '/data1/elasticsearch' ],
}
elasticsearch::template { 'templatename':
  host => $::ipaddress,
  port => 9200,
  content => '{"template":"*","settings":{"number_of_replicas":0}}'
}
示例中展示了三种资源：
class: 配置具体安装的 Elasticsearch 软件版本，全集群公用的一些基础配置项。注意，puppet-elasticsearch 模块默认并不负责 Java 的安装，它只是调用操作系统对应的 Yum，Apt 工具，而 elasticsearch.rpm 或者 elasticsearch.deb 本身没有定义其他依赖(因为 java 版本太多了，定义起来不方便)。所以，如果依然要走 puppet-elasticsearch 配置来安装 Java 的话，需要额外开启 java_install 选项。该选项会使用另一个 Puppet Module —— puppetlabs-java 来安装，默认安装的是 jdk。如果你要节省空间，可以再加一行 java_package 来明确指定软件全名。
instance: 配置具体单个进程实例的配置。其中 config 和 init_defaults 选项在 class 和 instance 资源中都可以定义，实际运行时，会自动做一次合并，当然，instance 里的配置优先级高于 class 中的配置。此外，最重要的定义是数据目录的位置。有多快磁盘的，可以在这里定义一个数组。
template: 模板是 Elasticsearch 创建索引映射和设置时的预定义方式。一般可以通过在 config/templates/ 目录下放置 JSON 文件，或者通过 RESTful API 上传配置两种方式管理。而这里，单独提供了 template 资源，通过 puppet 来管理模板。content 选项中直接填入模板内容，或者使用 file 选项读取文件均可。
事实上，模块还提供了 plugin 和 script 资源管理这两方面的内容。考虑在 ELK 中，二者用的不是很多，本节就不单独介绍了。想了解的读者可以参考官方文档。





集群版本升级

Elasticsearch 作为一个新兴项目，版本更新非常快。而且每次版本更新都或多或少带有一些重要的性能优化、稳定性提升等特性。可以说，ES 集群的版本升级，是目前 ES 运维必然要做的一项工作。
按照 ES 官方设计，有 restart upgrade 和 rolling upgrade 两种可选的升级方式。对于 1.0 版本以上的用户，推荐采用 rolling upgreade 方式。
但是，对于主要负载是数据写入的 ELK Stack 场景来说，却并不是这样！
rolling upgrade 的步骤大致如下：
暂停分片分配；
单节点下线升级重启；
开启分片分配；
等待集群状态变绿后继续上述步骤。
实际运行中，步骤 2 的 ES 单节点从 restart 到加入集群，大概要 100s 左右的时间。也就是说，这 100s 内，该节点上的所有分片都是 unassigned 状态。而按照 Elasticsearch 的设计，数据写入需要至少达到 replica/2+1 个分片完成才能算完成。也就意味着你所有索引都必须至少有 1 个以上副本分片开启。
但事实上，很多日志场景，由于写入性能上的要求要高于数据可靠性的要求，大家普遍减小了副本数量，甚至直接关掉副本复制。这样一来，整个 rolling upgrade 期间，数据写入就会受到严重影响，完全丧失了 rolling 的必要性。
其次，步骤 3 中的 ES 分片均衡过程中，由于 ES 的副本分片数据都需要从主分片走网络复制重新传输一次，而由于重启，新升级的节点上的分片肯定全是副本分片(除非压根没副本)。在数据量较大的情况下，这个步骤耗时可能是几十分钟甚至以小时计。而且并发和限速上稍微不注意，可能导致分片均衡的带宽直接占满网卡，正常写入也还是受到影响。
所以，对于写入压力较大，数据可靠性要求偏低的实时日志场景，依然建议大家进行主动停机式的 restart upgrade。
restart upgrade 的步骤如下：
首先适当加大集群的数据恢复和分片均衡并发度以及磁盘限速：
# curl -XPUT http://127.0.0.1:9200/_cluster/settings -d '{
  "persistent" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "disable_allocation" : "false",
          "cluster_concurrent_rebalance" : "5",
          "node_concurrent_recoveries" : "5",
          "enable" : "all"
        }
      }
    },
    "indices" : {
      "recovery" : {
        "concurrent_streams" : "30",
        "max_bytes_per_sec" : "2gb"
      }
    }
  },
  "transient" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "enable" : "all"
        }
      }
    }
  }
}'
暂停分片分配：
# curl -XPUT http://127.0.0.1:9200/_cluster/settings -d '{
  "transient" : {
    "cluster.routing.allocation.enable" : "none"
  }
}'
通过配置管理工具下发新版本软件包。
公告周知后，停止数据写入进程(即 logstash indexer 等)
如果使用 Elasticsearch 1.6 版本以上，可以手动运行一次 synced flush，同步副本分片的 commit id，缩小恢复时的网络传输带宽：
# curl -XPOST http://127.0.0.1:9200/_flush/synced
全集群统一停止进程，更新软件包，重新启动。
等待各节点都加入到集群以后，恢复分片分配：
# curl -XPUT http://127.0.0.1:9200/_cluster/settings -d '{
  "transient" : {
    "cluster.routing.allocation.enable" : "all"
  }
}'
由于同时启停，主分片几乎可以同时本地恢复，整个集群从 red 变成 yellow 只需要 2 分钟左右。而后的副本分片，如果有 synced flush，同样本地恢复，否则网络恢复总耗时，视数据大小而定，会明显大于单节点恢复的耗时。
如果有 synced flush，建议等待集群变成 green 状态后，恢复写入；否则在集群变成 yellow 状态之后，即可着手开始恢复数据写入进程。




镜像备份

本节作者：李宏旭
大多数公司在使用 Elasticsearch 之前，都已经维护有一套 Hadoop 系统。因此，在实时数据慢慢变得冷却，不再被经常使用的时候，一个需求自然而然的就出现了：怎么把 Elasticsearch 索引数据快速转移到 HDFS 上，以解决 Elasticsearch 上的磁盘空间；而在我们需要的时候，又可以较快的从 HDFS 上把索引恢复回来继续使用呢？
Elasticsearch 为此提供了 snapshot 接口。通过这个接口，我们可以快速导入导出索引镜像到本地磁盘，网络磁盘，当然也包括 HDFS。
HDFS 插件安装配置

下载repository-hdfs插件，通过标准的 elasticsearch plugin 安装命令安装：
bin/plugin install elasticsearch/elasticsearch-repository-hdfs/2.2.0
然后在 elasticsearch.yml 中增加以下配置：
# repository 配置
hdfs:
    uri:"hdfs://<host>:<port>"(默认port为8020)
    #Hadoop file-system URI
    path:"some/path"
    #path with the file-system where data is stored/loaded
    conf.hdfs_config:"/hadoop/hadoop-2.5.2/etc/hadoop/hdfs-site.xml"
    conf.hadoop_config:"/hadoop/hadoop-2.5.2/etc/hadoop/core-site.xml"
    load_defaults:"true"
    #whether to load the default Hadoop configuration (default) or not
    compress:"false"
    # optional - whether to compress the metadata or not (default)
    chunk_size:"10mb"
    # optional - chunk size (disabled by default)
# 禁用 jsm
security.manager.enabled: false
默认情况下，Elasticsearch 为了安全考虑会在运行 JVM 的时候执行 JSM。出于 Hadoop 和 HDFS 客户端权限问题，所以需要禁用 JSM。将 elasticsearch.yml 中的 security.manager.enabled 设置为 false。
将插件安装好，配置修改完毕后，需要重启 Elasticsearch 服务。没有重启节点插件可能会执行失败。
注意：Elasticsearch 集群的每个节点都要执行以上步骤！
Hadoop 配置

本节内容基于Hadoop版本：2.5.2，假定其配置文件目录：hadoop-2.5.2/etc/Hadoop。注意，安装hadoop集群需要建立主机互信，互信方法请自行查询，很简单。
相关配置文件如下：
mapred-site.xml.template

默认没有 mapred-site.xml 文件，复制 mapred-site.xml.template 一份，并把名字改为 mapred-site.xml，需要修改 3 处的 IP 为本机地址:
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobtracker.http.address</name>
        <value>XX.XX.XX.XX:50030</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value> XX.XX.XX.XX:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value> XX.XX.XX.XX:19888</value>
    </property>
</configuration>
yarn-site.xml

需要修改5处的IP为本机地址:
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
        <property>
        <name>yarn.resourcemanager.address</name>
        <value> XX.XX.XX.XX:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value> XX.XX.XX.XX:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value> XX.XX.XX.XX:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value> XX.XX.XX.XX:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value> XX.XX.XX.XX:8088</value>
    </property>
</configuration>
hadoop-env.sh

修改 jdk 路径和 jvm 内存配置，内存使用根据情况配置。
export JAVA_HOME=/usr/java/jdk1.7.0_79
export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"
export HADOOP_CLIENT_OPTS="-Xmx512m $HADOOP_CLIENT_OPTS"
core-site.xml

临时目录及 hdfs 机器 IP 端口指定: hadoop.tmp.dir /soft/hadoop-2.5.2/tmp Abase for other temporary directories. fs.defaultFS hdfs:// XX.XX.XX.XX:9000 io.file.buffer.size 4096
slaves

配置集群 IP 地址，集群有几个 IP 都要配置进去:
192.168.0.2
192.168.0.3
192.168.0.4
hdfs-site.xml

namenode 和 datenode 数据存放路径及别名:
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/data01/hadoop/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/data01/hadoop/data</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
启动 Hadoop

格式化完成后也可使用 sbin/start-all.sh 启动，但有可能出现异常，建议按照顺序分开启动。
首先需要格式化存储： bin/Hadoop namenode –format
启动start-dfs.sh sbin/start-dfs.sh
启动start-yarn.sh sbin/start-yarn.sh
备份导出

创建快照仓库

curl -XPUT 'localhost:9200/_snapshot/backup' -d 
'{
    "type":"hdfs",
    "settings":{
        "path":"/test/repo",
        "uri":"hdfs://<uri>:<port>"
    }
}'
在这步可能会报错。通常是因为 hadoop 配置问题，更改好配置需要重新格式化文件系统:
在 hadoop 目录下执行 bin/hadoop namenode -format
索引快照

执行索引快照命令，可写入crontab，定时执行
curl -XPUT 'http://localhost:9200/_snapshot/backup/snapshot_1' -d
 '{"indices":"indices_01,indices_02"}'
备份恢复

curl -XPOST "localhost:9200/_snapshot/backup/snapshot_1/_restore"
备份删除

curl -XDELETE "localhost:9200/_snapshot/backup/snapshot_1"
查看仓库信息

curl -XGET 'http://localhost:9200/_snapshot/backup?pretty'





spark streaming 交互

Apache Spark 是一个高性能集群计算框架，其中 Spark Streaming 作为实时批处理组件，因为其简单易上手的特性深受喜爱。在 es-hadoop 2.1.0 版本之后，也新增了对 Spark 的支持，使得结合 ES 和 Spark 成为可能。
目前最新版本的 es-hadoop 是 2.1.0-Beta4。安装如下：
wget http://d3kbcqa49mib13.cloudfront.net/spark-1.0.2-bin-cdh4.tgz
wget http://download.elasticsearch.org/hadoop/elasticsearch-hadoop-2.1.0.Beta4.zip
然后通过 ADD_JARS=../elasticsearch-hadoop-2.1.0.Beta4/dist/elasticsearch-spark_2.10-2.1.0.Beta4.jar 环境变量，把对应的 jar 包加入 Spark 的 jar 环境中。
下面是一段使用 spark streaming 接收 kafka 消息队列，然后写入 ES 的配置：
import org.apache.spark._
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql._
import org.elasticsearch.spark.sql._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.Logging
import org.apache.log4j.{Level, Logger}

object Elastic {
  def main(args: Array[String]) {
    val numThreads = 1
    val zookeeperQuorum = "localhost:2181"
    val groupId = "test"
    val topic = Array("test").map((_, numThreads)).toMap
    val elasticResource = "apps/blog"

    val sc = new SparkConf()
                 .setMaster("local[*]")
                 .setAppName("Elastic Search Indexer App")

    sc.set("es.index.auto.create", "true")
    val ssc = new StreamingContext(sc, Seconds(10))
    ssc.checkpoint("checkpoint")
    val logs = KafkaUtils.createStream(ssc,
                                       zookeeperQuorum,
                                       groupId,
                                       topic,
                                       StorageLevel.MEMORY_AND_DISK_SER)
                         .map(_._2)

    logs.foreachRDD { rdd =>
      val sc = rdd.context
      val sqlContext = new SQLContext(sc)
      val log = sqlContext.jsonRDD(rdd)
      log.saveToEs(elasticResource)
    }

    ssc.start()
    ssc.awaitTermination()

  }
}
注意，代码中使用了 spark SQL 提供的 jsonRDD() 方法，如果在对应的 kafka topic 里的数据，本身并不是已经处理好了的 JSON 数据的话，这里还需要自己写一写额外的处理函数，利用 cast class 来规范数据。





Shield 权限管理

本节作者：cameluo
Shield 是 Elastic 公司官方发布的权限管理产品。其主要特性包括：
提供集群节点身份验证和集群数据访问身份验证
提供基于身份角色的细粒度资源和行为访问控制，细到索引级别的读写控制
提供节点间数据传输通道加密保护输出传输安全
提供审计功能
以插件的形式发布
License管理策略

Shield 是一款商业产品，不过提供 30 天免费试用，试用期间是全功能的。过期后集群会不再正常工作。
Shield架构

用户认证：

Shield 通过定义一套用户集合来认证用户，采用抽象的域方式定义用户集合，支持本地用户(esusers 域)和 LDAP 用户(含 AD)。
Shield 提供工具 ./bin/shield/esusers 用于创建和管理本地用户。
集成 LDAP 认证支持映射 LDAP 安全组到 Shield 角色，LDAP 安全组与 Shield 角色可以是多对多的关系。
Shield 支持定义多个认证域，采用order字段进行优先级排序。如一个本地域 esusers，order=1，加一个 LDAP 域，order=2。如果用户不再本地用户域中则在 LDAP 域中查找验证。
./config/shield/roles.yml 文件中定义角色和角色的所拥有的权限。
./config/shield/group_to_role_mapping.yml 文件中定义 LDAP 组到角色映射关系。
节点认证与通道加密：

使用SSL/TLS证书进行相互认证和通讯加密。加密是可选配置。如果不使用，shield 节点之间可以进行简单的密码验证（明文传输）。
授权：

Shield 采用 RBAC 授权模型，数据模型包含如下元素：
受保护资源(Secured Resource)：控制用户访问的客体，包括 cluster、index/alias 等等。
权能(Priviliege)：用户可以对受保护资源执行的一种或多种操作，如 read, write 等。
许可(Permissions)：对被保护的资源拥有的一个或多个权能，如 read on the "products" index。
角色(Role)：命名的一组许可。
用户(Users)：用户实体，可以被赋予 0 个或多种角色，授权他们对被保护的资源执行各种权能。
审计：

增加认证尝试、授权失败等安全相关事件和活动日志。
安装

安装 License 和 Shield 插件
bin/plugin -i elasticsearch/license/latest
bin/plugin -i elasticsearch/shield/latest
注意：初次运行 Shield 需要重新启动 ES 集群。后续更新 License(license.json 为 License 文件)就可以在线运行：
# curl -XPUT -u admin 'http://127.0.0.1:9200/_licenses' -d @license.json
创建本地管理员：
./bin/shield/esusers useradd esadmin -r admin
配置

这里使用简单的配置先完成基本验证：使用纯本地用户认证或者使用本地认证 + 基本的 ldap 认证。
ES 配置

在elasticsearch.yml中增加如下配置：
hostname_verification: false
#shield.ssl.keystore.path:          /app/elasticsearch/node01.jks
#shield.ssl.keystore.password:      xxxxxx
shield:
  authc:
    realms:
      default:
        type: esusers
        order: 1
      ldaprealm:
        type: ldap
        order: 2
        url: "ldap://ldap.example.com:389"
        bind_dn: "uid=ldapuser, ou=users, o=services, dc=example, dc=com"
        bind_password: changeme
        user_search:
          base_dn: "dc=example,dc=com"
          attribute: uid
        group_search:
          base_dn: "dc=example,dc=com"
        files:
          role_mapping: "/app/elasticsearch/shield/group_to_role_mapping.yml"
        unmapped_groups_as_roles: false
角色配置

根据默认配置文件增减角色和访问控制权限。角色配置文件可以在线修改，保存后立即生效：
([INFO ][shield.authz.store ] [Winky Man] updated roles (roles file [/opt/elasticsearch/config/shield/roles.yml] changed))
注意：如果需要集成kibana认证，用户角色也需要有访问'.kibana'索引的访问权限和cluster:monitor/nodes/info的访问权限，具体参照kibana4角色中的定义，否则用户通过kibana认证后仍然无法访问到数据索引
用户组与角色映射配置

根据默认配置文件增减用户、用户组与角色配置中定义角色的映射关系，可以灵活实现各种需求。LDAP组仅支持安全组，不支持动态组。这个配置文件可以在线修改，保存后立即生效：
([INFO ][shield.authc.ldap.support] [Vishanti] role mappings file [/opt/elasticsearch/config/shield/group_to_role_mapping.yml] changed for realm [ldap/ldaprealm]. updating mappings...)
测试

curl -u username http://127.0.0.1:9200/





Searchguard的部署与权限策略配置

本节作者：elain
背景

当前es正在被各大互联网公司大量的使用，但目前安全方面还没有一个很成熟的方案，大部门都没有做安全认证或基于自身场景自己开发，没有一个好的开源方案 es官方推出了shield认证，试用了一番，很是方便，功能强大，文档也较全面，但最大的问题是收费的，我相信中国很多公司都不愿去花钱使用，所以随后在github 中找到了search-guard项目，接下来我们一起来了解并部署此项目到我们的ES环境中。
注：目前此项目只支持到1.6以下的es，1.7 还未支持，所以，我们在ES1.6下来部署此项目
软件版本：

ES 1.6.0
kibana 4.0.2
CentOS 6.3
功能特性：

基于用户与角色的权限控制
支持SSL/TLS方式安全认证
支持LDAP认证
支持最新的kibana4
更多特性见官网介绍
官网地址：http://floragunn.com/searchguard
目标：

实现用户访问es中日志需要登陆授权，不同用户访问不同索引，不授权的索引无法查看，分组控制不同rd查看各自业务的日志，
部署

download maven：

axel -n 10  http://mirror.bit.edu.cn/apache/maven/maven-3/3.3.3/binaries/apache-maven-3.3.3-bin.tar.gz
tar zxvf apache-maven-3.3.3-bin.tar.gz
cd apache-maven-3.3.3/

#git search-guard and build
git clone -b es1.6 https://github.com/floragunncom/search-guard.git
cd search-guard ;/home/work/app/maven/bin/mvn package -DskipTests

#把编译好的包放到一个下载地址(方便于es集群使用，单台测试可不使用这种方案)：
http://www.elain.org/dl/search-guard-16-0.6-SNAPSHOT.zip
在es上以插件方式安装编译好的包

cd /home/work/app/elasticsearch/plugins/
./bin/plugin -u http://www.elain.org/dl/search-guard-16-0.6-SNAPSHOT.zip -i search-guard
elasticsearch.yml 添加

#################search-guard###################
searchguard.enabled: true
searchguard.key_path: /home/work/app/elasticsearch/keys
searchguard.auditlog.enabled: true
searchguard.allow_all_from_loopback: true #本地调试可打开，建议在线上关闭
searchguard.check_for_root: false
searchguard.http.enable_sessions: true

#配置认证方式
searchguard.authentication.authentication_backend.impl: com.floragunn.searchguard.authentication.backend.simple.SettingsBasedAuthenticationBackend
searchguard.authentication.authorizer.impl: com.floragunn.searchguard.authorization.simple.SettingsBasedAuthorizator
searchguard.authentication.http_authenticator.impl: com.floragunn.searchguard.authentication.http.basic.HTTPBasicAuthenticator

#配置用户名和密码
searchguard.authentication.settingsdb.user.admin: admin
searchguard.authentication.settingsdb.user.user1: 123
searchguard.authentication.settingsdb.user.user2: 123

#配置用户角色
searchguard.authentication.authorization.settingsdb.roles.admin: ["root"]
searchguard.authentication.authorization.settingsdb.roles.user1: ["user1"]
searchguard.authentication.authorization.settingsdb.roles.user2: ["user2"]

#配置角色权限（只读）
searchguard.actionrequestfilter.names: ["readonly","deny"]
searchguard.actionrequestfilter.readonly.allowed_actions: ["indices:data/read/*", "indices:admin/exists","indices:admin/mappings/*","indices:admin/validate/query"]
searchguard.actionrequestfilter.readonly.forbidden_actions: ["indices:data/write/*"]

#配置角色权限（禁止访问）
searchguard.actionrequestfilter.deny.allowed_actions: []
searchguard.actionrequestfilter.deny.forbidden_actions: ["indices:data/write/*"]
#################search-guard###################
logging.yml 添加

logger.com.floragunn: DEBUG  #开启debug，方便调试
创建key

echo 'be226fd1e6ddc74b' >/home/work/app/elasticsearch/keys/searchguard.key
重启elasticsearch
配置权限策略如下 ：

curl -XPUT 'http://localhost:9200/searchguard/ac/ac?pretty' -d '
{"acl": [
    {
      "__Comment__": "Default is to execute all filters",
      "filters_bypass": [],
      "filters_execute": ["actionrequestfilter.deny"]
    }, //默认禁止访问
    {
      "__Comment__": "This means that every requestor (regardless of the requestors hostname and username) which has the root role can do anything",
      "roles": [
        "root"
      ],
      "filters_bypass": ["*"],
      "filters_execute": []
    }, // root角色完全权限
    {
      "__Comment__": "This means that for the user spock on index popstuff only the actionrequestfilter.readonly will be executed, no other",
      "users": ["user1"],
      "indices": ["index1-*","index2-*",".kibana"],
      "filters_bypass": ["actionrequestfilter.deny"],
      "filters_execute": ["actionrequestfilter.readonly"]
    }, //user1 用户只能访问index1-*,index2-* 索引，且只有只读权限 
    {
      "__Comment__": "This means that for the user spock on index popstuff only the actionrequestfilter.readonly will be executed, no other",
      "users": ["user2"],
      "indices": ["index3-*",".kibana"],
      "filters_bypass": ["actionrequestfilter.deny"],
      "filters_execute": ["actionrequestfilter.readonly"]
    } //user2 用户只能访问index3-* 索引，且只有只读权限 

  ]}}
查看策略

curl -XGET -uadmin:admin http://localhost:9200/searchguard/ac/ac
注：以上是我自己使用的策略，方便于不同用户访问不同索引，不授权的索引无法查看，分组控制不同rd查看各自业务的日志 +




searchguard 在 Elasticsearch 2.x 上的运用

本节作者：wdh
本节内容基于以下软件版本：
elasticsearch 2.1.1-1
kibana 4.3.1
logstash 2.1.1-1
searchguard 2.x 更新后跟 shield 配置上很相似，相比之前的版本简洁很多。
searchguard 优点有：
节点之间通过 SSL/TLS 传输
支持 JDK SSL 和 Open SSL
支持热载入，不需要重启服务
支持 kibana4 及 logstash 的配置
可以控制不同的用户访问不同的权限
配置简单
安装

安装search-guard-ssl
bin/plugin install com.floragunn/search-guard-ssl/2.1.1.5
安装search-guard-2
bin/plugin install com.floragunn/search-guard-2/2.1.1.0-alpha2
配置 elasticsearch 支持 ssl
elasticsearch.yml增加以下配置：
#############################################################################################
#                                     SEARCH GUARD SSL                                      #
#                                       Configuration                                       #
#############################################################################################

#This will likely change with Elasticsearch 2.2, see [PR 14108](https://github.com/elastic/elasticsearch/pull/14108)
security.manager.enabled: false

#############################################################################################
# Transport layer SSL                                                                       #
#                                                                                           #
#############################################################################################

# Enable or disable node-to-node ssl encryption (default: true)
#searchguard.ssl.transport.enabled: false
# JKS or PKCS12 (default: JKS)
#searchguard.ssl.transport.keystore_type: PKCS12
# Relative path to the keystore file (mandatory, this stores the server certificates), must be placed under the config/ dir
searchguard.ssl.transport.keystore_filepath: node0-keystore.jks
# Alias name (default: first alias which could be found)
searchguard.ssl.transport.keystore_alias: my_alias
# Keystore password (default: changeit)
searchguard.ssl.transport.keystore_password: changeit

# JKS or PKCS12 (default: JKS)
#searchguard.ssl.transport.truststore_type: PKCS12
# Relative path to the truststore file (mandatory, this stores the client/root certificates), must be placed under the config/ dir
searchguard.ssl.transport.truststore_filepath: truststore.jks
# Alias name (default: first alias which could be found)
searchguard.ssl.transport.truststore_alias: my_alias
# Truststore password (default: changeit)
searchguard.ssl.transport.truststore_password: changeit
# Enforce hostname verification (default: true)
#searchguard.ssl.transport.enforce_hostname_verification: true
# If hostname verification specify if hostname should be resolved (default: true)
#searchguard.ssl.transport.resolve_hostname: true
# Use native Open SSL instead of JDK SSL if available (default: true)
#searchguard.ssl.transport.enable_openssl_if_available: false

#############################################################################################
# HTTP/REST layer SSL                                                                       #
#                                                                                           #
#############################################################################################
# Enable or disable rest layer security - https, (default: false)
#searchguard.ssl.http.enabled: true
# JKS or PKCS12 (default: JKS)
#searchguard.ssl.http.keystore_type: PKCS12
# Relative path to the keystore file (this stores the server certificates), must be placed under the config/ dir
#searchguard.ssl.http.keystore_filepath: keystore_https_node1.jks
# Alias name (default: first alias which could be found)
#searchguard.ssl.http.keystore_alias: my_alias
# Keystore password (default: changeit)
#searchguard.ssl.http.keystore_password: changeit
# Do the clients (typically the browser or the proxy) have to authenticate themself to the http server, default is false
#searchguard.ssl.http.enforce_clientauth: false
# JKS or PKCS12 (default: JKS)
#searchguard.ssl.http.truststore_type: PKCS12
# Relative path to the truststore file (this stores the client certificates), must be placed under the config/ dir
#searchguard.ssl.http.truststore_filepath: truststore_https.jks
# Alias name (default: first alias which could be found)
#searchguard.ssl.http.truststore_alias: my_alias
# Truststore password (default: changeit)
#searchguard.ssl.http.truststore_password: changeit
# Use native Open SSL instead of JDK SSL if available (default: true)
#searchguard.ssl.http.enable_openssl_if_available: false
增加 searchguard 的管理员帐号配置，同样在 elasticsearch.yml 中，增加以下配置：
security.manager.enabled: false
searchguard.authcz.admin_dn:
  - "CN=admin,OU=client,O=client,l=tEst,C=De" #DN
重启 elasticsearch
将 node 证书和根证书放在 elasticsearch 配置文件目录下，证书可用 openssl 生成，修改了下官方提供的脚本。
注意：证书中的 client 的 DN 及 server 的 oid，证书不正确会导致 es 服务起不来。（我曾经用 ejbca 生成证书不能使用）
配置

searchguard 主要有5个配置文件在 plugins/search-guard-2/sgconfig 下：
sg_config.yml:
主配置文件不需要做改动
sg_internal_users.yml:
user 文件，定义用户。对于 ELK 我们需要一个 kibana 登录用户和一个 logstash 用户：
kibana4:
  hash: $2a$12$xZOcnwYPYQ3zIadnlQIJ0eNhX1ngwMkTN.oMwkKxoGvDVPn4/6XtO
  #password is: kirk
  roles:
    - kibana4
logstash:
  hash: $2a$12$xZOcnwYPYQ3zIadnlQIJ0eNhX1ngwMkTN.oMwkKxoGvDVPn4/6XtO
密码可用plugins/search-guard-2/tools/hash.sh生成
sg_roles.yml:
权限配置文件，这里提供 kibana4 和 logstash 的权限，以下是我修改后的内容，可自行修改该部分内容（插件安装自带的配置文件中权限不够，kibana 会登录不了，shield 中同样的权限却是够了）：
sg_kibana4:
  cluster:
      - cluster:monitor/nodes/info
      - cluster:monitor/health
  indices:
    '*':
      '*':
        - indices:admin/mappings/fields/get
        - indices:admin/validate/query
        - indices:data/read/search
        - indices:data/read/msearch
        - indices:admin/get
        - indices:data/read/field_stats
    '?kibana':
      '*':
        - indices:admin/exists
        - indices:admin/mapping/put
        - indices:admin/mappings/fields/get
        - indices:admin/refresh
        - indices:admin/validate/query
        - indices:data/read/get
sg_logstash:
  cluster:
    - indices:admin/template/get
    - indices:admin/template/put
  indices:
    'logstash-*':
      '*':
        - WRITE
        - indices:data/write/bulk
        - indices:data/write/delete
        - indices:data/write/update
        - indices:data/read/search
        - indices:data/read/scroll
        - CREATE_INDEX
sg_roles_mapping.yml:
定义用户的映射关系，添加 kibana 及 logstash 用户对应的映射：
sg_logstash:
  users:
    - logstash
sg_kibana4:
  backendroles:
    - kibana
  users:
    - kibana4
sg_action_groups.yml:
定义权限

加载配置并启用

plugins/search-guard-2/tools/sgadmin.sh -cd plugins/search-guard-2/sgconfig/ -ks plugins/search-guard-2/sgconfig/admin-keystore.jks -ts plugins/search-guard-2/sgconfig/truststore.jks  -nhnv
（如修改了密码，则需要使用plugins/search-guard-2/tools/sgadmin.sh -h查看对应参数）
注意证书路径，将生成的admin证书和根证书放在sgconfig目录下。
最后，可以尝试登录啦！
登录界面会有验证
帐号：kibana4 密码：kirk
更多的权限配置可以自己研究。+







监控方案

Elasticsearch 作为一个分布式系统，监控自然是重中之重。Elasticsearch 本身提供了非常完善的，由浅及深的各种性能数据接口。和数据读写检索接口一样，采用 RESTful 风格。我们可以直接使用 curl 来获取数据，编写监控程序，也可以使用一些现成的监控方案。通常这些方案也是通过接口读取数据，解析 JSON，渲染界面。
本章会先介绍一些常用的监控接口，以及其响应数据的含义。然后再介绍几种常用的开源和商业 Elasticsearch 监控产品。+




集群健康状态监控

说到 Elasticsearch 集群监控，首先我们肯定是需要一个从总体意义上的概要。不管是多大规模的集群，告诉我正常还是不正常？没错，集群健康状态接口就是用来回答这个问题的，而且这个接口的信息已经出于意料的丰富了。
命令示例

# curl -XGET 127.0.0.1:9200/_cluster/health?pretty
{
  "cluster_name" : "es1003",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 38,
  "number_of_data_nodes" : 27,
  "active_primary_shards" : 1332,
  "active_shards" : 2381,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "number_of_pending_tasks" : 0
}
状态信息

输出里最重要的就是 status 这行。很多开源的 ES 监控脚本，其实就是拿这行数据做报警判断。status 有三个可能的值：
green 绿灯，所有分片都正确运行，集群非常健康。
yellow 黄灯，所有主分片都正确运行，但是有副本分片缺失。这种情况意味着 ES 当前还是正常运行的，但是有一定风险。注意，在 Kibana4 的 server 端启动逻辑中，即使是黄灯状态，Kibana 4 也会拒绝启动，死循环等待集群状态变成绿灯后才能继续运行。
red 红灯，有主分片缺失。这部分数据完全不可用。而考虑到 ES 在写入端是简单的取余算法，轮到这个分片上的数据也会持续写入报错。
对 Nagios 熟悉的读者，可以直接将这个红黄绿灯对应上 Nagios 体系中的 Critical，Warning，OK 。
其他数据解释

number_of_nodes 集群内的总节点数。
number_of_data_nodes 集群内的总数据节点数。
active_primary_shards 集群内所有索引的主分片总数。
active_shards 集群内所有索引的分片总数。
relocating_shards 正在迁移中的分片数。
initializing_shards 正在初始化的分片数。
unassigned_shards 未分配到具体节点上的分片数。
显然，后面三项在正常情况下，一般都应该是 0。但是如果真的出来了长期非 0 的情况，怎么才能知道这些长期 unassign 或者 initialize 的分片影响的是哪个索引呢？本书随后还有有更多接口获取相关信息。不过在集群健康这层，本身就可以得到更详细一点的内容了。
level 请求参数

接口请求的时候，可以附加一个 level 参数，指定输出信息以 indices 还是 shards 级别显示。当然，一般来说，indices 级别就够了。
# curl -XGET http://127.0.0.1:9200/_cluster/health?level=indices
{
   "cluster_name": "es1003",
   "status": "red",
   "timed_out": false,
   "number_of_nodes": 38,
   "number_of_data_nodes": 27,
   "active_primary_shards": 1332,
   "active_shards": 2380,
   "relocating_shards": 0,
   "initializing_shards": 0,
   "unassigned_shards": 1
   "indices": {
      "logstash-2015.05.31": {
         "status": "green",
         "number_of_shards": 81,
         "number_of_replicas": 0,
         "active_primary_shards": 81,
         "active_shards": 81,
         "relocating_shards": 0,
         "initializing_shards": 0,
         "unassigned_shards": 0
      },
      "logstash-2015.05.30": {
         "status": "red",
         "number_of_shards": 81,
         "number_of_replicas": 0,
         "active_primary_shards": 80,
         "active_shards": 80,
         "relocating_shards": 0,
         "initializing_shards": 0,
         "unassigned_shards": 1
      },
      ...
   }
}
这就看到了，是 logstash-2015.05.30 索引里，有一个分片一直未能成功分配，导致集群状态异常的。
不过，一般来说，集群健康接口，还是只用来简单监控一下集群状态是否正常。一旦收到异常报警，具体确定 unassign shard 的情况，更推荐使用 kopf 工具在页面查看。+




节点状态监控接口

集群状态是从最上层高度来评估你的集群概况，而节点状态则更底层一些，会返回给你集群里每个节点的统计信息。这个接口的信息极为丰富，从硬件到数据到线程，应有尽有。本节会以单节点为例，分段介绍各部分数据的含义。
首先，通过如下命令获取节点状态：
# curl -XGET 127.0.0.1:9200/_nodes/stats
节点概要

返回数据的第一部分是节点概要，主要就是节点的主机名，网卡地址和监听端口等。这部分内容除了极少数时候(一个主机上运行了多个 ES 节点)一般没有太大用途。
{
   "cluster_name": "elasticsearch_zach",
   "nodes": {
      "UNr6ZMf5Qk-YCPA_L18BOQ": {
         "timestamp": 1408474151742,
         "name": "Zach",
         "transport_address": "inet[zacharys-air/192.168.1.131:9300]",
         "host": "zacharys-air",
         "ip": [
            "inet[zacharys-air/192.168.1.131:9300]",
            "NONE"
         ],
...
索引信息

这部分内容会列出该节点上存储的所有索引数据的状态统计。
首先是概要：
    "indices": {
        "docs": {
           "count": 6163666,
           "deleted": 0
        },
        "store": {
           "size_in_bytes": 2301398179,
           "throttle_time_in_millis": 122850
        },
docs.count 是节点上存储的数据条目总数；store.size_in_bytes 是节点上存储的数据占用磁盘的实际大小。而 store.throttle_time_in_millis 则是 ES 进程在做 segment merge 时出现磁盘限速的时长。如果你在 ES 的日志里经常会看到限速声明，那么这里的数值也会偏大。
写入性能：
        "indexing": {
           "index_total": 803441,
           "index_time_in_millis": 367654,
           "index_current": 99,
           "delete_total": 0,
           "delete_time_in_millis": 0,
           "delete_current": 0
        },
indexing.index_total 是一个递增累计数，表示节点完成的数据写入总次数。至于后面又删除了多少，额外记录在 indexing.delete_total 里。
读取性能：
        "get": {
           "total": 6,
           "time_in_millis": 2,
           "exists_total": 5,
           "exists_time_in_millis": 2,
           "missing_total": 1,
           "missing_time_in_millis": 0,
           "current": 0
        },
get 这里显示的是直接使用 _id 读取数据的状态。
搜索性能：
        "search": {
           "open_contexts": 0,
           "query_total": 123,
           "query_time_in_millis": 531,
           "query_current": 0,
           "fetch_total": 3,
           "fetch_time_in_millis": 55,
           "fetch_current": 0
        },
search.open_contexts 表示当前正在进行的搜索，而 search.query_total 表示节点启动以来完成过的总搜索数，search.query_time_in_millis 表示完成上述搜索数花费时间的总和。显然，query_time_in_millis/query_total 越大，说明搜索性能越差，可以通过 ES 的 slowlog，获取具体的搜索语句，做出针对性的优化。
search.fetch_total 等指标含义类似。因为 ES 的搜索默认是 query-then-fetch 式的，所以 fetch 一般是少而快的。如果计算出来 search.fetch_time_in_millis > search.query_time_in_millis，说明有人采用了较大的 size 参数做分页查询，通过 slowlog 抓到具体的语句，相机优化成 scan 式的搜索。
段合并性能：
        "merges": {
           "current": 0,
           "current_docs": 0,
           "current_size_in_bytes": 0,
           "total": 1128,
           "total_time_in_millis": 21338523,
           "total_docs": 7241313,
           "total_size_in_bytes": 5724869463
        },
merges 数据分为两部分，current 开头的是当前正在发生的段合并行为统计；total 开头的是历史总计数。一般来说，作为 ELK Stack 应用，都是以数据写入压力为主的，merges 相关数据会比较突出。
过滤器缓存：
        "filter_cache": {
           "memory_size_in_bytes": 48,
           "evictions": 0
        },
filter_cache.memory_size_in_bytes 表示过滤器缓存使用的内存，filter_cache.evictions 表示因内存满被回收的缓存大小，这个数如果较大，说明你的过滤器缓存大小不足，或者过滤器本身不太适合缓存。比如在 ELK Stack 场景中常用的时间过滤器，如果使用 @timestamp:["now-1d" TO "now"] 这种表达式的话，需要每次计算 now 值，就没法长期缓存。事实上，Kibana 中通过 timepicker 生成的 filtered 请求里，对 @timestamp 部分就并不是直接使用 "now"，而是在浏览器上计算成毫秒数值，再发送给 ES 的。
请注意，过滤器缓存是建立在 segment 基础上的，在当天新日志的索引中，存在大量的或多或少的 segments。一个已经 5GB 大小的 segment，和一个刚刚 2MB 大小的 segment，发生一次 filter_cache.evictions 对搜索性能的影响区别是巨大的。但是节点状态中本身这个计数并不能反应这点区别。所以，尽力减少这个数值，但如果搜索本身感觉不慢，那么有几个也无所谓。
id 缓存：
        "id_cache": {
           "memory_size_in_bytes": 0
        },
id_cache 是 parent/child mappings 使用的内存。不过在 ELK Stack 场景中，一般不会用到这个特性，所以此处数据应该一直是 0。
fielddata：
        "fielddata": {
           "memory_size_in_bytes": 0,
           "evictions": 0
        },
此处显示 fielddata 使用的内存大小。fielddata 用来做聚合，排序等工作。
注意：fielddata.evictions 应该永远是 0。一旦发现这个数据大于 0，请立刻检查自己的内存配置，fielddata 限制以及请求语句。
segments：
        "segments": {
           "count": 319,
           "memory_in_bytes": 65812120
        },
segments.count 表示节点上所有索引的 segment 数目的总和。一般来说，一个索引通常会有 50-150 个 segments。再多就对写入性能有较大影响了(可能 merge 速度跟不上新 segment 出现的速度)。所以，请根据节点上的索引数据正确评估节点 segment 的情况。
segments.memory_in_bytes 表示 segments 本身底层数据结构所使用的内存大小。像索引的倒排表，词典，bloom filter(ES1.4以后已经默认关闭) 等，都是要在内存里的。所以过多的 segments 会导致这个数值迅速变大。
操作系统和进程信息

操作系统信息主要包括 CPU，Loadavg，Memory 和 Swap 利用率，文件句柄等。这些内容都是常见的监控项，本书不再赘述。
进程，即 JVM 信息，主要在于 GC 相关数据。
GC

对不了解 JVM 的 GC 的读者，这里先介绍一下 GC(垃圾收集)以及 GC 对 Elasticsearch 的影响。
Java is a garbage-collected language, which means that the programmer does not manually manage memory allocation and deallocation. The programmer simply writes code, and the Java Virtual Machine (JVM) manages the process of allocating memory as needed, and then later cleaning up that memory when no longer needed. Java 是一个自动垃圾收集的编程语言，启动 JVM 虚拟机的时候，会分配到固定大小的内存块，这个块叫做 heap(堆)。JVM 会把 heap 分成两个组：
Young 新实例化的对象所分配的空间。这个空间一般来说只有 100MB 到 500MB 大小。Young 空间又分为两个 survivor(幸存)空间。当 Young 空间满，就会发生一次 young gc，还存活的对象，就被移入幸存空间里，已失效的对象则被移除。
Old 老对象存储的空间。这些对象应该是长期存活而且在较长一段时间内不会变化的内容。这个空间会大很多，在 ES 来说，一节点上可能就有 30GB 内存是这个空间。前面提到的 young gc 中，如果某个对象连续多次幸存下来，就会被移进 Old 空间内。而等到 Old 空间满，就会发生一次 old gc，把失效对象移除。
听起来很美好的样子，但是这些都是有代价的！在 GC 发生的时候，JVM 需要暂停程序运行，以便自己追踪对象图收集全部失效对象。在这期间，其他一切都不会继续运行。请求没有响应，ping 没有应答，分片不会分配……
当然，young gc 一般来说执行极快，没太大影响。但是 old 空间那么大，稍慢一点的 gc 就意味着程序几秒乃至十几秒的不可用，这太危险了。
JVM 本身对 gc 算法一直在努力优化，Elasticsearch 也尽量复用内部对象，复用网络缓冲，然后还提供像 Doc Values 这样的特性。但不管怎么说，gc 性能总是我们需要密切关注的数据，因为它是集群稳定性最大的影响因子。
如果你的 ES 集群监控里发现经常有很耗时的 GC，说明集群负载很重，内存不足。严重情况下，这些 GC 导致节点无法正确响应集群之间的 ping ，可能就直接从集群里退出了。然后数据分片也随之在集群中重新迁移，引发更大的网络和磁盘 IO，正常的写入和搜索也会受到影响。
在节点状态数据中，以下部分就是 JVM 相关的数据：
"jvm": {
    "timestamp": 1408556438203,
    "uptime_in_millis": 14457,
    "mem": {
       "heap_used_in_bytes": 457252160,
       "heap_used_percent": 44,
       "heap_committed_in_bytes": 1038876672,
       "heap_max_in_bytes": 1038876672,
       "non_heap_used_in_bytes": 38680680,
       "non_heap_committed_in_bytes": 38993920,
    },
首先可以看到的就是 heap 的情况。其中这个 heap_committed_in_bytes 指的是实际被进程使用的内存，以 JVM 的特性，这个值应该等于 heap_max_in_bytes。heap_used_percent 则是一个更直观的阈值数据。当这个数据大于 75% 的时候，ES 就要开始 GC。也就是说，如果你的节点这个数据长期在 75% 以上，说明你的节点内存不足，GC 可能会很慢了。更进一步，如果到 85% 或者 95% 了，估计节点一次 GC 能耗时 10s 以上，甚至可能会发生 OOM 了。
继续看下一段：
   "pools": {
      "young": {
         "used_in_bytes": 138467752,
         "max_in_bytes": 279183360,
         "peak_used_in_bytes": 279183360,
         "peak_max_in_bytes": 279183360
      },
      "survivor": {
         "used_in_bytes": 34865152,
         "max_in_bytes": 34865152,
         "peak_used_in_bytes": 34865152,
         "peak_max_in_bytes": 34865152
      },
      "old": {
         "used_in_bytes": 283919256,
         "max_in_bytes": 724828160,
         "peak_used_in_bytes": 283919256,
         "peak_max_in_bytes": 724828160
      }
   }
},
这段里面列出了 young, survivor, 和 old GC 区域的情况，不过一般来说用途不大。再看下一段：
    "gc": {
       "collectors": {
          "young": {
             "collection_count": 13,
             "collection_time_in_millis": 923
          },
          "old": {
             "collection_count": 0,
             "collection_time_in_millis": 0
          }
       }
    }
这里显示的 young 和 old gc 的计数和耗时。young gc 的 count 一般比较大，这是正常情况。old gc 的 count 应该就保持在比较小的状态，包括耗时的 collection_time_in_millis 也应该很小。注意这两个计数都是累计的，所以对于一个长期运行的系统，不能拿这个数值直接做报警的判断，应该是取两次节点数据的差值。有了差值之后，再来看耗时的问题，一般来说，一次 young gc 的耗时应该在 1-2 ms，old gc 在 100 ms 的水平。如果这个耗时有量级上的差距，建议打开 slow-GC 日志，具体研究原因。
线程池信息

Elasticsearch 内部是保持着几个线程池的，不同的工作由不同的线程池负责。一般来说，每个池子的工作线程数跟你的 CPU 核数一样。之前有传言中的优化配置是加大这方面的配置项，其实没有什么实际帮助 —— 能干活的 CPU 就那么些个数。所以这段状态数据目的不是用作 ES 配置调优，而是作为性能监控，方便优化你的读写请求。
ES 在 index、bulk、search、get、merge 等各种操作都有专门的线程池，大家的统计数据格式都是类似的：
  "index": {
     "threads": 1,
     "queue": 0,
     "active": 0,
     "rejected": 0,
     "largest": 1,
     "completed": 1
  }
这些数据中，最重要的是 rejected 数据。当线程中中所有的工作线程都在忙，即 active == threads，后续的请求就会暂时放到排队的队列里，即 queue > 0。但是每个线程池的 queue 也是有大小限制的，默认是 100。如果后续请求超过这个大小，意味着 ES 真的接受不过来这个请求了，就会把后续请求 reject 掉。
Bulk Rejections

如果你确实注意到了上面数据中的 rejected，很可能就是你在发送 bulk 写入的时候碰到 HTTP 状态码 429 的响应报错了。事实上，集群的承载能力是有上限的。如果你集群每秒就能写入 10000 条数据，以其浪费内存多放几条数据在排队，还不如直接拒绝掉。至少可以让你知道到瓶颈了。
另外有一点可以指出的是，因为 bulk queue 里的数据是维护在内存中，所以节点发生意外死机的时候，是会丢失的。
如果你碰到 bulk rejected，可以尝试以下步骤：
暂停所有的写入进程。
从 bulk 响应中过滤出来 rejected 的那部分。因为 bulk index 中可能大部分已经成功了。
重发一次失败的请求。
恢复写入进程，或者重新来一次上述步骤。
大家可能看出来了，没错，对 rejected 其实压根没什么特殊的操作，重试一次而已。
当然，如果这个 rejected 是持续存在并增长的，那重试也无济于事。你可能需要考虑自己集群是否足以支撑当前的写入速度要求。
如果确实没问题，那么可能是因为客户端并发太多，超过集群的 bulk threads 总数了。尝试减少自己的写入进程个数，改成加大每次 bulk 请求的 size。
文件系统和网络

数据继续往下走，是文件系统和网络的数据。文件系统方面，不管是剩余空间还是 IO 数据，都推荐大家还是通过更传统的系统层监控手段来做。而网络数据方面，主要有两部分内容：
        "transport": {
            "server_open": 13,
            "rx_count": 11696,
            "rx_size_in_bytes": 1525774,
            "tx_count": 10282,
            "tx_size_in_bytes": 1440101928
         },
         "http": {
            "current_open": 4,
            "total_opened": 23
         },
我们知道 ES 同时运行着 transport 和 http ，默认分别是 9300 和 9200 端口。由于 ES 使用了一些 transport 连接来维护节点内部关系，所以 transport.server_open 正常情况下一直会有一定大小。而 http.current_open 则是实际连接上来的 HTTP 客户端的数量，考虑到 HTTP 建联的消耗，强烈建议大家使用 keep-alive 长连接的客户端。
Circuit Breaker

继续往下，是 fielddata circuit breaker 的数据：
         "fielddata_breaker": {
            "maximum_size_in_bytes": 623326003,
            "maximum_size": "594.4mb",
            "estimated_size_in_bytes": 0,
            "estimated_size": "0b",
            "overhead": 1.03,
            "tripped": 0
         }
fielddata_breaker.maximum_size 是一个请求能使用的内存的最大值。fielddata_breaker.tripped 记录的是触发 circuit breaker 的次数。如果这个数值太高，或者持续增长，说明目前 ES 收到的请求亟待优化，或者单纯的，加机器，加内存。
hot_threads 状态

除了 stats 信息以外，/_nodes/ 下还有另一个监控接口：
# curl -XGET 'http://127.0.0.1:9200/_nodes/_local/hot_threads?interval=60s'
该接口会返回在 interval 时长内，该节点消耗资源最多的前三个线程的堆栈情况。这对于性能调优初期，采集现状数据，极为有用。
默认的采样间隔是 500ms，一般来说，这个时间范围是不太够的，建议至少 60s 以上。
默认的，资源消耗是按照 CPU 来衡量，还可以用 ?type=wait 或者 ?type=block 来查看在等待和堵塞状态的当前线程排名。+




索引状态监控接口

索引状态监控接口的输出信息和节点状态监控接口非常类似。一般情况下，这个接口单独监控起来的意义并不大。
不过在 ES 最新的 1.6 版，新加入了对索引分片级别的 commit id 功能。
回忆一下之前原理章节的内容，commit 是在分片内部，对每个 segment 做的。而数据在主分片和副本分片上，是由各自节点自行做 segment merge 操作，所以副本分片和主分片的 segment 的 commit id 是不一致的。这导致 ES 副本恢复时，跟主分片比对 commit id，基本上每个 segment 都不一样，所以才需要从主分片完整重传一份数据。
新加入分片级别的 commit id 后，副本恢复时，先比对跟主分片的分片级 commit id，如果一致，直接本地恢复副本分片内容即可。
查看分片级别 commit id 的命令如下：
# curl 'http://127.0.0.1:9200/logstash-mweibo-2015.06.15/_stats/commit?level=shards&pretty'
...
  "indices" : {
    "logstash-2015.06.15" : {
      "primaries" : { },
      "total" : { },
      "shards" : {
        "0" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "AqaYWFQJRIK0ZydvVgASEw",
            "relocating_node" : null
          },
          "commit" : {
            "generation" : 726,
            "user_data" : {
              "translog_id" : "1434297603053",
              "sync_id" : "AU4LEh6wnBE6n0qcEXs5"
            },
            "num_docs" : 36792652
          }
        } ],
...
注意：为了节约频繁变更的资源消耗，ES 并不会实时更新分片级 commit id。只有连续 5 分钟没有新数据写入的索引，才会触发给索引各分片更新 commit id 的操作。如果你查看的是一个还在新写入数据的索引，看到的内容应该是下面这样：
          "commit" : {
            "generation" : 590,
            "user_data" : {
              "translog_id" : "1434038402801"
            },
            "num_docs" : 29051938
          }




等待执行的任务列表

首先需要解释的是，这个接口返回的列表，只是对于集群的 master 节点来说的等待执行。数据节点本身的写入和查询线程，如果因为较慢导致排队，是不在这个列表里的。
之前章节已经讲过，master 节点负责处理的任务其实很少，只有集群状态的数据维护。所以绝大多数情况下，这个任务列表应该都是空的。
# curl -XGET http://127.0.0.1:9200/_cluster/pending_tasks
{
   "tasks": []
}
但是如果你碰上集群有异常，比如频繁有索引映射更新，数据恢复啊，分片分配或者初始化的时候反复出错啊这种时候，就会看到一些任务列表了：
{ "tasks" : [ { "insert_order": 767003, "priority": "URGENT", "source": "create-index [logstash-2015.06.01], cause [api]", "time_in_queue_millis": 86, "time_in_queue": "86ms" }, { "insert_order" : 767004, "priority" : "HIGH", "source" : "shard-failed ([logstash-2015.05.31][50], node[F3EGSRWGSvWGFlcZ6K9pRA], [P], s[INITIALIZING]), reason [shard failure [failed recovery][IndexShardGatewayRecoveryException[[logstash-2015.05.31][50] failed to fetch index version after copying it over]; nested: CorruptIndexException[[logstash-2015.05.31][50] Preexisting corrupted index [corrupted_nC9t_ramRHqsbQqZO78KVg] caused by: CorruptIndexException[did not read all bytes from file: read 269 vs size 307 (resource: BufferedChecksumIndexInput(NIOFSIndexInput(path=\"/data1/elasticsearch/data/es1003/nodes/0/indices/logstash-2015.05.31/50/index/_16c.si\")))]\norg.apache.lucene.index.CorruptIndexException: did not read all bytes from file: read 269 vs size 307 (resource: BufferedChecksumIndexInput(NIOFSIndexInput(path=\"/data1/elasticsearch/data/es1003/nodes/0/indices/logstash-2015.05.31/50/index/_16c.si\")))\n\tat org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:216)\n\tat org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoReader.read(Lucene46SegmentInfoReader.java:73)\n\tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:359)\n\tat org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:462)\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:923)\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:769)\n\tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:458)\n\tat org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:89)\n\tat org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:158)\n\tat org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:148)\n\tat org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:675)\n\tat org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:593)\n\tat org.elasticsearch.index.shard.IndexShard.flush(IndexShard.java:675)\n\tat org.elasticsearch.index.translog.TranslogService$TranslogBasedFlush$1.run(TranslogService.java:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n]; ]]", "executing" : true, "time_in_queue_millis" : 2813, "time_in_queue" : "2.8s" }, { "insert_order" : 767005, "priority" : "HIGH", "source" : "refresh-mapping [logstash-2015.06.03][[curl_debuginfo]]", "executing" : false, "time_in_queue_millis" : 2787, "time_in_queue" : "2.7s" }, { "insert_order" : 767006, "priority" : "HIGH", "source" : "refresh-mapping [logstash-2015.05.29][[curl_debuginfo]]", "executing" : false, "time_in_queue_millis" : 448, "time_in_queue" : "448ms" } ] } ```
可以看到列表中的任务都有各自的优先级，URGENT 优先于 HIGH。然后是任务在队列中的排队时间，任务的具体内容等。
在上例中，由于磁盘文件损坏，一个分片中某个 segment 的实际内容和长度对不上，导致分片数据恢复无法正常完成，堵塞了后续的索引映射更新操作。这个错误一般来说不太常见，也只能是关闭索引，或者放弃这部分数据。更常见的可能，是集群存储长期数据导致索引映射数据确实大到了 master 节点内存不足以快速处理的地步。
这时候，根据实际情况，可以有以下几种选择：
索引就是特别多：给 master 加内存。
索引里字段太多：改用 nested object 方式节省字段数量。
索引多到内存就是不够了：把一部分数据拆出来另一个集群。






cat 接口的命令行使用

之前介绍的各种接口数据，其响应数据都是 JSON 格式，更适用于程序处理。对于我们日常运维，在 Linux 命令行终端环境来说，简单的分行和分列表格才是更方便的样式。为此，Elasticsearch 提供了 cat 接口。
cat 接口可以读取各种监控数据，可用接口列表如下：
/_cat/allocation
/_cat/shards
/_cat/shards/{index}
/_cat/master
/_cat/nodes
/_cat/indices
/_cat/indices/{index}
/_cat/segments
/_cat/segments/{index}
/_cat/count
/_cat/count/{index}
/_cat/recovery
/_cat/recovery/{index}
/_cat/health
/_cat/pending_tasks
/_cat/aliases
/_cat/aliases/{alias}
/_cat/thread_pool
/_cat/plugins
/_cat/fielddata
/_cat/fielddata/{fields}
集群状态

还是以最基础的集群状态为例，采用 cat 接口查询集群状态的命令如下：
# curl -XGET http://127.0.0.1:9200/_cat/health
1434300299 00:44:59 es1003 red 39 27 2589 1505 4 1 0 0
如果单看这行输出，或许不熟悉的用户会有些茫然。可以通过添加 ?v 参数，输出表头：
# curl -XGET http://127.0.0.1:9200/_cat/health?v
epoch      timestamp cluster status node.total node.data shards  pri relo init unassign pending_tasks
1434300334 00:45:34  es1003  green          39        27   2590 1506    5    0        0             0
节点状态

# curl -XGET http://127.0.0.1:9200/_cat/nodes?v
host      ip            heap.percent ram.percent load node.role master name
esnode109 10.19.0.109             62          69 6.37 d         -      10.19.0.109 
esnode096 10.19.0.96              63          69 0.29 -         -      10.19.0.96  
esnode100 10.19.0.100             56          79 0.07 -         m      10.19.0.100
跟集群状态不一样的是，节点状态数据太多，cat 接口不方便在一行表格中放下所有数据。所以默认的返回，只是最基本的内存和负载数据。具体想看某方面的数据，也是通过请求参数的方式额外指明。比如想看 heap 百分比和最大值：
# curl -XGET 'http://127.0.0.1:9200/_cat/nodes?v&h=ip,port,heapPercent,heapMax'
ip            port heapPercent heapMax
192.168.1.131 9300          66 25gb
h 请求参数可用的值，可以通过 ?help 请求参数来查询：
# curl -XGET http://127.0.0.1:9200/_cat/nodes?help
id               | id,nodeId               | unique node id
host             | h                       | host name
ip               | i                       | ip address
port             | po                      | bound transport port
heap.percent     | hp,heapPercent          | used heap ratio
heap.max         | hm,heapMax              | max configured heap
ram.percent      | rp,ramPercent           | used machine memory ratio
ram.max          | rm,ramMax               | total machine memory
load             | l                       | most recent load avg
node.role        | r,role,dc,nodeRole      | d:data node, c:client node
...
中间第二列就是对应的请求参数的值及其缩写。也就是说上面示例还可以写成：
# curl -XGET http://127.0.0.1:9200/_cat/nodes?v&h=i,po,hp,hm
索引状态

查询索引列表和存储的数据状态是也是 cat 接口最常用的功能之一。为了方便阅读，默认输出时会把数据大小以更可读的方式自动换算成合适的单位，比如 3.2tb 这样。
如果你打算通过 shell 管道做后续处理，那么可以加上 ?bytes 参数，指明统一采用字节数输出，这样保证在同一个级别上排序：
# curl -XGET http://127.0.0.1:9200/_cat/indices?bytes=b | sort -rnk8
green open logstash-mweibo-2015.06.12        26 1  754641614   0 2534955821580 1256680767317 
green open logstash-mweibo-2015.06.14        27 1  855516794   0 2419569433696 1222355996673
分片状态

# curl -XGET http://127.0.0.1:9200/_cat/shards?v
index                             shard prirep state            docs    store ip        node
logstash-mweibo-h5view-2015.06.10 20    p      STARTED       4690968  679.2mb 127.0.0.1 10.19.0.108
logstash-mweibo-h5view-2015.06.10 20    r      STARTED       4690968  679.4mb 127.0.0.1 10.19.0.39
logstash-mweibo-h5view-2015.06.10 2     p      STARTED       4725961  684.3mb 127.0.0.1 10.19.0.53
logstash-mweibo-h5view-2015.06.10 2     r      STARTED       4725961  684.3mb 127.0.0.1 10.19.0.102
同样，可以用 ?help 查询其他可用数据细节。比如每个分片的 segment.count：
# curl -XGET 'http://127.0.0.1:9200/_cat/shards/logstash-mweibo-nginx-2015.06.14?v\&h=n,iic,sc'
n           iic sc
10.19.0.72    0 42
10.19.0.41    0 36
10.19.0.104   0 32
10.19.0.102   0 40
恢复状态

在出现集群状态波动时，通过这个接口查看数据迁移和恢复速度也是一个非常有用的功能。不过默认输出是把集群历史上所有发生的 recovery 记录都返回出来，所以一般会加上 ?active_only 参数，仅列出当前还在运行的恢复状态：
# curl -XGET 'http://127.0.0.1:9200/_cat/recovery?active_only&v&h=i,s,shost,thost,fp,bp,tr,trp,trt'
i                          s  shost     thost     fp    bp    tr trp    trt
logstash-mweibo-2015.06.12 10 esnode041 esnode080 87.6% 35.3% 0  100.0% 0
logstash-mweibo-2015.06.13 10 esnode108 esnode080 95.5% 88.3% 0  100.0% 0
logstash-mweibo-2015.06.14 17 esnode102 esnode080 96.3% 92.5% 0  0.0%   118758
线程池状态

curl -s -XGET http://127.0.0.1:9200/_cat/thread_pool?v
host      ip          bulk.active bulk.queue bulk.rejected index.active index.queue index.rejected search.active search.queue search.rejected 
esnode073 127.0.0.1             1          0         20669            0           0             50             4            0               0
包括 merge，optimize，flush，refresh 等其他线程池状态，也可以通过 ?h 参数指明获取。



日志记录

Elasticsearch 作为一个服务，本身也会记录很多日志信息。默认情况下，日志都放在 $ES_HOME/logs/ 目录里。
日志级别可以通过 elasticsearch.yml 配置设置，也可以通过 /_cluster/settings 接口动态调整。比如说，如果你的节点一直无法正确的加入集群，你可以将集群自动发现方面的日志级别修改成 DEBUG，来关注这方面的问题：
# curl -XPUT http://127.0.0.1:9200/_cluster/settings -d'
{
    "transient" : {
        "logger.discovery" : "DEBUG"
    }
}'
性能日志

除了进程状态的日志输出，ES 还支持跟性能相关的日志输出。针对数据写入，检索，读取三个阶段，都可以设置具体的慢查询阈值，以及不同的输出等级。
此外，慢查询日志是针对索引级别的设置。除了在 elasticsearch.yml 中设置(注意，默认是全注释不开启的状态)以及通过 /_cluster/settings 接口配置一组集群各索引共用的参数以外，还可以针对每个索引设置不同的参数。
比如说，我们可以先设置集群共同的参数：
# curl -XPUT http://127.0.0.1:9200/_cluster/settings -d'
{
    "transient" : {
        "logger.index.search.slowlog" : "DEBUG",
        "logger.index.indexing.slowlog" : "WARN",
        "index.search.slowlog.threshold.query.debug" : "10s",
        "index.search.slowlog.threshold.fetch.debug": "500ms",
        "index.indexing.slowlog.threshold.index.warn": "5s"
    }
}'
然后针对某个比较大的索引，调高设置：
# curl -XPUT http://127.0.0.1:9200/logstash-wwwlog-2015.06.21/_settings -d'
{
    "index.search.slowlog.threshold.query.warn" : "10s",
    "index.search.slowlog.threshold.fetch.debug": "500ms",
    "index.indexing.slowlog.threshold.index.info": "10s"
}



bigdesk

要想最快的了解 ES 各节点的性能细节，推荐使用 bigdesk 插件，其 GitHub 地址见：https://github.com/lukas-vlcek/bigdesk
bigdesk 通过浏览器直连 ES 节点，发起 RESTful 请求，并渲染结果成图。所以其安装部署极其简单：
# git clone https://github.com/lukas-vlcek/bigdesk
# cd bigdesk
# python -mSimpleHTTPServer
Serving HTTP on 0.0.0.0 port 8000 ...
浏览器打开 http://localhost:8000 即可看到 bigdesk 页面。在 endpoint 输入框内填写要连接的 ES 节点地址，选择 refresh 间隔和 keep 时长，点击 connect，完成。


注意：设置 refresh 间隔请考虑 ELK Stack 使用的 template 里实际的 refresh_interval 是多少。否则你可能看到波动太大的数据，不足以说明情况。
点选某个节点后，就可以看到该节点性能的实时走势。一般重点关注 JVM 性能和索引性能。
有关 JVM 部分截图如下：


有关数据读写性能部分截图如下：


Elasticsearch 2.x 上的部署方法

bigdesk 的源码开发在 2015 年夏天就已经停止，所以默认是无法支持 Elasticsearch 2.x 的监控的。不过总体来说，Elasticsearch 性能监控接口没有什么变化，我们只需要稍微修改 bigdesk，使之符合 Elasticsearch 2.x 的插件规范即可继续使用。
Elasticsearch 2.x 的插件目录规范，可以参考 marvel-agent，kopf 等。
mkdir -p $ES_HOME/plugins/bigdesk
cd $ES_HOME/plugins/bigdesk
git clone https://github.com/lukas-vlcek/bigdesk _site
sed -i '142s/==/>=/' _site/js/store/BigdeskStore.js
cat >plugin-descriptor.properties<<EOF
description=bigdesk - Live charts and statistics for Elasticsearch cluster.
version=2.5.1
site=true
name=bigdesk
EOF
然后启动 Elasticsearch 服务，通过 http://127.0.0.1:9200/_plugin/bigdesk 访问即可。
注意：这种方式只能恢复 node 监控，cluster 部分，bigdesk 是采用了 /_status 接口，这个接口从 Elasticsearch 1.2 开始就已经宣布废弃，2.x 里已经彻底没有了，数据被拆分成了 /_stats 和 /_recovery 两个接口。不是简单修改可以搞定的了。




marvel

本节作者：lemontree
marvel 是 Elastic.co 公司推出的商业监控方案，也是用来监控 Elasticsearch 集群实时、历史状态的有力用具，便于性能优化以及故障诊断。监控主要分为六个层面，分别是集群层、节点层、索引层、分片层、事件层、Sense。
集群层：主要对集群健康情况进行汇总，包括集群名称、集群状态、节点数量、索引个数、分片数、总数据量、集群版本等信息。同时，对节点、索引整体情况分别展示。
节点层：主要对每个节点的 CPU 、内存、负载、索引相关的性能数据等信息进行统计，并进行图形化展示。
索引层：展示的信息与节点层类似，主要从索引的角度展示。
分片层：从索引、节点两个角度展示分片的分布情况，并提供 playback 功能演示分片分配的历史过程。
事件层：展示集群相关事件，如节点脱离、加入，Master 选举、索引创建、Shard 分配等。
Sense：轻量级的开发界面，主要用于通过 API 查询数据，管理集群。
Elastic.co 公司的收费标准是：
开发模式免费
生产环境前 5 个节点，每年 1000 美元
之后每增加 5 个节点，每年加收 250 美元
安装和卸载

marvel 是以 elasticsearch 的插件形式存在的，可以直接通过插件安装：
# ./bin/plugin -i elasticsearch/marvel/latest
如果你是从官网下载的安装包，则运行：
# ./bin/plugin -i marvel -u file:///path/to/marvel-latest.zip
各节点都安装完毕后，可以通过下行命令来查看节点上的插件列表，检查列表中是否含有 marvel：
# curl http://127.0.0.1:9200/_nodes/_local/plugins
安装之后，插件自动运行，并将定期获取到的集群状态数据，存储在 .marvel-YYYY.MM.DD 索引中，以单台 ES 计算，该索引的大小在 500MB 左右。所以，如果在小规模环境下运行，首先请注意，不要让你宝贵的内存都花在 marvel 的数据索引上了。
如果不打算使用 marvel，在各节点上通过下行命令下载：
# ./bin/plugin -r marvel
配置

如果不想让 marvel 数据索引影响到生产环境 ES 的运行，可以搭建单独的 marvel 数据集群，而生产数据集群上通过主动汇报的方式把数据发送过去。
在两个集群都安装好 marvel 插件后，生产集群的 elasticsearch.yml 上添加如下配置：
marvel.agent.exporter.es.hosts: ["marvel-cluster-ip:9200"]
和大多数 cluster 设置一样，marvel 设置也是可以动态变更的：
# curl –XPUT 127.0.0.1:9200/_cluster/settings -d '{
    "transient" : {
        "marvel.agent.exporter.es.hosts": [ "192.168.0.2:9200", "192.168.0.3:9200" ]
    }
}'
数据接收端的 marvel 集群(即上一行写的 marvel-cluster-ip 代表的主机)则添加如下配置：
marvel.agent.enabled: false
即本身不启用 marvel，以免数据有混淆。
访问

既然是 ES 插件，访问地址自然是插件式的：http://marvel-cluster-ip:9200/_plugin/marvel/index.html
marvel 的监控页面是在 Kibana3 基础上稍有改造。如下图所示，其顶部菜单栏设计了一个下拉选择框，可以切换几个不同纬度的仪表板：


面板定制

Marvel的信息展示能够以Panel为单元进行个性化定制。每个Panel定制的过程比较类似。这里举例定制一个DOCUMENT COUNT文档数Panel，配置过程如下：
点击红色椭圆部分，添加一个Panel: 
输入Panel的名字DOCUMENT COUNT: 
输入Panel Y 轴显示的值 "Primaries.docx.count": 
选择展示风格 
选择查看的集合，这里选择 all: 
一个Document Count Panel就完成了: 



zabbix

之前提到的都是 Elasticsearch 的 sites 类型插件，其实质是实时从浏览器读取 cluster stats 接口数据并渲染页面。这种方式直观，但不适合生产环境的自动化监控和报警处理。要达到这个目标，还是需要使用诸如 nagios、zabbix、ganglia、collectd 这类监控系统。
本节以 zabbix 为例，介绍如何使用监控系统完成 Elasticsearch 的监控报警。
github 上有好几个版本的 ESZabbix 仓库，都源自 Elastic 公司员工 untergeek 最早的贡献。但是当时 Elasticsearch 还没有官方 python 客户端，所以监控程序都是用的是 pyes 库。对于最新版的 ES 来说，已经不推荐使用了。
这里推荐一个修改使用了官方 elasticsearch.py 库的衍生版。GitHub 地址见：https://github.com/Wprosdocimo/Elasticsearch-zabbix。
安装配置

仓库中包括三个文件：
ESzabbix.py
ESzabbix.userparm
ESzabbix_templates.xml
其中，前两个文件需要分发到每个 ES 节点上。如果节点上运行的是 yum 安装的 zabbix，二者的默认位置应该分别是：
/etc/zabbix/zabbix_externalscripts/ESzabbix.py
/etc/zabbix/agent_include/ESzabbix.userparm
然后在各节点安装运行 ESzabbix.py 所需的 python 库依赖：
# yum install -y python-pbr python-pip python-urllib3 python-unittest2
# pip install elasticsearch
安装成功后，你可以试运行下面这行命令，看看命令输出是否正常：
# /etc/zabbix/zabbix_externalscripts/ESzabbix.py cluster status
最后一个文件是 zabbix server 上的模板文件，不过在导入模板之前，还需要先创建一个数值映射，因为在模板中，设置了集群状态的触发报警，没有映射的话，报警短信只有 0, 1, 2 数字不是很易懂。
创建数值映射，在浏览器登录 zabbix-web，菜单栏的 Zabbix Administration 中选择 General 子菜单，然后在右侧下拉框中点击 Value Maping。
选择 create， 新建表单中填写：
name: ES Cluster State
0 ⇒ Green 1 ⇒ Yellow 2 ⇒ Red
完成以后，即可在 Templates 页中通过 import 功能完成导入 ESzabbix_templates.xml。
在给 ES 各节点应用新模板之前，需要给每个节点定义一个 {$NODENAME} 宏，具体值为该节点 elasticsearch.yml 中的 node.name 值。从统一配管的角度，建议大家都设置为 ip 地址。
模板应用

导入完成后，zabbix 里多出来三个可用模板：
Elasticsearch Node & Cache 其中包括两个 Application：ES Cache 和 ES Node。分别有 Node Field Cache Size, Node Filter Cache Size 和 Node Storage Size, Records indexed per second 共计 4 个 item 监控项。在完成上面说的宏定义后，就可以把这个模板应用到各节点(即监控主机)上了。
Elasticsearch Service 只有一个监控项 Elasticsearch service status，做进程监控的，也应用到各节点上。
Elasticsearch Cluster 包括 11 个监控项，如下列所示。其中，ElasticSearch Cluster Status 这个监控项连带有报警的触发器，并对应之前创建的那个 Value Map。
Cluster-wide records indexed per second
Cluster-wide storage size
ElasticSearch Cluster Status
Number of active primary shards
Number of active shards
Number of data nodes
Number of initializing shards
Number of nodes
Number of relocating shards
Number of unassigned shards
Total number of records 这个模板下都是集群总体情况的监控项，所以，运用在一台有 ES 集群读取权限的主机上即可，比如 zabbix server。
其他

untergeek 最近刚更新了他的仓库，重构了一个 es_stats_zabbix 模块用于 Zabbix 监控，有兴趣的读者可以参考：https://github.com/untergeek/zabbix-grab-bag/blob/master/Elasticsearch/es_stats_zabbix.README.md





Elasticsearch 在运维领域的其他运用

目前 Elasticsearch 虽然以 ELK Stack 作为主打产品，但其优秀的分布式设计，灵活的搜索评分函数和强大简洁的检索聚合功能，在运维领域也衍生出不少其他有趣的应用方式。
对于 Elastic 公司来说，这些周边应用，也随时可能成为他们的后续目标产品。就在本书编写期间，packetbeat 就被 Elastic 公司收购，并且可能作为未来数据采集端的标准应用。所以，ELK Stack 用户提前了解其他方面的多种可能，也是非常有意义的。



percolator 接口

在运维体系中，监控和报警总是成双成对的出现。ELK Stack 在时序统计方面的便捷，在很多时候被作为监控的一种方式在使用。那么，自然就引申出一个问题：ELK Stack 如何做报警？
对于简单而且固定需求的模式，我们可以在 Logstash 中利用 filter/metric 和 filter/ruby 等插件做预处理，直接 output/nagios 或 output/zabbix 来报警；但是对于针对全局的、更复杂的情况，Logstash 就无能为力了。
目前比较通行的办法。有两种：
对于匹配报警，采用 ES 的 Percolator 接口做响应报警；
对于时序统计，采用定时任务方式，发送 ES aggs 请求，分析响应体报警。
针对报警的需求，ES 官方也在最近开发了 Watcher 商业产品，和 Shield 一样以 ES 插件形式存在。本节即稍微描述一下 Percolator 接口的用法和 Watcher 产品的思路。相信稍有编程能力的读者都可以根据自己的需求写出来类似的程序。
Percolator 接口

Percolator 接口和我们习惯的搜索接口正好相反，它要求预先定义好 query，然后通过接口提交文档看能匹配上哪个 query。也就是说，这是一个实时的模式过滤接口。
比如我们通过 syslog 来发现硬件报错的时候，可以预先定义 query：
# curl -XPUT http://127.0.0.1:9200/syslog/.percolator/memory -d '{
    "query" : {
        "query_string" : {
            "default_field" : "message",
            "default_operator" : "OR",
            "query" : "mem DMA segfault page allocation AND severity:>2 AND program:kernel"
        }
    }
}'
# curl -XPUT http://127.0.0.1:9200/syslog/.percolator/disk -d '{
    "query" : {
        "query_string" : {
            "default_field" : "message",
            "default_operator" : "OR",
            "query" : "scsi sata hdd sda AND severity:>2 AND program:kernel"
        }
    }
}'
然后，将标准的数据写入请求稍微做一点改动：
# curl -XPOST http://127.0.0.1:9200/syslog/msg/_percolate -d '{
    "doc" : {
        "timestamp" : "Jul 17 03:57:23",
        "host" : "localhost",
        "program" : "kernel",
        "facility" : 0,
        "severity" : 3,
        "message" : "swapper/0: page allocation failure: order:4, mode:0x4020"
    }
}'
得到如下结果：
{
  ...,
  "total": 1,
  "matches": [
     {
        "_index": "syslog",
        "_id": "memory"
     }
  ]
}
从结果可以看出来，这条 syslog 日志匹配上了 memory 异常。接下来就可以发送给报警系统了。
如果是 syslog 索引中已经有的数据，也可以重新过一遍 Percolator 接口：
# curl -XPOST http://127.0.0.1:9200/syslog/msg/existsid/_percoloate
利用更复杂的 query DSL 做 Percolator 请求的示例，推荐阅读官网这篇 geo 定位的文章：https://www.elastic.co/blog/using-percolator-geo-tagging+





Watcher 产品

Watcher 也是 Elastic.co 公司的商业产品，和 Shield，Marvel 一样插件式安装即可：
bin/plugin -i elasticsearch/license/latest
bin/plugin -i elasticsearch/watcher/latest
Watcher 使用方面，也提供标准的 RESTful 接口，示例如下：
# curl -XPUT http://127.0.0.1:9200/_watcher/watch/error_status -d'
{
    "trigger": {
        "schedule" : { "cron" : "0/5 * * * * ?" }
    },
    "input" : {
        "search" : {
            "request" : {
                "indices" : [ "<logstash-{now/d}>", "<logstash-{now/d-1d}>" ],
                "body" : {
                    "query" : {
                        "filtered" : {
                            "query" : { "match" : { "status" : "error" }},
                            "filter" : { "range" : { "@timestamp" : { "from" : "now-5m" }}}
                        }
                    }
                }
            }
        }
    },
    "condition" : {
        "compare" : { "ctx.payload.hits.total" : { "gt" : 0 }}
    },
    "transform" : {
        "search" : {
            "request" : {
                "indices" : [ "<logstash-{now/d}>", "<logstash-{now/d-1d}>" ],
                "body" : {
                    "query" : {
                        "filtered" : {
                            "query" : { "match" : { "status" : "error" }},
                            "filter" : { "range" : { "@timestamp" : { "from" : "now-5m" }}}
                        }
                    },
                    "aggs" : {
                        "topn" : {
                            "terms" : {
                                "field" : "userid"
                            }
                        }
                    }
                }
            }
        }
    },
    "actions" : {
        "email_admin" : {
            "throttle_period" : "15m",
            "email" : {
                "to" : "admin@domain",
                "subject" : "Found {{ctx.payload.hits.total}} Error Events at {{ctx.trigger.triggered_time}}",
                "priority" : "high",
                "body" : "Top10 users:\n{{#ctx.payload.aggregations.topn.buckets}}\t{{key}} {{doc_count}}\n{{/ctx.payload.aggregations.topn.buckets}}"
            }
        }
    }
}'
上面这行命令，意即:
每 5 分钟，向最近两天的 logstash-yyyy.MM.dd 索引发起一次条件为最近五分钟，status 字段内容为 error 的查询请求;
对查询结果做 hits 总数大于 0 的判断;
如果为真，再请求一次上述条件下，userid 字段的 Top 10 数据集作为后续处理的来源;
如果最近 15 分钟内未发送过报警，则向 admin@domain 邮箱发送一个标题为 "Found N erroneous events at yyyy-MM-ddTHH:mm:ssZ"，内容为 "Top10 users" 列表的报警邮件。
整个请求体顺序执行。目前 trigger 只支持 scheduler 方式(但是 schedule 下有 crontab、interval、hourly、daily、weekly、monthly、yearly 等多种写法)，input 支持 search 和 http 方式，actions 支持 email，logging，webhook 方式，transform 是可选项，而且可以设置在 actions 里，不同 actions 做不同的 payload 转换。
crontab 定义语法和 Linux 标准不太一致，采用的是 Quartz，文件见：http://www.quartz-scheduler.org/documentation/quartz-1.x/tutorials/crontrigger。
condition, transform 和 actions 中，默认使用 Watcher 增强版的 xmustache 模板语言（示例中的数组循环就是一例）。也可以使用固化的脚本文件，比如有 threshold_hits.groovy 的话，可以执行：
  "condition" : {
    "script" : {
      "file" : "threshold_hits",
      "params" : {
        "threshold" : 0
      }
    }
  }
Watcher 中可用的 ctx 变量包括：
ctx.watch_id
ctx.execution_time
ctx.trigger.triggered_time
ctx.trigger.scheduled_time
ctx.metadata.*
ctx.payload.*
完整的 Watcher 插件内部执行流程如下图。相信有编程能力的读者都可以用 crontab/at 配合 curl，email 工具仿造出来类似功能的 shell 脚本。


注意：

在 search 中，对 indices 内容可以写完整的索引名比如 syslog，也可以写通配符比如 logstash-*，也可以写时序索引动态定义方式如 <logstash-{now/d}>。而这个动态定义，Watcher 是支持根据时区来确定的，这个需要在 elasticsearch.yml 里配置一行：
watcher.input.search.dynamic_indices.time_zone: '+08:00'



ElastAlert

ElastAlert 是 Yelp 公司开源的一套用 Python2.6 写的报警框架。属于后来 Elastic.co 公司出品的 Watcher 同类产品。官网地址见：http://elastalert.readthedocs.org/。
安装

比官网文档说的步骤稍微复杂一点，因为其中 mock 模块安装时依赖的 setuptools 要求版本在 0.17 以上，CentOS6 默认的不够，需要通过 yum 命令升级，当前可以升级到的是 0.18 版。
# yum install python-setuptools
# git clone https://github.com/Yelp/elastalert.git
# cd elastalert
# python setup.py install
# cp config.yaml.example config.yaml
安装完成后会自带三个命令：
elastalert-create-index ElastAlert 会把执行记录存放到一个 ES 索引中，该命令就是用来创建这个索引的，默认情况下，索引名叫 elastalert_status。其中有 4 个 _type，都有自己的 @timestamp 字段，所以同样也可以用 kibana 来查看这个索引的日志记录情况。
elastalert-rule-from-kibana 从 Kibana3 已保存的仪表盘中读取 Filtering 设置，帮助生成 config.yaml 里的配置。不过注意，它只会读取 filtering，不包括 queries。
elastalert-test-rule 测试自定义配置中的 rule 设置。
最后，运行命令:
# python -m elastalert.elastalert --config ./config.yaml
或者单独执行 rules_folder 里的某个 rule：
# python -m elastalert.elastalert --config ./config.yaml --rule ./examele_rules/one_rule.yaml
配置结构

和 Watcher 类似(或者说也只有这种方式)，ElastAlert 配置结构也分几个部分，但是它有自己的命名。
query 部分

除了有关 ES 服务器的配置以外，主要包括：
run_every 配置，用来设置定时向 ES 发请求，默认 5 分钟。
buffer_time 配置，用来设置请求里时间字段的范围，默认 45 分钟。
rules_folder 配置，用来加载下一阶段的 rule 设置，默认是 example_rules。
timestamp_field 配置，设置 buffer_time 时针对哪个字段，默认是 @timestamp。
timestamp_type 配置，设置 timestamp_field 的时间类型，ElastAlert 内部也需要转换成时间对象，默认是 ISO8601，也可以是 UNIX。
rule 部分

rule 设置各自独立以文件方式存储在 rules_folder 设置的目录里。其中可以定义下面这些参数：
name 配置，每个 rule 需要有自己独立的 name，一旦重复，进程将无法启动。
type 配置，选择某一种数据验证方式。
index 配置，从某类索引里读取数据，目前只支持通配符。
filter 配置，设置向 ES 请求的过滤条件。
timeframe 配置，累积触发报警的时长。
alert 配置，设置触发报警时执行哪些报警手段。
不同的 type 还有自己独特的配置选项。目前 ElastAlert 有以下几种自带 ruletype：
any: 只要有匹配就报警；
blacklist: compare_key 字段的内容匹配上 blacklist 数组里任意内容；
whitelist: compare_key 字段的内容一个都没能匹配上 whitelist 数组里内容；
change: 在相同 query_key 条件下，compare_key 字段的内容，在 timeframe 范围内发送变化；
frequency: 在相同 query_key 条件下，timeframe 范围内有 num_events 个被过滤出来的异常；
spike: 在相同 query_key 条件下，前后两个 timeframe 范围内数据量相差比例超过 spike_height。其中可以通过 spike_type 设置具体涨跌方向是up, down, both。还可以通过threshold_ref 设置要求上一个周期数据量的下限，threshold_cur 设置要求当前周期数据量的下限，如果数据量不到下限，也不触发；
flatline: timeframe 范围内，数据量小于 threshold 阈值；
new_term: fields 字段新出现之前 terms_window_size(默认 30 天) 范围内最多的 terms_size(默认 50) 个结果以外的数据；
cardinality: 在相同 query_key 条件下，timeframe 范围内 cardinality_field 的值超过 max_cardinality 或者低于 min_cardinality。
alert 部分

alert 配置是一个数组，目前支持 command, email，jira，opsgenie，sns，hipchat，slack 等方式。
command
command 最灵活也最简单。默认会采用 %(fieldname)s 格式：
command: ["/bin/send_alert", "--username", "%(username)s", "--time", "%(key_as_string)s"]
如果要用的比较多，可以开启 pipe_match_json 参数，会把整个过滤到的内容，以一整个 JSON 字符串的方式管道输入指定脚本。
email
email 方式采用 SMTP 协议，所以有一系列 smtp_* 配置，然后加上 email 参数提供收件人地址数组。
特殊的是，email 和 jira 两种方式，ElastAlert 提供了一些内容格式化模板：
比如可以这样控制邮件标题：
alert_subject: "Issue {0} occurred at {1}"
alert_subject_args:
  - issue.name
  - "@timestamp"
而默认的邮件内容模板是：
body = rule_name
       [alert_text]
       ruletype_text
       {top_counts}
       {field_values}
这些内容同样可以通过 alert_text(及对应 alert_text_args)等来灵活修改。
此外，alert 还有一系列控制报警风暴的选项，从属于 rule：
aggregation：设置一个时长，则该时长内所有报警最终合在一起发一次；
realert：设置一个时长，则该时长内，相同 query_key 的报警只发一个；
exponential_realert：设置一个时长，必须大于 realert 设置。则在 realert 到 exponential_realert 之间，每次报警后，realert 自动翻倍。
enhancements 部分

match_enhancements 配置，设置一个数组，在报警内容发送到 alert 之前修改具体数据。ElastAlert 默认不提供具体的 enhancements 实现，需要自己扩展。
不过，作为通用方式，ElastAlert 提供几个便捷选项，把 Kibana 地址加入报警：
generate_kibana_link: 自动生成一个 Kibana3 的临时仪表盘附在报警内容上。
use_kibana_dashboard: 采用现成的 Kibana3 仪表盘附在报警内容上。
use_kibana4_dashboard: 采用现成的 Kibana4 仪表盘附在报警内容上。
扩展

rule

创建一个自己的 rule，是以 Python 模块的形式存在的，所以首先创建目录：
# mkdir rule_modules
# cd rule_modules
# touch __init__.py example_rule.py
example_rule.py 的内容如下：
import dateutil.parser
from elastalert.util import ts_to_dt
from elastalert.ruletypes import RuleType

class AwesomeNewRule(RuleType):
    # 用来指定本 rule 对应的配置文件中必要的参数项
    required_options = set(['time_start', 'time_end', 'usernames'])
    # 每次运行获取的数据以时间排序数据传递给 add_data 函数
    def add_data(self, data):
        for document in data:
            # 配置文件中的设置可以通过 self.rule[] 获取
            if document['username'] in self.rule['usernames']:
                login_time = document['@timestamp'].time()
                time_start = dateutil.parser.parse(self.rule['time_start']).time()
                time_end = dateutil.parser.parse(self.rule['time_end']).time()
                if login_time > time_start and login_time < time_end:
                    # 最终过滤结果，使用 self.add_match 添加
                    self.add_match(document)

    # alert_text 中使用的文本
    def get_match_str(self, match):
        return "%s logged in between %s and %s" % (match['username'],
                                                   self.rule['time_start'],
                                                   self.rule['time_end'])
    def garbage_collect(self, timestamp):
        pass
配置中，指定
type: rule_modules.example_rule.AwesomeRule
time_start: "20:00"
time_end: "24:00"
usernames:
  - "admin"
  - "userXYZ"
  - "foobaz"
即可使用。
alerter

alerter 也是以 Python 模块的形式存在的，所以还是要创建目录(如果之前二次开发 rule 已经创建过可以跳过)：
# mkdir rule_modules
# cd rule_modules
# touch __init__.py example_alert.py
example_alert.py 的内容如下：
from elastalert.alerts import Alerter, basic_match_string

class AwesomeNewAlerter(Alerter):
    required_options = set(['output_file_path'])
    def alert(self, matches):
        for match in matches:
            with open(self.rule['output_file_path'], "a") as output_file:
                # basic_match_string 函数用来转换异常数据成默认格式的字符串
                match_string = basic_match_string(self.rule, match)
                output_file.write(match_string)
    # 报警发出后，ElastAlert 会调用该函数的结果写入 ES 索引的 alert_info 字段内
    def get_info(self):
        return {'type': 'Awesome Alerter',
                'output_file': self.rule['output_file_path']}
配置中，指定
alert: "rule_modules.example_alert.AwesomeNewAlerter"
output_file_path: "/tmp/alerts.log"
即可使用。
enhancement

enhancement 也是以 Python 模块的形式存在的，所以还是要创建目录(如果之前二次开发 rule 或 alert 已经创建过可以跳过)：
# mkdir rule_modules
# cd rule_modules
# touch __init__.py example_enhancement.py
example_enhancement.py 的内容如下：
from elastalert.enhancements import BaseEnhancement
class MyEnhancement(BaseEnhancement):
    def process(self, match):
        if 'domain' in match:
            url = "http://who.is/whois/%s" % (match['domain'])
            match['domain_whois_link'] = url
在需要的 rule 配置文件中添加如下内容即可启用：
match_enhancements:
  - "rule_modules.example_enhancement.MyEnhancement"
因为 match_enhancements 是个数组，也就是说，如果数组有多个 enhancement，会依次执行，完全完成后，才传递给 alert。+




时序数据

之前已经介绍过，ES 默认存储数据时，是有索引数据、_all 全文索引数据、_source JSON 字符串三份的。其中，索引数据由于倒排索引的结构，压缩比非常高。因此，在某些特定环境和需求下，可以只保留索引数据，以极小的容量代价，换取 ES 灵活的数据结构和聚合统计功能。
在监控系统中，对监控项和监控数据的设计一般是这样：
metric_path value timestamp (Graphite 设计) { "host": "Host name 1", "key": "item_key", "value": "33", "clock": 1381482894 } (Zabbix 设计)
这些设计有个共同点，数据是二维平面的。以最简单的访问请求状态监控为例，一次请求，可能转换出来的 metric_path 或者说 key 就有：{city,isp,host,upstream}.{urlpath...}.{status,rt,ut,size,speed} 这么多种。假设 urlpath 有 1000 个，就是 20000 个组合。意味着需要发送 20000 条数据，做 20000 次存储。
而在 ES 里，这就是实实在在 1000 条日志。而且在多条日志的时候，因为词元的相对固定，压缩比还会更高。所以，使用 ES 来做时序监控数据的存储和查询，是完全可行的办法。
对时序数据，关键就是定义缩减数据重复。template 示例如下：
{
  "order" : 2,
  "template" : "logstash-monit-*",
  "settings" : {
  },
  "mappings" : {
    "_default_" : {
      "_source" : {
        "enabled" : false
      },
      "_all" : {
        "enabled" : false
      }
    }
  },
  "aliases" : { }
}
如果有些字段，是完全不用 Query ，只参加 Aggregation 的，还可以设置：
      "properties" : {
        "sid" : {
          "index" : "no",
          "doc_values" : true,
          "type" : "string"
        }
      },
关于 Elasticsearch 用作 rrd 用途，与 MongoDB 等其他工具的性能测试与对比，可以阅读腾讯工程师写的系列文章：http://segmentfault.com/a/1190000002690600



juttle 介绍

juttle 是一个 nodejs 项目，专注于数据处理和可视化。它自定义了一套自己的 DSL，提供交互式命令行、程序运行、界面访问三种运行方式。
在 juttle 的 DSL 中，可以用 | 管道符串联下列指令实现数据处理：
通过 read 指令读取来自 http、file、elasticsearch、graphite、influxdb、opentsdb、mysql 等数据源，
通过 filter 指令及自定义的 JavaScript 函数做数据过滤，
通过 reduce 指令做数据聚合，
通过 join 指令做数据关联，
通过 write 指令做数据转储，
通过 view 指令做数据可视化。
更关键的，可以用 () 并联同一层级的多条指令进行处理。
看起来非常有意思的项目，赶紧试试吧。
安装部署

既然说了这是一个 nodejs 项目，自然是通过 npm 安装了：
sudo npm install -g juttle
sudo npm install -g juttle-engine
注意，如果是在 MacBook 上安装的话，一定要先通过 AppStore 安装好 Xcode 并确认完 license。npm 安装依赖的 sqlite3 的时候没有 xcode 会僵死在那。
juttle 包提供了命令行交互，juttle-engine 包提供了网页访问的服务器。
juttle 的配置文件默认读取位置是 $HOME/.juttle/config.json。比如读取本机 elasticsearch 的数据，那么定义如下：
{
    "adapters": {
        "elastic": {
            "address": "localhost",
            "port": 9200
        }
    }
}
甚至可以读取多个不同来源的 elasticsearch，这样：
{
    "adapters": {
        "elastic": [{
            "id": "one",
            "address": "localhost",
            "port": 9200
        }, {
            "id": "two",
            "address": "localhost",
            "port": 9201
        }],
        "influx": {
            "url": "http://examples_influxdb_1:8086",
            "user": "root",
            "password": "root"
        }
    }
}
命令行运行示例

配置完成，就可以交互式命令行运行了。终端输入 juttle 回车进入交互界面。我们输入下面一段查询：
juttle> read elastic -id one -index 'logstash-*'  -from :1 year ago: -to :now: 'MacBook-Pro' | reduce -every :1h: c = count() by path | filter c > 1000 | put line = 10000 | view table -columnOrder 'time', 'c', 'line', 'path'
输出如下：
┌────────────────────────────────────┬──────────┬──────────┬─────────────────────────────┐
│ time                               │ c        │ line     │ path                        │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-02T10:00:00.000Z           │ 4392     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-02T11:00:00.000Z           │ 4818     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-02T12:00:00.000Z           │ 2038     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-02T13:00:00.000Z           │ 1826     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-02T15:00:00.000Z           │ 10267    │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-02T16:00:00.000Z           │ 10999    │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-02T17:00:00.000Z           │ 3528     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-03T00:00:00.000Z           │ 2498     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-03T03:00:00.000Z           │ 4600     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-03T04:00:00.000Z           │ 7751     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-03T05:00:00.000Z           │ 3249     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-03T06:00:00.000Z           │ 5715     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-03T07:00:00.000Z           │ 4374     │ 10000    │ /var/log/system.log         │
├────────────────────────────────────┼──────────┼──────────┼─────────────────────────────┤
│ 2016-03-03T08:00:00.000Z           │ 2600     │ 10000    │ /var/log/system.log         │
└────────────────────────────────────┴──────────┴──────────┴─────────────────────────────┘
漂亮的终端表格！
警告

需要注意的是，juttle 和 es-hadoop 一样，也是通过 RESTful API 和 elasticsearch 交互，所以除了个别已经提前实现好了的 reduce 方法可以转换成 aggregation 以外，其他的 juttle 指令，都是通过 query 把数据拿回来以后，由 juttle 本身做的运算处理。juttle-adapter-elastic 模块的 DEFAULT_FETCH_SIZE 设置是 10000 条。
而比 es-hadoop 更差的是，因为 juttle 是单机程序，它还没有像 es-hadoop 那样并发 partition 直连每个 elasticsearch 的 shard 做并发请求。
juttle-viz 可视化界面

上一小节介绍了一下怎么用 juttle 交互式命令行查看表格式输出。juttle 事实上还提供了一个 web 服务器，做数据可视化效果，这个同样是用 juttle 语言描述配置。
我们已经安装好了 juttle-engine 模块，那么直接启动服务器即可：
~$ juttle-engine -d
然后浏览器打开 http://localhost:8080 就能看到页面了。注意，请使用 Chrome v45 以上版本或者 Safari 等其他浏览器，否则有个 Array 上的 bug。
但是目前这个页面上本身不提供输入框直接写 juttle 语言。所以需要我们把 juttle 语言写成脚本文件，再来通过页面加载。
~$ cat > ~/test.juttle <<EOF
read elastic -index 'logstash-*'  -from :-2d: -to :now: 'MacBook-Pro'
  | reduce -every :1h: count() by 'path.raw'
  | (
      view timechart -row 0 -col 0;;
      view table -height 200 -row 1 -col 0;
      view piechart -row 1 -col 0;
  );
(
  read elastic -index 'logstash-*'  -from :-2d: -to :-1d: 'MacBook-Pro' AND '/var/log/system.log'
    | reduce -every :1h: count();
  read elastic -index 'logstash-*'  -from :-1d: -to :now: 'MacBook-Pro' AND '/var/log/system.log'
    | reduce -every :1h: count();
)
  | (
      view timechart -duration :1 day: -overlayTime true -height 400 -row 0 -col 1 -title 'syslog hour-on-hour';
      view table -height 200 -row 1 -col 1;
  );
EOF
然后访问 http://localhost:8080?path=/test.juttle，注意这里的path参数的写法，这个/其实指的是你运行 juttle-engine 命令的时候的路径，而不是真的设备根目录。
就可以在浏览器上看到如下效果：


页面上还有一行有关 path.raw 的 WARNING 提示，那是因为 juttle 目前对 elasticsearch 的 mapping 解析支持的不是很好，但是不影响使用，可以不用管。
可视化相关指令介绍

我们可以看到这次的 juttle 脚本，跟昨天在命令行下运行的几个区别：
我们用上了 ()，这是 juttle 的一大特技，对同一结果并联多个 view ，或者并联多个输入结果做相同的后续处理等等。
我们对 view 用上了 row 和 col 参数，用来指定他们在页面上的布局。
有一个 timechart 我们用了 -durat :1d: -overlayTime true 参数。这是 timechart 独有的参数，专门用来实现同比环比的。在图上的效果大家也可以看到了。不过目前也有小问题，就是鼠标放到图上的时候，只能看到第二个结果的指标说明，看不到第一个的。



Kale 系统

Kale 系统是 Etsy 公司开源的一个监控分析系统。Kale 分为两个部分：skyline 和 oculus。skyline 负责对时序数据进行概率分布校验，对校验失败率超过阈值的时序数据发报警；oculus 负责给被报警的时序，找出趋势相似的其他时序作为关联性参考。
看到“相似”两个字，你一定想到了。没错，oculus 组件，就是利用了 Elasticsearch 的相似度打分。
oculus 中，为 Elasticsearch 的 org.elasticsearch.script.ExecutableScript 扩展了 DTW 和 Euclidian 两种 NativeScript。可以在界面上选择用其中某一种算法来做相似度打分：


然后相似度最高的几个时序图就依次排列出来了。
Euclidian 即欧几里得距离，是时序相似度计算里最基础的方式。
DWT 即动态时间规整(Dynamic Time Warping)，也是时序相似度计算的常用方式，它和欧几里得距离的差别在于，欧几里得距离要求比对的时序数据是一一对应的，而动态时间规整计算的时序数据并不要求长度相等。在运维监控来说，也就是延后一定时间发生的相近趋势也可以以很高的打分项排名靠前。
不过，oculus 插件仅更新到支持 Elasticsearch-0.90.3 版本为止。Etsy 性能优化团队在 Oreilly 2015 大会上透露，他们内部已经根据 Kale 的经验教训，重新开发了 Kale 2.0 版。会在年内开源放出来。大家一起期待吧！

